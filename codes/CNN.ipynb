{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step0: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from torchviz import make_dot\n",
    "from string import punctuation\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torchvision.models import AlexNet\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"../data/english_yep_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Count the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['someon', 'ha', 'work', 'mani', 'museum', 'wa', 'eager', 'visit', 'thi', 'galleri']\n",
      "Top ten occuring words :  [('wa', 199857), ('thi', 86639), ('place', 55772), ('food', 53489), ('good', 50852), ('great', 44401), ('veri', 44062), ('time', 42695), ('get', 38251), ('would', 38160)]\n"
     ]
    }
   ],
   "source": [
    "all_reviews = list(reviews['cleaned'])\n",
    "all_text = \" \".join(all_reviews)\n",
    "all_words = all_text.split()\n",
    "print(all_words[0:10])\n",
    "\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(all_words)\n",
    "total_words = len(all_words)\n",
    "sorted_words=count_words.most_common(total_words)\n",
    "print(\"Top ten occuring words : \", sorted_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "# We will start createing dictionary with index 1 because 0 \n",
    "    # is reserved for padding\n",
    "\n",
    "vocab_to_int = {w: i+1 for i, (w, c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4: Encode the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode review\n",
    "encoded_reviews = list()\n",
    "for review in all_reviews:\n",
    "    encoded_review = list()\n",
    "    for word in review.split():\n",
    "        if word not in vocab_to_int.keys():\n",
    "            # if word is not available in vocab_to_int put 0 in that place\n",
    "            encoded_review.append(0)\n",
    "        else:\n",
    "            encoded_review.append(vocab_to_int[word])\n",
    "    encoded_reviews.append(encoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Make the encode_review of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all the encoded_review of the same length\n",
    "# this step will return features of review_ints,\n",
    "# where each review is padded with 0's or truncated to the input seq_length.\n",
    "# the longest review has 564 words\n",
    "# sequence_length is 100, but also could be 150, 200, 250 (here just for save energy)\n",
    "\n",
    "sequence_length = 100\n",
    "features = np.zeros((len(encoded_reviews), sequence_length), dtype=int)\n",
    "for i, review in enumerate(encoded_reviews):\n",
    "    review_len = len(review)\n",
    "    if review_len <= sequence_length:\n",
    "        zeros = list(np.zeros(sequence_length-review_len))\n",
    "        new = zeros+review\n",
    "    else:\n",
    "        new = review[:sequence_length]\n",
    "    features[i, :] = np.array(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6: Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels, 0 negative, 1 neutral, 2 positive\n",
    "labels = list(reviews['Review_Labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step7: Split this feature data into Traning, Testing and Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79999 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# split this feature data into training and validation set\n",
    "# 80% training, 10% test and 10% validation dataset\n",
    "\n",
    "# However, for cpu running, set 10% of them\n",
    "# features = features[:int(0.5*len(features))]\n",
    "# labels = labels[:int(0.5*len(labels))]\n",
    "train_x = features[:int(0.8*len(features))]\n",
    "train_y = labels[:int(0.8*len(features))]\n",
    "valid_x = features[int(0.8*len(features)):int(0.9*len(features))]\n",
    "valid_y = labels[int(0.8*len(features)):int(0.9*len(features))]\n",
    "test_x = features[int(0.9*len(features)):]\n",
    "test_y = labels[int(0.9*len(features)):]\n",
    "print(len(train_y), len(valid_y), len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step8: Create DataLoader objects for Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#create Tensor Dataset\n",
    "train_data=TensorDataset(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n",
    "valid_data=TensorDataset(torch.FloatTensor(valid_x), torch.FloatTensor(valid_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(test_x), torch.FloatTensor(test_y))\n",
    "\n",
    "#dataloader\n",
    "# remember to add drop_last=True, which will delete the last batch of the data if it's size is not equal to batch_size\n",
    "batch_size=400\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step9: Analyze the dataloader data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([400, 100])\n",
      "Sample input: \n",
      " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3350e+03, 5.4300e+02,\n",
      "         1.3600e+03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.0000e+00, 4.0000e+00,\n",
      "         1.2000e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.9000e+01, 1.5000e+02,\n",
      "         4.2000e+01],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.8000e+01, 5.0000e+01,\n",
      "         1.4800e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.8000e+01, 6.0000e+00,\n",
      "         1.2000e+01],\n",
      "        [1.1630e+03, 1.6300e+03, 3.6200e+02,  ..., 9.0000e+00, 7.3890e+03,\n",
      "         1.7390e+03]])\n",
      "Sample label size:  torch.Size([400])\n",
      "Sample label: \n",
      " tensor([0., 2., 2., 2., 2., 2., 2., 2., 0., 2., 2., 0., 2., 0., 2., 0., 0., 2.,\n",
      "        2., 2., 1., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 1., 2., 1., 1., 0.,\n",
      "        2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 1., 0., 2., 2., 0., 2., 2., 2.,\n",
      "        2., 1., 2., 2., 2., 2., 0., 2., 2., 0., 2., 2., 2., 0., 1., 2., 2., 0.,\n",
      "        0., 0., 2., 2., 0., 0., 2., 2., 0., 2., 2., 2., 0., 0., 2., 2., 2., 0.,\n",
      "        0., 2., 2., 2., 2., 0., 2., 0., 2., 2., 2., 2., 2., 2., 2., 1., 2., 0.,\n",
      "        2., 2., 0., 2., 2., 0., 2., 2., 2., 2., 1., 2., 2., 1., 2., 2., 2., 2.,\n",
      "        1., 0., 0., 2., 2., 0., 2., 0., 1., 2., 2., 2., 2., 0., 1., 2., 2., 2.,\n",
      "        0., 2., 1., 2., 2., 0., 2., 2., 0., 2., 1., 0., 2., 2., 0., 0., 2., 2.,\n",
      "        2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 0., 2., 2., 0., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 0., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        0., 0., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 2., 2., 2., 2., 2., 2.,\n",
      "        0., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 2., 2., 0., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 1., 0., 2., 0., 2., 2.,\n",
      "        2., 2., 0., 2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 1.,\n",
      "        2., 2., 2., 2., 2., 2., 1., 2., 2., 0., 2., 0., 2., 1., 0., 2., 1., 2.,\n",
      "        2., 2., 0., 0., 2., 0., 2., 2., 2., 2., 0., 2., 2., 0., 0., 2., 1., 2.,\n",
      "        0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2.,\n",
      "        0., 2., 1., 2., 2., 2., 2., 0., 1., 0., 1., 0., 1., 0., 2., 2., 2., 2.,\n",
      "        1., 0., 0., 2., 2., 0., 2., 2., 2., 0., 2., 2., 0., 0., 1., 2., 2., 2.,\n",
      "        2., 2., 0., 0., 2., 2., 0., 2., 2., 1., 2., 2., 0., 2., 2., 1., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 0., 2., 2., 0., 2., 2., 2., 2., 2., 1., 2., 2.,\n",
      "        0., 1., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step10: Create an CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed, num_filters, filter_size, num_classes, dropout):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed, padding_idx=vocab_size - 1)\n",
    "        self.convs = nn.ModuleList(\n",
    "                    [nn.Conv2d(1, num_filters, (k, embed)) for k in filter_sizes])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
    "        \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        out = self.embedding(x[0])\n",
    "#         print(out.shape)\n",
    "        out = out.unsqueeze(1)\n",
    "#         print(out.shape)\n",
    "        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n",
    "#         print(out.shape)\n",
    "        out = self.dropout(out)\n",
    "#         print(out.shape)\n",
    "        out = self.fc(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step11: Initialize the CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Model(\n",
      "  (embedding): Embedding(77398, 300, padding_idx=77397)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))\n",
      "    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
      "    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
      "    (3): Conv2d(1, 256, kernel_size=(5, 300), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int) + 1\n",
    "embed = 300\n",
    "filter_sizes = (2, 3, 4, 5)\n",
    "num_filters = 256\n",
    "num_classes = 3\n",
    "dropout = 0.5\n",
    "\n",
    "model_cnn = CNN_Model(vocab_size, embed, num_filters, filter_sizes, num_classes, dropout)\n",
    "print(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step12: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(epoch, model):\n",
    "    lr = 0.001  # learning rate\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "    total_batch = 0\n",
    "    train_loss = 0\n",
    "    result_train_dict = {}\n",
    "    result_valid_dict = {}\n",
    "    epoch_train_list = []\n",
    "    epoch_valid_list = []\n",
    "    batch_train_list = []\n",
    "    batch_valid_list = []\n",
    "    loss_train_list = []\n",
    "    loss_valid_list = []\n",
    "    acc_train_list = []\n",
    "    acc_valid_list = []\n",
    "    f1_train_list = []\n",
    "    f1_valid_list = []\n",
    "    recall_train_list = []\n",
    "    recall_valid_list = []\n",
    "    \n",
    "    for batch_idx, (trains, labels) in enumerate(train_loader):\n",
    "        \n",
    "        outputs = model(trains.long()[None, ...])\n",
    "        model.zero_grad()\n",
    "        loss = F.cross_entropy(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        true = labels.data.cpu()\n",
    "        predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "        train_acc = metrics.accuracy_score(true, predic)\n",
    "        train_f1 = metrics.f1_score(true, predic, average='micro')\n",
    "        train_recall = metrics.recall_score(true, predic, average='micro')\n",
    "        train_loss_value = train_loss/(batch_idx+1)\n",
    "        \n",
    "        epoch_train_list.append(epoch)\n",
    "        batch_train_list.append(batch_idx)\n",
    "        loss_train_list.append(train_loss_value)\n",
    "        acc_train_list.append(train_acc)\n",
    "        f1_train_list.append(train_f1)\n",
    "        recall_train_list.append(train_recall)\n",
    "        \n",
    "        if total_batch % 20 == 0 :\n",
    "            \n",
    "            for trains, labels in valid_loader:\n",
    "                outputs = model(trains.long()[None, ...])\n",
    "                valid_loss = F.cross_entropy(outputs, labels.long())\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                valid_acc = metrics.accuracy_score(true, predic)\n",
    "                valid_f1 = metrics.f1_score(true, predic, average='micro')\n",
    "                valid_recall = metrics.recall_score(true, predic, average='micro')\n",
    "                \n",
    "                epoch_valid_list.append(epoch)\n",
    "                batch_valid_list.append(batch_idx)\n",
    "                loss_valid_list.append(valid_loss.item())\n",
    "                acc_valid_list.append(valid_acc)\n",
    "                f1_valid_list.append(valid_f1)\n",
    "                recall_valid_list.append(valid_recall)\n",
    "                \n",
    "                avg_acc_valid = np.array(acc_valid_list).mean()\n",
    "                avg_loss_valid = np.array(loss_valid_list).mean()\n",
    "                \n",
    "            print('epoch: {}'.format(epoch), 'batch: {}'.format(batch_idx), \n",
    "                  'total train loader: {}'.format(len(train_loader)),\n",
    "                  'T_Loss: %.3f | T_Acc: %.3f |' % (train_loss_value, train_acc),\n",
    "                  'T_f1: %.3f | T_recall: %.3f ||' % (train_f1, train_recall),\n",
    "                  'V_Loss: %.3f | V_Acc: %.3f' % (avg_loss_valid, avg_acc_valid),\n",
    "                  'V_f1: %.3f | V_recall: %.3f' % (valid_f1, valid_recall))\n",
    "            \n",
    "        total_batch += 1\n",
    "        \n",
    "    result_train_dict['epoch'] = epoch_train_list\n",
    "    result_train_dict['batch'] = batch_train_list\n",
    "    result_train_dict['loss'] = loss_train_list\n",
    "    result_train_dict['acc'] = acc_train_list\n",
    "    result_train_dict['f1'] = f1_train_list\n",
    "    result_train_dict['recall'] = recall_train_list\n",
    "    \n",
    "    result_valid_dict['epoch'] = epoch_valid_list\n",
    "    result_valid_dict['batch'] = batch_valid_list\n",
    "    result_valid_dict['loss'] = loss_valid_list\n",
    "    result_valid_dict['acc'] = acc_valid_list\n",
    "    result_valid_dict['f1'] = f1_valid_list\n",
    "    result_valid_dict['recall'] = recall_valid_list\n",
    "\n",
    "    pd_train = pd.DataFrame(result_train_dict)\n",
    "    pd_valid = pd.DataFrame(result_valid_dict)\n",
    "    \n",
    "    return pd_train, pd_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step12-1. CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 total train loader: 199 T_Loss: 1.283 | T_Acc: 0.680 | T_f1: 0.680 | T_recall: 0.680 || V_Loss: 1.199 | V_Acc: 0.391 V_f1: 0.395 | V_recall: 0.395\n",
      "epoch: 0 batch: 20 total train loader: 199 T_Loss: 0.844 | T_Acc: 0.703 | T_f1: 0.703 | T_recall: 0.703 || V_Loss: 0.935 | V_Acc: 0.562 V_f1: 0.725 | V_recall: 0.725\n",
      "epoch: 0 batch: 40 total train loader: 199 T_Loss: 0.754 | T_Acc: 0.730 | T_f1: 0.730 | T_recall: 0.730 || V_Loss: 0.825 | V_Acc: 0.630 V_f1: 0.785 | V_recall: 0.785\n",
      "epoch: 0 batch: 60 total train loader: 199 T_Loss: 0.704 | T_Acc: 0.792 | T_f1: 0.792 | T_recall: 0.792 || V_Loss: 0.763 | V_Acc: 0.665 V_f1: 0.767 | V_recall: 0.767\n",
      "epoch: 0 batch: 80 total train loader: 199 T_Loss: 0.671 | T_Acc: 0.745 | T_f1: 0.745 | T_recall: 0.745 || V_Loss: 0.719 | V_Acc: 0.689 V_f1: 0.802 | V_recall: 0.802\n",
      "epoch: 0 batch: 100 total train loader: 199 T_Loss: 0.648 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.688 | V_Acc: 0.707 V_f1: 0.792 | V_recall: 0.792\n",
      "epoch: 0 batch: 120 total train loader: 199 T_Loss: 0.628 | T_Acc: 0.797 | T_f1: 0.797 | T_recall: 0.797 || V_Loss: 0.665 | V_Acc: 0.719 V_f1: 0.787 | V_recall: 0.787\n",
      "epoch: 0 batch: 140 total train loader: 199 T_Loss: 0.613 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.646 | V_Acc: 0.729 V_f1: 0.765 | V_recall: 0.765\n",
      "epoch: 0 batch: 160 total train loader: 199 T_Loss: 0.598 | T_Acc: 0.797 | T_f1: 0.797 | T_recall: 0.797 || V_Loss: 0.629 | V_Acc: 0.737 V_f1: 0.807 | V_recall: 0.807\n",
      "epoch: 0 batch: 180 total train loader: 199 T_Loss: 0.588 | T_Acc: 0.815 | T_f1: 0.815 | T_recall: 0.815 || V_Loss: 0.616 | V_Acc: 0.745 V_f1: 0.787 | V_recall: 0.787\n",
      "epoch: 1 batch: 0 total train loader: 199 T_Loss: 0.453 | T_Acc: 0.828 | T_f1: 0.828 | T_recall: 0.828 || V_Loss: 0.772 | V_Acc: 0.683 V_f1: 0.670 | V_recall: 0.670\n",
      "epoch: 1 batch: 20 total train loader: 199 T_Loss: 0.524 | T_Acc: 0.848 | T_f1: 0.848 | T_recall: 0.848 || V_Loss: 0.631 | V_Acc: 0.745 V_f1: 0.812 | V_recall: 0.812\n",
      "epoch: 1 batch: 40 total train loader: 199 T_Loss: 0.496 | T_Acc: 0.825 | T_f1: 0.825 | T_recall: 0.825 || V_Loss: 0.582 | V_Acc: 0.767 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 1 batch: 60 total train loader: 199 T_Loss: 0.489 | T_Acc: 0.818 | T_f1: 0.818 | T_recall: 0.818 || V_Loss: 0.559 | V_Acc: 0.777 V_f1: 0.775 | V_recall: 0.775\n",
      "epoch: 1 batch: 80 total train loader: 199 T_Loss: 0.485 | T_Acc: 0.815 | T_f1: 0.815 | T_recall: 0.815 || V_Loss: 0.543 | V_Acc: 0.784 V_f1: 0.787 | V_recall: 0.787\n",
      "epoch: 1 batch: 100 total train loader: 199 T_Loss: 0.480 | T_Acc: 0.787 | T_f1: 0.787 | T_recall: 0.787 || V_Loss: 0.531 | V_Acc: 0.789 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 1 batch: 120 total train loader: 199 T_Loss: 0.475 | T_Acc: 0.807 | T_f1: 0.807 | T_recall: 0.807 || V_Loss: 0.523 | V_Acc: 0.793 V_f1: 0.843 | V_recall: 0.843\n",
      "epoch: 1 batch: 140 total train loader: 199 T_Loss: 0.473 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.517 | V_Acc: 0.796 V_f1: 0.848 | V_recall: 0.848\n",
      "epoch: 1 batch: 160 total train loader: 199 T_Loss: 0.472 | T_Acc: 0.782 | T_f1: 0.782 | T_recall: 0.782 || V_Loss: 0.512 | V_Acc: 0.799 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 1 batch: 180 total train loader: 199 T_Loss: 0.471 | T_Acc: 0.805 | T_f1: 0.805 | T_recall: 0.805 || V_Loss: 0.507 | V_Acc: 0.801 V_f1: 0.843 | V_recall: 0.843\n",
      "epoch: 2 batch: 0 total train loader: 199 T_Loss: 0.375 | T_Acc: 0.863 | T_f1: 0.863 | T_recall: 0.863 || V_Loss: 0.484 | V_Acc: 0.810 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 2 batch: 20 total train loader: 199 T_Loss: 0.427 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.477 | V_Acc: 0.816 V_f1: 0.812 | V_recall: 0.812\n",
      "epoch: 2 batch: 40 total train loader: 199 T_Loss: 0.427 | T_Acc: 0.825 | T_f1: 0.825 | T_recall: 0.825 || V_Loss: 0.480 | V_Acc: 0.815 V_f1: 0.833 | V_recall: 0.833\n",
      "epoch: 2 batch: 60 total train loader: 199 T_Loss: 0.428 | T_Acc: 0.815 | T_f1: 0.815 | T_recall: 0.815 || V_Loss: 0.476 | V_Acc: 0.816 V_f1: 0.802 | V_recall: 0.802\n",
      "epoch: 2 batch: 80 total train loader: 199 T_Loss: 0.432 | T_Acc: 0.797 | T_f1: 0.797 | T_recall: 0.797 || V_Loss: 0.474 | V_Acc: 0.816 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 2 batch: 100 total train loader: 199 T_Loss: 0.435 | T_Acc: 0.815 | T_f1: 0.815 | T_recall: 0.815 || V_Loss: 0.474 | V_Acc: 0.816 V_f1: 0.797 | V_recall: 0.797\n",
      "epoch: 2 batch: 120 total train loader: 199 T_Loss: 0.438 | T_Acc: 0.815 | T_f1: 0.815 | T_recall: 0.815 || V_Loss: 0.476 | V_Acc: 0.815 V_f1: 0.757 | V_recall: 0.757\n",
      "epoch: 2 batch: 140 total train loader: 199 T_Loss: 0.442 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.475 | V_Acc: 0.815 V_f1: 0.823 | V_recall: 0.823\n",
      "epoch: 2 batch: 160 total train loader: 199 T_Loss: 0.443 | T_Acc: 0.835 | T_f1: 0.835 | T_recall: 0.835 || V_Loss: 0.473 | V_Acc: 0.816 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 2 batch: 180 total train loader: 199 T_Loss: 0.443 | T_Acc: 0.833 | T_f1: 0.833 | T_recall: 0.833 || V_Loss: 0.472 | V_Acc: 0.816 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 3 batch: 0 total train loader: 199 T_Loss: 0.438 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.586 | V_Acc: 0.763 V_f1: 0.750 | V_recall: 0.750\n",
      "epoch: 3 batch: 20 total train loader: 199 T_Loss: 0.424 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.524 | V_Acc: 0.791 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 3 batch: 40 total train loader: 199 T_Loss: 0.424 | T_Acc: 0.833 | T_f1: 0.833 | T_recall: 0.833 || V_Loss: 0.502 | V_Acc: 0.802 V_f1: 0.825 | V_recall: 0.825\n",
      "epoch: 3 batch: 60 total train loader: 199 T_Loss: 0.422 | T_Acc: 0.863 | T_f1: 0.863 | T_recall: 0.863 || V_Loss: 0.492 | V_Acc: 0.806 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 3 batch: 80 total train loader: 199 T_Loss: 0.422 | T_Acc: 0.838 | T_f1: 0.838 | T_recall: 0.838 || V_Loss: 0.485 | V_Acc: 0.809 V_f1: 0.825 | V_recall: 0.825\n",
      "epoch: 3 batch: 100 total train loader: 199 T_Loss: 0.423 | T_Acc: 0.825 | T_f1: 0.825 | T_recall: 0.825 || V_Loss: 0.481 | V_Acc: 0.812 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 3 batch: 120 total train loader: 199 T_Loss: 0.422 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.478 | V_Acc: 0.813 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 3 batch: 140 total train loader: 199 T_Loss: 0.422 | T_Acc: 0.823 | T_f1: 0.823 | T_recall: 0.823 || V_Loss: 0.475 | V_Acc: 0.815 V_f1: 0.818 | V_recall: 0.818\n",
      "epoch: 3 batch: 160 total train loader: 199 T_Loss: 0.422 | T_Acc: 0.805 | T_f1: 0.805 | T_recall: 0.805 || V_Loss: 0.472 | V_Acc: 0.816 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 3 batch: 180 total train loader: 199 T_Loss: 0.424 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.471 | V_Acc: 0.817 V_f1: 0.807 | V_recall: 0.807\n",
      "epoch: 4 batch: 0 total train loader: 199 T_Loss: 0.344 | T_Acc: 0.882 | T_f1: 0.882 | T_recall: 0.882 || V_Loss: 0.482 | V_Acc: 0.819 V_f1: 0.815 | V_recall: 0.815\n",
      "epoch: 4 batch: 20 total train loader: 199 T_Loss: 0.390 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.471 | V_Acc: 0.819 V_f1: 0.833 | V_recall: 0.833\n",
      "epoch: 4 batch: 40 total train loader: 199 T_Loss: 0.393 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.468 | V_Acc: 0.819 V_f1: 0.828 | V_recall: 0.828\n",
      "epoch: 4 batch: 60 total train loader: 199 T_Loss: 0.399 | T_Acc: 0.858 | T_f1: 0.858 | T_recall: 0.858 || V_Loss: 0.466 | V_Acc: 0.821 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 4 batch: 80 total train loader: 199 T_Loss: 0.402 | T_Acc: 0.843 | T_f1: 0.843 | T_recall: 0.843 || V_Loss: 0.466 | V_Acc: 0.820 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 4 batch: 100 total train loader: 199 T_Loss: 0.405 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.464 | V_Acc: 0.821 V_f1: 0.782 | V_recall: 0.782\n",
      "epoch: 4 batch: 120 total train loader: 199 T_Loss: 0.409 | T_Acc: 0.838 | T_f1: 0.838 | T_recall: 0.838 || V_Loss: 0.462 | V_Acc: 0.821 V_f1: 0.815 | V_recall: 0.815\n",
      "epoch: 4 batch: 140 total train loader: 199 T_Loss: 0.412 | T_Acc: 0.858 | T_f1: 0.858 | T_recall: 0.858 || V_Loss: 0.461 | V_Acc: 0.822 V_f1: 0.823 | V_recall: 0.823\n",
      "epoch: 4 batch: 160 total train loader: 199 T_Loss: 0.412 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.460 | V_Acc: 0.822 V_f1: 0.815 | V_recall: 0.815\n",
      "epoch: 4 batch: 180 total train loader: 199 T_Loss: 0.413 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.459 | V_Acc: 0.822 V_f1: 0.828 | V_recall: 0.828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 batch: 0 total train loader: 199 T_Loss: 0.364 | T_Acc: 0.875 | T_f1: 0.875 | T_recall: 0.875 || V_Loss: 0.468 | V_Acc: 0.823 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 5 batch: 20 total train loader: 199 T_Loss: 0.370 | T_Acc: 0.848 | T_f1: 0.848 | T_recall: 0.848 || V_Loss: 0.460 | V_Acc: 0.826 V_f1: 0.818 | V_recall: 0.818\n",
      "epoch: 5 batch: 40 total train loader: 199 T_Loss: 0.386 | T_Acc: 0.823 | T_f1: 0.823 | T_recall: 0.823 || V_Loss: 0.457 | V_Acc: 0.825 V_f1: 0.828 | V_recall: 0.828\n",
      "epoch: 5 batch: 60 total train loader: 199 T_Loss: 0.391 | T_Acc: 0.815 | T_f1: 0.815 | T_recall: 0.815 || V_Loss: 0.457 | V_Acc: 0.825 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 5 batch: 80 total train loader: 199 T_Loss: 0.390 | T_Acc: 0.863 | T_f1: 0.863 | T_recall: 0.863 || V_Loss: 0.455 | V_Acc: 0.826 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 5 batch: 100 total train loader: 199 T_Loss: 0.393 | T_Acc: 0.828 | T_f1: 0.828 | T_recall: 0.828 || V_Loss: 0.454 | V_Acc: 0.825 V_f1: 0.785 | V_recall: 0.785\n",
      "epoch: 5 batch: 120 total train loader: 199 T_Loss: 0.397 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.453 | V_Acc: 0.825 V_f1: 0.802 | V_recall: 0.802\n",
      "epoch: 5 batch: 140 total train loader: 199 T_Loss: 0.400 | T_Acc: 0.823 | T_f1: 0.823 | T_recall: 0.823 || V_Loss: 0.452 | V_Acc: 0.825 V_f1: 0.777 | V_recall: 0.777\n",
      "epoch: 5 batch: 160 total train loader: 199 T_Loss: 0.403 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.452 | V_Acc: 0.826 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 5 batch: 180 total train loader: 199 T_Loss: 0.404 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.451 | V_Acc: 0.826 V_f1: 0.812 | V_recall: 0.812\n",
      "epoch: 6 batch: 0 total train loader: 199 T_Loss: 0.383 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.449 | V_Acc: 0.828 V_f1: 0.848 | V_recall: 0.848\n",
      "epoch: 6 batch: 20 total train loader: 199 T_Loss: 0.368 | T_Acc: 0.863 | T_f1: 0.863 | T_recall: 0.863 || V_Loss: 0.452 | V_Acc: 0.826 V_f1: 0.767 | V_recall: 0.767\n",
      "epoch: 6 batch: 40 total train loader: 199 T_Loss: 0.371 | T_Acc: 0.825 | T_f1: 0.825 | T_recall: 0.825 || V_Loss: 0.449 | V_Acc: 0.827 V_f1: 0.835 | V_recall: 0.835\n",
      "epoch: 6 batch: 60 total train loader: 199 T_Loss: 0.378 | T_Acc: 0.875 | T_f1: 0.875 | T_recall: 0.875 || V_Loss: 0.449 | V_Acc: 0.826 V_f1: 0.835 | V_recall: 0.835\n",
      "epoch: 6 batch: 80 total train loader: 199 T_Loss: 0.384 | T_Acc: 0.825 | T_f1: 0.825 | T_recall: 0.825 || V_Loss: 0.447 | V_Acc: 0.827 V_f1: 0.825 | V_recall: 0.825\n",
      "epoch: 6 batch: 100 total train loader: 199 T_Loss: 0.386 | T_Acc: 0.843 | T_f1: 0.843 | T_recall: 0.843 || V_Loss: 0.445 | V_Acc: 0.828 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 6 batch: 120 total train loader: 199 T_Loss: 0.388 | T_Acc: 0.848 | T_f1: 0.848 | T_recall: 0.848 || V_Loss: 0.444 | V_Acc: 0.828 V_f1: 0.835 | V_recall: 0.835\n",
      "epoch: 6 batch: 140 total train loader: 199 T_Loss: 0.389 | T_Acc: 0.823 | T_f1: 0.823 | T_recall: 0.823 || V_Loss: 0.444 | V_Acc: 0.828 V_f1: 0.848 | V_recall: 0.848\n",
      "epoch: 6 batch: 160 total train loader: 199 T_Loss: 0.391 | T_Acc: 0.838 | T_f1: 0.838 | T_recall: 0.838 || V_Loss: 0.444 | V_Acc: 0.828 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 6 batch: 180 total train loader: 199 T_Loss: 0.394 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.443 | V_Acc: 0.829 V_f1: 0.825 | V_recall: 0.825\n",
      "epoch: 7 batch: 0 total train loader: 199 T_Loss: 0.302 | T_Acc: 0.895 | T_f1: 0.895 | T_recall: 0.895 || V_Loss: 0.436 | V_Acc: 0.828 V_f1: 0.825 | V_recall: 0.825\n",
      "epoch: 7 batch: 20 total train loader: 199 T_Loss: 0.359 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.437 | V_Acc: 0.829 V_f1: 0.828 | V_recall: 0.828\n",
      "epoch: 7 batch: 40 total train loader: 199 T_Loss: 0.364 | T_Acc: 0.838 | T_f1: 0.838 | T_recall: 0.838 || V_Loss: 0.436 | V_Acc: 0.829 V_f1: 0.818 | V_recall: 0.818\n",
      "epoch: 7 batch: 60 total train loader: 199 T_Loss: 0.366 | T_Acc: 0.838 | T_f1: 0.838 | T_recall: 0.838 || V_Loss: 0.436 | V_Acc: 0.829 V_f1: 0.823 | V_recall: 0.823\n",
      "epoch: 7 batch: 80 total train loader: 199 T_Loss: 0.370 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.435 | V_Acc: 0.830 V_f1: 0.805 | V_recall: 0.805\n",
      "epoch: 7 batch: 100 total train loader: 199 T_Loss: 0.373 | T_Acc: 0.858 | T_f1: 0.858 | T_recall: 0.858 || V_Loss: 0.435 | V_Acc: 0.830 V_f1: 0.802 | V_recall: 0.802\n",
      "epoch: 7 batch: 120 total train loader: 199 T_Loss: 0.376 | T_Acc: 0.863 | T_f1: 0.863 | T_recall: 0.863 || V_Loss: 0.436 | V_Acc: 0.829 V_f1: 0.825 | V_recall: 0.825\n",
      "epoch: 7 batch: 140 total train loader: 199 T_Loss: 0.380 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.435 | V_Acc: 0.829 V_f1: 0.815 | V_recall: 0.815\n",
      "epoch: 7 batch: 160 total train loader: 199 T_Loss: 0.381 | T_Acc: 0.828 | T_f1: 0.828 | T_recall: 0.828 || V_Loss: 0.436 | V_Acc: 0.830 V_f1: 0.818 | V_recall: 0.818\n",
      "epoch: 7 batch: 180 total train loader: 199 T_Loss: 0.383 | T_Acc: 0.868 | T_f1: 0.868 | T_recall: 0.868 || V_Loss: 0.435 | V_Acc: 0.830 V_f1: 0.853 | V_recall: 0.853\n",
      "epoch: 8 batch: 0 total train loader: 199 T_Loss: 0.367 | T_Acc: 0.882 | T_f1: 0.882 | T_recall: 0.882 || V_Loss: 0.431 | V_Acc: 0.833 V_f1: 0.812 | V_recall: 0.812\n",
      "epoch: 8 batch: 20 total train loader: 199 T_Loss: 0.356 | T_Acc: 0.877 | T_f1: 0.877 | T_recall: 0.877 || V_Loss: 0.432 | V_Acc: 0.833 V_f1: 0.815 | V_recall: 0.815\n",
      "epoch: 8 batch: 40 total train loader: 199 T_Loss: 0.359 | T_Acc: 0.873 | T_f1: 0.873 | T_recall: 0.873 || V_Loss: 0.430 | V_Acc: 0.835 V_f1: 0.845 | V_recall: 0.845\n",
      "epoch: 8 batch: 60 total train loader: 199 T_Loss: 0.364 | T_Acc: 0.882 | T_f1: 0.882 | T_recall: 0.882 || V_Loss: 0.428 | V_Acc: 0.836 V_f1: 0.835 | V_recall: 0.835\n",
      "epoch: 8 batch: 80 total train loader: 199 T_Loss: 0.363 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.428 | V_Acc: 0.835 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 8 batch: 100 total train loader: 199 T_Loss: 0.368 | T_Acc: 0.833 | T_f1: 0.833 | T_recall: 0.833 || V_Loss: 0.428 | V_Acc: 0.835 V_f1: 0.828 | V_recall: 0.828\n",
      "epoch: 8 batch: 120 total train loader: 199 T_Loss: 0.370 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.428 | V_Acc: 0.835 V_f1: 0.843 | V_recall: 0.843\n",
      "epoch: 8 batch: 140 total train loader: 199 T_Loss: 0.371 | T_Acc: 0.868 | T_f1: 0.868 | T_recall: 0.868 || V_Loss: 0.427 | V_Acc: 0.835 V_f1: 0.843 | V_recall: 0.843\n",
      "epoch: 8 batch: 160 total train loader: 199 T_Loss: 0.373 | T_Acc: 0.855 | T_f1: 0.855 | T_recall: 0.855 || V_Loss: 0.427 | V_Acc: 0.835 V_f1: 0.858 | V_recall: 0.858\n",
      "epoch: 8 batch: 180 total train loader: 199 T_Loss: 0.376 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.426 | V_Acc: 0.835 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 9 batch: 0 total train loader: 199 T_Loss: 0.402 | T_Acc: 0.833 | T_f1: 0.833 | T_recall: 0.833 || V_Loss: 0.419 | V_Acc: 0.838 V_f1: 0.848 | V_recall: 0.848\n",
      "epoch: 9 batch: 20 total train loader: 199 T_Loss: 0.350 | T_Acc: 0.873 | T_f1: 0.873 | T_recall: 0.873 || V_Loss: 0.423 | V_Acc: 0.837 V_f1: 0.818 | V_recall: 0.818\n",
      "epoch: 9 batch: 40 total train loader: 199 T_Loss: 0.357 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.424 | V_Acc: 0.835 V_f1: 0.825 | V_recall: 0.825\n",
      "epoch: 9 batch: 60 total train loader: 199 T_Loss: 0.361 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.424 | V_Acc: 0.836 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 9 batch: 80 total train loader: 199 T_Loss: 0.364 | T_Acc: 0.863 | T_f1: 0.863 | T_recall: 0.863 || V_Loss: 0.423 | V_Acc: 0.836 V_f1: 0.853 | V_recall: 0.853\n",
      "epoch: 9 batch: 100 total train loader: 199 T_Loss: 0.363 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.422 | V_Acc: 0.836 V_f1: 0.823 | V_recall: 0.823\n",
      "epoch: 9 batch: 120 total train loader: 199 T_Loss: 0.364 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.421 | V_Acc: 0.837 V_f1: 0.828 | V_recall: 0.828\n",
      "epoch: 9 batch: 140 total train loader: 199 T_Loss: 0.366 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.420 | V_Acc: 0.837 V_f1: 0.873 | V_recall: 0.873\n",
      "epoch: 9 batch: 160 total train loader: 199 T_Loss: 0.368 | T_Acc: 0.843 | T_f1: 0.843 | T_recall: 0.843 || V_Loss: 0.420 | V_Acc: 0.838 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 9 batch: 180 total train loader: 199 T_Loss: 0.369 | T_Acc: 0.843 | T_f1: 0.843 | T_recall: 0.843 || V_Loss: 0.420 | V_Acc: 0.838 V_f1: 0.863 | V_recall: 0.863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 batch: 0 total train loader: 199 T_Loss: 0.291 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.413 | V_Acc: 0.838 V_f1: 0.853 | V_recall: 0.853\n",
      "epoch: 10 batch: 20 total train loader: 199 T_Loss: 0.343 | T_Acc: 0.895 | T_f1: 0.895 | T_recall: 0.895 || V_Loss: 0.416 | V_Acc: 0.837 V_f1: 0.843 | V_recall: 0.843\n",
      "epoch: 10 batch: 40 total train loader: 199 T_Loss: 0.352 | T_Acc: 0.815 | T_f1: 0.815 | T_recall: 0.815 || V_Loss: 0.415 | V_Acc: 0.838 V_f1: 0.838 | V_recall: 0.838\n",
      "epoch: 10 batch: 60 total train loader: 199 T_Loss: 0.355 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.415 | V_Acc: 0.838 V_f1: 0.833 | V_recall: 0.833\n",
      "epoch: 10 batch: 80 total train loader: 199 T_Loss: 0.357 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.414 | V_Acc: 0.838 V_f1: 0.858 | V_recall: 0.858\n",
      "epoch: 10 batch: 100 total train loader: 199 T_Loss: 0.361 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.414 | V_Acc: 0.839 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 10 batch: 120 total train loader: 199 T_Loss: 0.363 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.413 | V_Acc: 0.839 V_f1: 0.858 | V_recall: 0.858\n",
      "epoch: 10 batch: 140 total train loader: 199 T_Loss: 0.367 | T_Acc: 0.855 | T_f1: 0.855 | T_recall: 0.855 || V_Loss: 0.413 | V_Acc: 0.840 V_f1: 0.855 | V_recall: 0.855\n",
      "epoch: 10 batch: 160 total train loader: 199 T_Loss: 0.370 | T_Acc: 0.835 | T_f1: 0.835 | T_recall: 0.835 || V_Loss: 0.412 | V_Acc: 0.840 V_f1: 0.823 | V_recall: 0.823\n",
      "epoch: 10 batch: 180 total train loader: 199 T_Loss: 0.370 | T_Acc: 0.833 | T_f1: 0.833 | T_recall: 0.833 || V_Loss: 0.413 | V_Acc: 0.840 V_f1: 0.845 | V_recall: 0.845\n",
      "epoch: 11 batch: 0 total train loader: 199 T_Loss: 0.391 | T_Acc: 0.863 | T_f1: 0.863 | T_recall: 0.863 || V_Loss: 0.417 | V_Acc: 0.837 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 11 batch: 20 total train loader: 199 T_Loss: 0.348 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.415 | V_Acc: 0.839 V_f1: 0.807 | V_recall: 0.807\n",
      "epoch: 11 batch: 40 total train loader: 199 T_Loss: 0.351 | T_Acc: 0.900 | T_f1: 0.900 | T_recall: 0.900 || V_Loss: 0.413 | V_Acc: 0.840 V_f1: 0.838 | V_recall: 0.838\n",
      "epoch: 11 batch: 60 total train loader: 199 T_Loss: 0.351 | T_Acc: 0.845 | T_f1: 0.845 | T_recall: 0.845 || V_Loss: 0.412 | V_Acc: 0.840 V_f1: 0.855 | V_recall: 0.855\n",
      "epoch: 11 batch: 80 total train loader: 199 T_Loss: 0.354 | T_Acc: 0.887 | T_f1: 0.887 | T_recall: 0.887 || V_Loss: 0.411 | V_Acc: 0.840 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 11 batch: 100 total train loader: 199 T_Loss: 0.359 | T_Acc: 0.848 | T_f1: 0.848 | T_recall: 0.848 || V_Loss: 0.412 | V_Acc: 0.841 V_f1: 0.863 | V_recall: 0.863\n",
      "epoch: 11 batch: 120 total train loader: 199 T_Loss: 0.361 | T_Acc: 0.825 | T_f1: 0.825 | T_recall: 0.825 || V_Loss: 0.411 | V_Acc: 0.841 V_f1: 0.900 | V_recall: 0.900\n",
      "epoch: 11 batch: 140 total train loader: 199 T_Loss: 0.364 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.410 | V_Acc: 0.841 V_f1: 0.863 | V_recall: 0.863\n",
      "epoch: 11 batch: 160 total train loader: 199 T_Loss: 0.366 | T_Acc: 0.873 | T_f1: 0.873 | T_recall: 0.873 || V_Loss: 0.410 | V_Acc: 0.841 V_f1: 0.828 | V_recall: 0.828\n",
      "epoch: 11 batch: 180 total train loader: 199 T_Loss: 0.369 | T_Acc: 0.868 | T_f1: 0.868 | T_recall: 0.868 || V_Loss: 0.410 | V_Acc: 0.841 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 12 batch: 0 total train loader: 199 T_Loss: 0.343 | T_Acc: 0.865 | T_f1: 0.865 | T_recall: 0.865 || V_Loss: 0.402 | V_Acc: 0.847 V_f1: 0.812 | V_recall: 0.812\n",
      "epoch: 12 batch: 20 total train loader: 199 T_Loss: 0.355 | T_Acc: 0.863 | T_f1: 0.863 | T_recall: 0.863 || V_Loss: 0.407 | V_Acc: 0.843 V_f1: 0.853 | V_recall: 0.853\n",
      "epoch: 12 batch: 40 total train loader: 199 T_Loss: 0.360 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.408 | V_Acc: 0.842 V_f1: 0.853 | V_recall: 0.853\n",
      "epoch: 12 batch: 60 total train loader: 199 T_Loss: 0.363 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.408 | V_Acc: 0.842 V_f1: 0.838 | V_recall: 0.838\n",
      "epoch: 12 batch: 80 total train loader: 199 T_Loss: 0.367 | T_Acc: 0.858 | T_f1: 0.858 | T_recall: 0.858 || V_Loss: 0.407 | V_Acc: 0.843 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 12 batch: 100 total train loader: 199 T_Loss: 0.368 | T_Acc: 0.863 | T_f1: 0.863 | T_recall: 0.863 || V_Loss: 0.407 | V_Acc: 0.843 V_f1: 0.848 | V_recall: 0.848\n",
      "epoch: 12 batch: 120 total train loader: 199 T_Loss: 0.371 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.406 | V_Acc: 0.843 V_f1: 0.865 | V_recall: 0.865\n",
      "epoch: 12 batch: 140 total train loader: 199 T_Loss: 0.371 | T_Acc: 0.858 | T_f1: 0.858 | T_recall: 0.858 || V_Loss: 0.406 | V_Acc: 0.843 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 12 batch: 160 total train loader: 199 T_Loss: 0.372 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.405 | V_Acc: 0.843 V_f1: 0.843 | V_recall: 0.843\n",
      "epoch: 12 batch: 180 total train loader: 199 T_Loss: 0.374 | T_Acc: 0.828 | T_f1: 0.828 | T_recall: 0.828 || V_Loss: 0.405 | V_Acc: 0.844 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 13 batch: 0 total train loader: 199 T_Loss: 0.364 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.404 | V_Acc: 0.845 V_f1: 0.843 | V_recall: 0.843\n",
      "epoch: 13 batch: 20 total train loader: 199 T_Loss: 0.366 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.405 | V_Acc: 0.843 V_f1: 0.843 | V_recall: 0.843\n",
      "epoch: 13 batch: 40 total train loader: 199 T_Loss: 0.369 | T_Acc: 0.833 | T_f1: 0.833 | T_recall: 0.833 || V_Loss: 0.406 | V_Acc: 0.842 V_f1: 0.823 | V_recall: 0.823\n",
      "epoch: 13 batch: 60 total train loader: 199 T_Loss: 0.364 | T_Acc: 0.855 | T_f1: 0.855 | T_recall: 0.855 || V_Loss: 0.404 | V_Acc: 0.843 V_f1: 0.823 | V_recall: 0.823\n",
      "epoch: 13 batch: 80 total train loader: 199 T_Loss: 0.367 | T_Acc: 0.845 | T_f1: 0.845 | T_recall: 0.845 || V_Loss: 0.404 | V_Acc: 0.844 V_f1: 0.848 | V_recall: 0.848\n",
      "epoch: 13 batch: 100 total train loader: 199 T_Loss: 0.371 | T_Acc: 0.885 | T_f1: 0.885 | T_recall: 0.885 || V_Loss: 0.403 | V_Acc: 0.844 V_f1: 0.835 | V_recall: 0.835\n",
      "epoch: 13 batch: 120 total train loader: 199 T_Loss: 0.374 | T_Acc: 0.807 | T_f1: 0.807 | T_recall: 0.807 || V_Loss: 0.403 | V_Acc: 0.844 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 13 batch: 140 total train loader: 199 T_Loss: 0.375 | T_Acc: 0.855 | T_f1: 0.855 | T_recall: 0.855 || V_Loss: 0.403 | V_Acc: 0.844 V_f1: 0.853 | V_recall: 0.853\n",
      "epoch: 13 batch: 160 total train loader: 199 T_Loss: 0.377 | T_Acc: 0.863 | T_f1: 0.863 | T_recall: 0.863 || V_Loss: 0.403 | V_Acc: 0.844 V_f1: 0.855 | V_recall: 0.855\n",
      "epoch: 13 batch: 180 total train loader: 199 T_Loss: 0.378 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.403 | V_Acc: 0.844 V_f1: 0.855 | V_recall: 0.855\n",
      "epoch: 14 batch: 0 total train loader: 199 T_Loss: 0.304 | T_Acc: 0.895 | T_f1: 0.895 | T_recall: 0.895 || V_Loss: 0.422 | V_Acc: 0.838 V_f1: 0.855 | V_recall: 0.855\n",
      "epoch: 14 batch: 20 total train loader: 199 T_Loss: 0.372 | T_Acc: 0.833 | T_f1: 0.833 | T_recall: 0.833 || V_Loss: 0.414 | V_Acc: 0.841 V_f1: 0.807 | V_recall: 0.807\n",
      "epoch: 14 batch: 40 total train loader: 199 T_Loss: 0.369 | T_Acc: 0.838 | T_f1: 0.838 | T_recall: 0.838 || V_Loss: 0.410 | V_Acc: 0.842 V_f1: 0.825 | V_recall: 0.825\n",
      "epoch: 14 batch: 60 total train loader: 199 T_Loss: 0.372 | T_Acc: 0.833 | T_f1: 0.833 | T_recall: 0.833 || V_Loss: 0.408 | V_Acc: 0.842 V_f1: 0.823 | V_recall: 0.823\n",
      "epoch: 14 batch: 80 total train loader: 199 T_Loss: 0.375 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.407 | V_Acc: 0.842 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 14 batch: 100 total train loader: 199 T_Loss: 0.375 | T_Acc: 0.853 | T_f1: 0.853 | T_recall: 0.853 || V_Loss: 0.406 | V_Acc: 0.843 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 14 batch: 120 total train loader: 199 T_Loss: 0.378 | T_Acc: 0.823 | T_f1: 0.823 | T_recall: 0.823 || V_Loss: 0.406 | V_Acc: 0.842 V_f1: 0.868 | V_recall: 0.868\n",
      "epoch: 14 batch: 140 total train loader: 199 T_Loss: 0.380 | T_Acc: 0.845 | T_f1: 0.845 | T_recall: 0.845 || V_Loss: 0.405 | V_Acc: 0.843 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 14 batch: 160 total train loader: 199 T_Loss: 0.380 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.405 | V_Acc: 0.843 V_f1: 0.858 | V_recall: 0.858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 batch: 180 total train loader: 199 T_Loss: 0.382 | T_Acc: 0.825 | T_f1: 0.825 | T_recall: 0.825 || V_Loss: 0.404 | V_Acc: 0.843 V_f1: 0.818 | V_recall: 0.818\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "cnn_train_result = pd.DataFrame()\n",
    "cnn_valid_result = pd.DataFrame()\n",
    "for epoch in range(epochs):\n",
    "    train_result, valid_result = train_cnn(epoch, model_cnn)\n",
    "    cnn_train_result = cnn_train_result.append(train_result, ignore_index=True)\n",
    "    cnn_valid_result = cnn_valid_result.append(train_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_result['roll_180_acc'] = cnn_train_result['acc'].rolling(180).mean()\n",
    "cnn_valid_result['roll_180_acc'] = cnn_valid_result['acc'].rolling(180).mean()\n",
    "cnn_train_result['roll_180_loss'] = cnn_train_result['loss'].rolling(180).mean()\n",
    "cnn_valid_result['roll_180_loss'] = cnn_valid_result['loss'].rolling(180).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_result.to_csv('../result/CNN/cnn_train_result.csv')\n",
    "cnn_valid_result.to_csv('../result/CNN/cnn_valid_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>roll_730_acc</th>\n",
       "      <th>roll_730_loss</th>\n",
       "      <th>roll_180_acc</th>\n",
       "      <th>roll_180_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.283019</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.249004</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.168219</td>\n",
       "      <td>0.5225</td>\n",
       "      <td>0.5225</td>\n",
       "      <td>0.5225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.139716</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.082650</td>\n",
       "      <td>0.7175</td>\n",
       "      <td>0.7175</td>\n",
       "      <td>0.7175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  batch      loss     acc      f1  recall  roll_730_acc  \\\n",
       "0      0      0  1.283019  0.6800  0.6800  0.6800           NaN   \n",
       "1      0      1  1.249004  0.4100  0.4100  0.4100           NaN   \n",
       "2      0      2  1.168219  0.5225  0.5225  0.5225           NaN   \n",
       "3      0      3  1.139716  0.5975  0.5975  0.5975           NaN   \n",
       "4      0      4  1.082650  0.7175  0.7175  0.7175           NaN   \n",
       "\n",
       "   roll_730_loss  roll_180_acc  roll_180_loss  \n",
       "0            NaN           NaN            NaN  \n",
       "1            NaN           NaN            NaN  \n",
       "2            NaN           NaN            NaN  \n",
       "3            NaN           NaN            NaN  \n",
       "4            NaN           NaN            NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_valid_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch             14.000000\n",
       "batch            198.000000\n",
       "loss               0.383695\n",
       "acc                0.827500\n",
       "f1                 0.827500\n",
       "recall             0.827500\n",
       "roll_730_acc       0.851836\n",
       "roll_730_loss      0.369081\n",
       "roll_180_acc       0.848431\n",
       "roll_180_loss      0.376631\n",
       "Name: 2984, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_train_result.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step13: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model):\n",
    "    lr = 0.001  # learning rate\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    total_batch = 0\n",
    "    result_dict = {}\n",
    "    epoch_list = []\n",
    "    batch_list = []\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    \n",
    "    for batch_idx, (trains, labels) in enumerate(test_loader):\n",
    "\n",
    "        outputs = model(trains.long()[None, ...])\n",
    "        \n",
    "        true = labels.data.cpu()\n",
    "        predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "        test_acc = metrics.accuracy_score(true, predic)\n",
    "        test_loss = F.cross_entropy(outputs, labels.long())\n",
    "        test_f1 = metrics.f1_score(true, predic, average='micro')\n",
    "        test_recall = metrics.recall_score(true, predic, average='micro')\n",
    "        \n",
    "        epoch_list.append(epoch)\n",
    "        batch_list.append(batch_idx)\n",
    "        loss_list.append(test_loss.item())\n",
    "        acc_list.append(test_acc)\n",
    "        recall_list.append(test_recall)\n",
    "        f1_list.append(test_f1)\n",
    "            \n",
    "        if total_batch % 50 == 0 :\n",
    "            print('epoch: {}'.format(epoch), 'batch: {}'.format(batch_idx), \n",
    "                  'total train loader: {}'.format(len(test_loader)),\n",
    "                  'Loss: %.3f | Acc: %.3f |' % (test_loss, test_acc),\n",
    "                  'F1: %.3f | Recall: %.3f ' % (test_f1, test_recall))\n",
    "        \n",
    "#         total_batch += 1\n",
    "        \n",
    "    result_dict['epoch'] = epoch_list\n",
    "    result_dict['batch'] = batch_list\n",
    "    result_dict['loss'] = loss_list\n",
    "    result_dict['acc'] = acc_list\n",
    "    result_dict['f1'] = f1_list\n",
    "    result_dict['recall'] = recall_list\n",
    "    \n",
    "    return pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 total train loader: 25 Loss: 0.350 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 1 total train loader: 25 Loss: 0.452 | Acc: 0.825 | F1: 0.825 | Recall: 0.825 \n",
      "epoch: 0 batch: 2 total train loader: 25 Loss: 0.409 | Acc: 0.845 | F1: 0.845 | Recall: 0.845 \n",
      "epoch: 0 batch: 3 total train loader: 25 Loss: 0.391 | Acc: 0.845 | F1: 0.845 | Recall: 0.845 \n",
      "epoch: 0 batch: 4 total train loader: 25 Loss: 0.449 | Acc: 0.825 | F1: 0.825 | Recall: 0.825 \n",
      "epoch: 0 batch: 5 total train loader: 25 Loss: 0.441 | Acc: 0.833 | F1: 0.833 | Recall: 0.833 \n",
      "epoch: 0 batch: 6 total train loader: 25 Loss: 0.399 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 7 total train loader: 25 Loss: 0.487 | Acc: 0.812 | F1: 0.812 | Recall: 0.812 \n",
      "epoch: 0 batch: 8 total train loader: 25 Loss: 0.418 | Acc: 0.833 | F1: 0.833 | Recall: 0.833 \n",
      "epoch: 0 batch: 9 total train loader: 25 Loss: 0.429 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 10 total train loader: 25 Loss: 0.460 | Acc: 0.818 | F1: 0.818 | Recall: 0.818 \n",
      "epoch: 0 batch: 11 total train loader: 25 Loss: 0.391 | Acc: 0.858 | F1: 0.858 | Recall: 0.858 \n",
      "epoch: 0 batch: 12 total train loader: 25 Loss: 0.430 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 13 total train loader: 25 Loss: 0.406 | Acc: 0.843 | F1: 0.843 | Recall: 0.843 \n",
      "epoch: 0 batch: 14 total train loader: 25 Loss: 0.420 | Acc: 0.833 | F1: 0.833 | Recall: 0.833 \n",
      "epoch: 0 batch: 15 total train loader: 25 Loss: 0.383 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 16 total train loader: 25 Loss: 0.433 | Acc: 0.825 | F1: 0.825 | Recall: 0.825 \n",
      "epoch: 0 batch: 17 total train loader: 25 Loss: 0.446 | Acc: 0.828 | F1: 0.828 | Recall: 0.828 \n",
      "epoch: 0 batch: 18 total train loader: 25 Loss: 0.409 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 19 total train loader: 25 Loss: 0.382 | Acc: 0.848 | F1: 0.848 | Recall: 0.848 \n",
      "epoch: 0 batch: 20 total train loader: 25 Loss: 0.417 | Acc: 0.843 | F1: 0.843 | Recall: 0.843 \n",
      "epoch: 0 batch: 21 total train loader: 25 Loss: 0.377 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 22 total train loader: 25 Loss: 0.405 | Acc: 0.858 | F1: 0.858 | Recall: 0.858 \n",
      "epoch: 0 batch: 23 total train loader: 25 Loss: 0.433 | Acc: 0.845 | F1: 0.845 | Recall: 0.845 \n",
      "epoch: 0 batch: 24 total train loader: 25 Loss: 0.406 | Acc: 0.855 | F1: 0.855 | Recall: 0.855 \n"
     ]
    }
   ],
   "source": [
    "cnn_test_result = pd.DataFrame()\n",
    "cnn_test_result = cnn_test_result.append(test(0, model_cnn), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test_result.to_csv('../result/CNN/cnn_test_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8395"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_test_result['acc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8395"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_test_result['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8395"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_test_result['recall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step14: plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAGfCAYAAAATcNWCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU9b0//tc5s6+ZzJo9ZGEnYV9Exbq3COJttVK5wq0tvV3UK1/rT696W7TVar3Wfttav2prW1vtFdGqeBUporQKggiyGfZAyD6ZyTL7ds7vj2h6cwWSQGZOZvJ6Ph4+yEnOnPPOm5G88jmf8zmCLMsyiIiIiEgRotIFEBEREY1mDGNERERECmIYIyIiIlIQwxgRERGRghjGiIiIiBTEMEZERESkIIYxIsp5qVQKv/vd7/DlL38ZS5YswcKFC/HII48gHo/jl7/8Jc477zx4vd5+r1m0aBG2bdsGALjkkktwxx139Pv63r17cckll2TseyCi3MUwRkQ5b/Xq1di1axf+8Ic/4NVXX8XatWtRX1+Pe+65BwAQDAZx55134kzLLq5fvx6vvvpqpkomolGEYYyIclpjYyPWrVuHBx98EBaLBQBgNBpx33334bLLLgMAXH311Whvb8czzzxz2uOsWrUKP/7xj3Hy5MmM1E1EowfDGBHltP3796O6uhpms7nf510uF6688koAgE6nw6OPPopf//rX2L9//ymPM3v2bNxwww34/ve/j2Qymfa6iWj0YBgjopwmiiIkSRpwv/Hjx+O2227D7bffjnA4fMp9brnlFsiyjF/+8pfDXSYRjWIMY0SU02pra3Hs2DEEg8F+n29ra8O3vvUtRKPRvs/deOONKC8vxwMPPHDKY6nVajz66KN4/vnnsWPHjrTWTUSjB8MYEeU0j8eDxYsX4+677+4LZMFgEKtXr4bNZoNer++3/09+8hNs3rwZJ06cOOXxSktLcc899+BnP/tZ2msnotGBYYyIct4Pf/hDVFdXY+nSpViyZAmuu+46VFdX48c//vHn9rXb7XjooYeQSCROe7xrrrmmb74ZEdG5EuQz3ctNRERERGnFkTEiIiIiBTGMERERESmIYYyIiIhIQQxjRERERApiGCMiIiJSEMMYERERkYLUShdwLjo7Q5AkrsxxrhwOM3y+4MA70jljrzODfc4c9joz2OfMSUevRVFAfr7ptF/P6jAmSTLD2DBhHzOHvc4M9jlz2OvMYJ8zJ9O95mVKIiIiIgUxjBEREREpiGGMiIiISEEMY0REREQKYhgjIiIiUhDDGBEREZGCGMaIiIiIFMQwRkRERKQghjEiIiIiBTGMERERESmIYYyIiIhIQQxjRERERArK6geFExFRekmJOJL+TmjcbgiCoHQ5GSFLEuREAqlgEICMhM+HeEsLkl2dUOflQWWxQuNyIenrgKDVwThhIgSVSumyKYsxjBERjWJyKoXwgTok/T6Yp88ERAGBbR8g+PEuQBARrT8KOZGAxu6AcdIk2Bcvgdpi7XeMhN+PVE83Ej4fRL0e0WNHkejogKASEWtqghQKIRUOQW3Lh2H8BOjLy6HOt0NXWobAh9uhMhohGo2QolGkerphmlIL0WiEqNMhGQhA1GggmkxIeL0Q1GoEtn8AOZlE3kVf+Fwtn/v+ZLkvREqJOKRQGMFdOxE9Xo/IwQOQ4jFonE7oK6qgttuRaG9DaM9uJDs7obJakerpgba4BLqSEqR6AlBZrUh2+hFvaYbKYoEsSUh2dEBbVIy8iy5G3gUXMpjRkAmyLMtKF3G2fL4gJClryx8xXC4LvN6A0mWMCux1ZmRrnz8LDlIshlQwAI3DOajXpAIBiAY9BJUaEIRTjmClgkGE9u5B5NgRJFpbET1xHFI4DJXZArXdDogi4o0nAQCmaTNgqB4LADBPmwZ1vh2xhhPo2vwuAh9sge3yK6ErKUXPlvcQPXwIUjwOQauFoWosUpEw1FYr9FXVEAQB+opKyLIMtc2GZGcnQvv2IuFtR6K9DfHmZujKyqHOz0eysxPq/HxI4TCiDQ2ALEGOxwGVCpAkQBShMhohRSIwTZ0GQaNBaO8eWOeeB43HA62nAFI8jnhTI3q2bYWo0yPhbYecSEBlNkM0mpBoa4Wg0cA4eQqMEydDV1oKUaNBpP4Ykh0dSHjboSsbA+PEidCVlUMQxX5h7pT9lySkeroRa2pCx9o1iLe1wjr/AujHVCBafwyiwQCN0wmVxQpRr4e+fAxUZvPAf6/JJCAIiB47CrXdjqIJFfB6A71/3z09iBw9AkEQ+kJq9PgxaJwuIJWCypoHQauBZdYcCCJnIw1VOv79EEUBDsfp/94Zxihrf3BlI/Y6M0Zqn2VZhhyLQdTrAQCxpibEW5oQPX4c0aNHEDl8CGqHA0mfD1CpYKiqhmPxEuDTUBOtP4bI4UMI7d0NKZ6AIIqINZwAAIhmM+REEhqnE8ZJkyFFwkj19CDe2gopFoUUjUJfPgbGSZMharXQlZZBW1jUO+rlcgEAUuEwABkqo+m030O8rQ2+1/6CVCAA86zZKJ49FQG1CaJGexa9iELUGz7/NUmCIIqQolEIOh3keBwJXwe0hUWALPcFjHhbK4I7dyLWeBIJbxsElRr6yiqorFboy8qhLSyElEhAjicgRSNQmS3QejxDqnNI31MqhXhrCzpeeRlSMAjT1GmfXuJshhQKQdCoEWtogJxMQl89FiqLBaJGAwgCUqEQYo0nIYgqiFot4q0tAACN29M7alldhXBrG1Ld3RDUauirqpEKBCAnE9B6CqCvrOp7L6SCQSR7uiHqDTBUV0Pr9gAqNQAZKoMRaqcL+rKy3roSCejHVCDe0ozI4UOIHq+HFI1BZTRAlmXoSkqhMlsQ3r8PKqsVapsNAKAymWCZPReCWg0pkQAkCaJO1+/vUI7HkQoGoLY7sioUMowNEcPY8BipP7hyEXudGafrsyzLiDc2ItpwHIHt26C25UNts0HtcEDUamGeNqMvKJ0tWZaR9PsROXQQKqsVkYMHEPpkPwAg2elHqrsb6vx8pEIhiHo91HYHjBMnQVdYBFNNLVKhICDLUDud6Nq4Ed2b34GgViPe2gJtURH0lVWwzJ4LlcnUOyJmMvX+MG1qRCoU6h0dajwJCAI0bg+0hYUQdXqo8/PT8gOR7+mhkaIRSIkEovX1SAV6IMfjkGIxqPPt0HoKICcSEI2G3r8vlRqCRoN4UyNMiCNitEE0GM4Ylj8jp1II7tqJWMMJJLu6ICeTkJMJJLu7EW9qhJRIQGO3Q06lIIXDEHQ6mCZNgaDXQeN09f7SYDb3jnxGo9BXVvWONH46Upjw+RA9Xg+1NQ/Jrk5AVEFXXARAQKK9vTcEx6IQtFpAEKHJz4e+ohKWeecBsozIkcMQNBoYxo5D+JP90BUVQ1dWBo27NywrGd4YxoaIYWx48B/TzGGv00dKxBGuq0PC246iWVPh9wWBVAqylIKg0SJy4BN0//1vkGUJ+tJyWGbPQSoUQrK7E8muLkQOHkDC64V98RLkX3p5v0tJUiyG2MneEQ1BrUa8rRWJjg7I0Sg0BYWQkwnEjh9HvK3lHyMfn45cGCdNhqmmFoJKhMblhsqah1RXJ0STGSqjEYJ64Km7siwj0eGF1uVOZwvPCt/TmTGcfZZluW+EsfeXBx/Utvwhz3WLt7Yi4W2HtrgYot6AWONJJLs6oXUXQDQZoXG6IHw66hc9dhSBD7ch4fdDCgV7L19LEmInjkNbVIxkVxdiTY2QgkFonC4YJ0+GaOz9RUNlNEJbWIRYUyOCH+1AKhyCymwBpBQiR49CbbX2jvjqdIi1tCDV0w3zjJlQ59sh6vWQkymEdu+CymJBvLUVapsNot6AvAsX9BvNS0evP8MwRgPiP6aZw16fu88uf8RbWxBvaUasqQk9W97ruwwnGo2IHDwAlTUPspSCymRG0u+DaUot8i76AvSVVaf8rVtOJpHwdcD3+muIHDgA0WiEHItB43Yj1tQIQaWGymSEFI1B7XBAX14OKRKBFI0CoghD9ViozGboyyt6RzUGEbJyAd/TmTEa+izFYkiFgkj6/YieOI5ERwcSHV6kurv75s7ZvnAJVHk2JDv9ELU6aIuLkfT7EGtqhBxPQNBoIEsSEl4v5FgUCb8PcioFc+00SLEY5FQKapsNsYYTiBw9AkNVNfIuXADLvPl9cwOVCGOj418LIsq4ZKAHot6ARGsL4m2tMIybALXV2jvZW6MZ1DIJqUAAsZZmqMxmSKEw/G+9gciRw72/PRcUQFtYBLXNhsJvfQcalxsahwMA4HSa0dERHFK9gloNracAhd/4FgIf7YDK1HspKN7eBtfSG6ArKh56E4ho0ESdDqJOB43d0XcDSTqlAgEEd+1EcPfHME2phcpiSfs5T4dhjIiGTSocRuTIYXRvfgeRQwchxWIQdTpoi4rR/qc/QuPxIHbiONROZ+88qaISmKdP7710Fwgg0eFFvLkJUiSKyJFDCO3bC7U1DwAgJROwXXwpnEu+DG1h4RlHns51PSzLzFl9HxsnTDynYxHRyKSyWJC34CLkLbhI6VIYxoioP1mSEG9ugsbj6btD7rOJ4oIoItnTg8D2D5Ds7ES8pRnJ7m6kgoHeSwDRKLTFJbDMmgPPv9zUe2eY0QhBEPrmlxjGje+9K/DoEYT27kb7c89CZbMh1dXVO9Jlt0PQaKBxOFHx4MNQ2/IV7ggRUXoxjBFlAVmSkGhrRby1FdrCImgLCpDw+XpDUXc3Uj09kKIRqG02yJIEdZ4NsaZGxE6cAFQikEpBSiTgWLQEtksu7TdnKt7WhsjhQ4idbEDC70P02LHPzgqtpwDJnm4kO7sAWYI6Px+JtjboK6tgnjYdeRcuAEQVNHZH7x1gdsdpR6W0BQXQFhQA6B1t+mzESZYkxBoaoC3wnHKZAyKiXMcwRjTCSIk4Eq1tiDacQLylGYmODgR3bAcEAaYpNYger4fKZEa8tQWm2qnQlY+Bxu2GOi8PqZ4AUsEA5FQSxgmT4Lr+a0j6/b13Tmm18L7wZ/R8sAWW2XOgMhgROXYEgW0f9K54bjJBV1wM15evhaagEPHGRiT8vt5J6RWVSHZ2IhUKQtQboHUP3119gihCP2bMsB2PiCjb8G5KGhV36QxEliTIqRSSPh+iJ+oROXIEGocDGrsD2uISpEJB6MvHQIqEEW9thcbphNrh7DcKJMVivaFHrUbC74eo1fYtkPgZl8uCtsYOdG9+B6FP9veuKaTVQpWXh2h9PaRwCMnOTohGI/SV1Z8+NiYfpsk1UDt6F06UYrHeNac8Qx9JkiUJwV07EfhwG5J+P4yTp8A6//wRuWTCueB7OnPY68xgnzOHd1MSpUm8pRld72xCvKUZACDodBDUGmhcLsSbGhE+eBByLAoA0BYUwjr/fMSbmhA5eADx9jYAQKKtDaLB0LdgJwCIGi0ErRYqiwXR+mO9jzCRZYgmU+9jWEym3tuqp8+E2mpFQE6ibdM70FdUwlRTC62noHe+VSQKQ2V176rcJhNUFstpFz0UdTroy8ecVR8EUYRl5qx+E9SJiEhZDGOUlaRYDJGjR5Dq7kb40AFoCwp7H1Tc0QHjxEkwTpiIeHs74i3NCO/fh2jDCdguuhjW8y+AqNNBiseR9PmQCoVgnTcfnhU3QRBFiAbDae/SSwZ6oDJbIAhC7yKcrS1IdndDZTIh1tyEou/e0rtooiBA1OmQikR6n1l3sgHxtjYkvO0wV5Sh9P/7d+iKSzLcMSIiGql4mZJG/PC3nEohtGc3kl2dSPb0IPjRh0j4/L2PedHroSspQaqnB6JeD43ThfCBOsSaGvvWoDLVTIVx/PgRcVfeSO91rmCfM4e9zgz2OXN4mZLoU1I0gp6tW+B/601I4Qg07t4FPdW2fLiuWwrD2HGnfYagfeGiDFdLRER09hjGaESREnEEtm+H//VXoTJbkHf+hbDMOy/nJpgTERF9hmGMFBNrakL4k30IH6hDKhhEor0dcjIBbWEhXNffAPO06UqXSERElHYMY5QxCb8PSb8fgR3bEfjwQ0iRMCxz58E69zwku7qgLS6G1u2BxuVSulQiIqKMYRgbZWRZhhyLItnZCXV+PkS9AXIqhXhrC8J1dZAiYRgnTYHaYYfKbEFw105EjxyGFIv1roHV1QWN04WE34d4SzPMM2dDCoeh9Xigr6oGZAmpULhvRXdRpwPQOwqGVAqiyQjL7DkovuXfoHG7oTKaFO4IERGRshjGcpgUiyF8sA5SJILQ7o+RCoUQPlAHSBJUJjNSwQA0BQU4Fg4j2dMDw7jxUFks6Nr8DpI+HwBAW1QM87TpUOl00BYWQdRqET15AoaxY+G+4Z8R+HAbVAYDUuEwfK+8DDmZhMpqhWHceFhmzoIUjyMVDMBdPQ4ap7N36QciIiLqwzCWY+RUCpFDB5Hs6kTHX15CsrsbuqIimGqmwjJnHlxfXQptQSEElQqpcAhJnw92pwVBfe9K8Z+tKC9LEqRwGKJe/7l1t/L+x8dcL4uIiOjcMIzlCFmWEdi+Dd41f4bKbIHKYoHrq1+DZdbs075GZTRBZTTB6LIg9L/WVBFEESrz6ddEISIiouHBMJbFZElC5OABhPbuQfffN0Ntd6Dwm/8K48RJSpdGREREg8QwlmUSHV7433gdiY4OhOs+ga6kFIZx41F82+3QV1b1e3A1ERERjXwMY1kkXPcJGh/7T6ht+bBdehkKbvrmiHjEDxEREZ09hrEsEG9tgf/NN9Dz/t/h+upS5F/xRaVLIiIiomHCMDYCJbu74V//BiKHDkJOJpH0+2C79DJUPPQINE4uiEpERJRLGMZGgFhzE/z/vQ4qkxkJXwdCuz+GeeYsuK67HnIqBa3HwxBGRESUoxjGFJTw+dD6zNOInWyAZdYcyMkEzDNmwXPjCs4FIyIiGiUYxhQgSxKCH+9C+/N/hO2ii1F82/+BqNEqXRYREREpgGEsg1LBIAI7d6Bj7RqobfnwLLsR5ukzlS6LiIiIFMQwliE9H2xB+3N/hLaoGAVf/yZM06ZzTTAiIiJiGEsnKR5H4MNt6Nm6BfGmJpTedS90xcVKl0VEREQjSFrD2Lp16/DEE08gmUxixYoVWLZsWb+v79+/Hz/4wQ+QSCRQWFiIRx55BFarNZ0lZUwy0IPmX/0CgijCdullMI6fCJXFonRZRERENMKI6TpwW1sbHnvsMTz//PN45ZVX8MILL+DIkSP99nnggQdw66234rXXXkNFRQV++9vfpqucjOr5YAuO330njOMnoOSOu2CZNYdBjIiIiE4pbSNjW7Zswbx582Cz2QAAV155JdavX4+bb765bx9JkhAKhQAAkUgEeXl56SonI2LNzWj5f79Csrsbpf9+L3RFvCRJREREZ5a2MNbe3g6X6x8LlbrdbuzZs6ffPnfddRduuukmPPjggzAYDFizZs2QzuFwmIel1uFw4k/Po+X1N2CfMxvlK26EzmFXuqQhcbk4cpcp7HVmsM+Zw15nBvucOZnuddrCmCRJ/e4WlGW533Y0GsU999yD3//+96itrcXvfvc73HnnnXjqqacGfQ6fLwhJkoe17qGSEgl0vPQiQnt2o+y+B6Cx29EjAfAGFK1rKFwuC7xZVG82Y68zg33OHPY6M9jnzElHr0VROOMAUtrmjBUUFMDr9fZte71euN3uvu1Dhw5Bp9OhtrYWAHD99ddj+/bt6SonLaRoBC1P/hrx1haU3H4HNPbsGg0jIiIi5aUtjM2fPx9bt26F3+9HJBLBhg0bsGDBgr6vl5eXo7W1FceOHQMAvP3226ipqUlXOcNOlmU0/eoXEHU6FH33FmgcTqVLIiIioiyUtsuUHo8Hq1atwvLly5FIJHDttdeitrYWK1euxK233oqamhr85Cc/wW233QZZluFwOPDggw+mq5xh1/P3v0GKRlGw6vsQVCqlyyEiIqIsJciyrOykq3Og1JyxVCCA4z+4ByX/5/vQlZZl/PzDjXMRMoe9zgz2OXPY68xgnzMnp+aM5SopFkPLb56EZe68nAhiREREpCyGsSGQJQmtzzwNUa+H69qvKl0OERER5QCGsSHwvfIyEh0d8Nz4LxDUfKwnERERnTuGsUHq+MtL8L/xOopu/jeozCNnsVkiIiLKbgxjgyDFYuje/C7Kf/gjaPLzlS6HiIiIcgjD2CB0v/c3GMaNg660VOlSiIiIKMcwjA1ATibRuWE98r+4UOlSiIiIKAcxjA0g+PFOaBxOGCqrlC6FiIiIchDD2AC6Nr0N2yWXKl0GERER5SiGsTOIt7Ui3toC87QZSpdCREREOYph7AyCO3fCPHMW1xQjIiKitGEYO4NY40nox1QoXQYRERHlMIaxM0h426B1FyhdBhEREeUwhrHTkGUZ8dY2aIuKlC6FiIiIchjD2GkIgoAx9z8AlcmkdClERESUwxjGzkBtsyldAhEREeU4hjEiIiIiBTGMERERESmIYYyIiIhIQQxjRERERApiGCMiIiJSEMMYERERkYIYxoiIiIgUxDBGREREpCCGMSIiIiIFMYwRERERKYhhjIiIiEhBDGNERERECmIYIyIiIlIQwxgRERGRghjGiIiIiBTEMEZERESkIIYxIiIiIgUxjBEREREpiGGMiIiISEEMY0REREQKYhgjIiIiUhDDGBEREZGCGMaIiIiIFMQwRkRERKQghjEiIiIiBTGMERERESmIYYyIiIhIQQxjRERERApiGCMiIiJSEMMYERERkYIYxoiIiIgUxDBGREREpCCGMSIiIiIFMYwRERERKYhhjIiIiEhBDGNERERECmIYIyIiIlIQwxgRERGRghjGiIiIiBSkTufB161bhyeeeALJZBIrVqzAsmXL+r5WV1eHu+66q2/b7/cjLy8Pr7/+ejpLIiIiIhpR0hbG2tra8Nhjj+Hll1+GVqvF0qVLMXfuXFRXVwMAJk6ciFdffRUAEIlEcN1112H16tXpKuesNLQFUOaxKF0GERER5bC0XabcsmUL5s2bB5vNBqPRiCuvvBLr168/5b5PPvkkZs+ejVmzZqWrnCFLJCU88MePIMuy0qUQERFRDkvbyFh7eztcLlffttvtxp49ez63XyAQwJo1a7Bu3bohn8PhMJ9TjQMRRQFmqwFGvSat5xkJXC6OAGYKe50Z7HPmsNeZwT5nTqZ7nbYwJkkSBEHo25Zlud/2Z1577TVcdtllcDgcQz6HzxeEJKVv5Mqs16D+ZCfcNkPazjESuFwWeL0BpcsYFdjrzGCfM4e9zgz2OXPS0WtRFM44gJS2y5QFBQXwer19216vF263+3P7bdy4EQsXLkxXGefEYtQgEI4rXQYRERHlsLSFsfnz52Pr1q3w+/2IRCLYsGEDFixY0G8fWZaxf/9+TJ8+PV1lnBOLUYtAOKF0GURERJTD0hbGPB4PVq1aheXLl+Oaa67BokWLUFtbi5UrV2Lv3r0Aepez0Gg00Ol06SrjnFiMGgQZxoiIiCiN0rrO2OLFi7F48eJ+n3v66af7PnY4HHj//ffTWcI5MRs0CER4mZKIiIjShyvwn0HvnDGOjBEREVH6MIydQe+cMY6MERERUfowjJ2BzaxFd5BhjIiIiNKHYewM7BY9fD1RpcsgIiKiHMYwdgZ2qx7+QIyPRCIiIqK0YRg7A6NeDQFAOJZUuhQiIiLKUQxjA3BY9fD3xJQug4iIiHIUw9gA8q06+DlvjIiIiNKEYWwAdoueYYyIiIjShmFsAA6rDj5epiQiIqI0YRgbgMtmgLcronQZRERElKMYxgbgsRvR5g8rXQYRERHlKIaxAXjyjWjrjHCtMSIiIkoLhrEBGPVq6DQiuvhYJCIiIkoDhrFB4KVKIiIiSheGsUHw2I1oZRgjIiKiNGAYG4QihwnNvpDSZRAREVEOYhgbhBK3CY3tQaXLICIiohzEMDYIJS4zGr0h3lFJREREw45hbBDyTFoA4B2VRERENOwYxgZBEASUus1o9PJSJREREQ0vhrFB6r1UyTBGREREw4thbJA4iZ+IiIjSgWFskErdZpxs5/IWRERENLwYxgapyGFCW2cYyZSkdClERESUQxjGBkmrUcGZp0erjyvxExER0fBhGBuCMo8Fx1sDSpdBREREOYRhbAgqC6041tKjdBlERESUQxjGhqCyyIpjzd1Kl0FEREQ5hGFsCMo8ZrT6w4glUkqXQkRERDmCYWwINGoVip1mnOC8MSIiIhomDGNDVFlkxVFeqiQiIqJhwjA2RFVFVhxr5iR+IiIiGh4MY0NUyTBGREREw4hhbIhcNgMSSQmdgZjSpRAREVEOYBgbIkEQuMQFERERDRuGsbPQO4mflyqJiIjo3DGMnQXOGyMiIqLhwjB2FioLrTjRFkBKkpQuhYiIiLIcw9hZMOo1sFt0aPKGlC6FiIiIshzD2FmqLOSlSiIiIjp3gwpjra2t2Lx5M1KpFJqbm9NdU1aoLM7jSvxERER0zgYMY++++y6WLl2K++67Dz6fD1dddRU2btyYidpGNI6MERER0XAYMIw9/vjjWLNmDaxWK9xuN55//nn84he/yERtI1qJ2wR/TwzhaELpUoiIiCiLDRjGUqkU3G533/bEiRMhCEJai8oGKlFEuceM+paA0qUQERFRFhswjBkMBjQ3N/cFsB07dkCn06W9sGxQWZTHlfiJiIjonKgH2uH222/HTTfdBK/Xi+uvvx7Hjx/HL3/5y0zUNuJVFlnx3t4WpcsgIiKiLDZgGJsxYwbWrFmDXbt2QZIkTJ06FXa7PRO1jXiVRVY8+9ZByLLMS7dERER0VgYMY/v37wcAOJ1OAEBLSwtaWlowefLk9FaWBexWPdQqAd7uKNw2g9LlEBERURYaMIzdcsstfR8nEgl4vV5MmTIFa9euTWth2aKqKA/HmroZxoiIiOisDBjGNm3a1G9727ZtWLduXdoKyjafPTR83uQCpUshIiKiLDTkxyHNnTu379Il9Yaxo1z8lYiIiM7SoOeMAYAsy9i3bx+i0Whai8omYwqsaOoIIpFMQaNWKV0OERERZZkhzRkTBAF2ux2rV69OZ01ZRadVodBhQn1LAONKbUqXQ0RERFlmyHPG6PMmlNlwoH89t4AAACAASURBVKGTYYyIiIiG7LRh7Mc//vEZX3jvvfcOePB169bhiSeeQDKZxIoVK7Bs2bJ+Xz927Bh++MMforu7Gy6XCz/72c+Ql5c3yNJHjvGl+fjrjpPA+UpXQkRERNnmtBP4bTbbGf8bSFtbGx577DE8//zzeOWVV/DCCy/gyJEjfV+XZRnf+c53sHLlSrz22muYOHEinnrqqeH5rjJsXGkejrX0IJmSlC6FiIiIssxpR8Zuvvnm074oHA4PeOAtW7Zg3rx5fcHtyiuvxPr16/uOu3//fhiNRixYsAAA8O1vfxs9Pdl5V6JRr4En34D6lh6MLeGlSiIiIhq8AeeMbdy4Eb/4xS8QDochyzIkSUJXVxd27dp1xte1t7fD5XL1bbvdbuzZs6dvu6GhAU6nE3fffTfq6upQWVmJ//iP/xhS8Q6HeUj7p9O08W40+iKYP71U6VLOistlUbqEUYO9zgz2OXPY68xgnzMn070eMIz99Kc/xW233YY///nPWLlyJTZu3AiTyTTggSVJ6ve8xv/9/MZkMont27fjT3/6E2pqavDzn/8cDz30EB566KFBF+/zBSFJ8qD3T6dypwmbdjXh4qmFSpcyZC6XBV5vQOkyRgX2OjPY58xhrzODfc6cdPRaFIUzDiANuOirwWDAwoULMW3aNOh0OqxevRrvvvvugCcuKCiA1+vt2/Z6vXC73X3bLpcL5eXlqKmpAQAsWrSo38hZthlbasPRpm7OGyMiIqIhGTCM6XQ6xONxlJWVoa6uDqIo9hvhOp358+dj69at8Pv9iEQi2LBhQ9/8MACYPn06/H4/Dhw4AKB3CY1sfvi42aCBM8+AE638zYWIiIgG77SXKb/3ve9h2bJluOSSS/Ctb30LDz/8MK6//np89NFHyM/PH/DAHo8Hq1atwvLly5FIJHDttdeitrYWK1euxK233oqamho8/vjjuPfeexGJRFBQUICf/vSnw/rNZdpn641VFWff8hxERESkDEGW5VNOunrmmWfw4osvAui9hLhixQo0NDTgww8/xKJFi+BwODJa6KmMpDljALDrkBebdjbi9qXTlS5lSDgXIXPY68xgnzOHvc4M9jlzRtScsZtuuglvvvkm7r//ftTX1+Pyyy/HmjVrMH/+/BERxEaiCeX5ONLcg3gipXQpRERElCUGvJty9uzZmD17Nrq6uvDqq6/izjvvhNlsxrPPPpuJ+rKKQadGmduMQ41dmFLBwEpEREQDG3AC/2e0Wi2MRiNMJhM6OzvTWVNWm1xhx/56v9JlEBERUZYYcGTso48+wtq1a/H2229j/vz5uOWWWzBnzpxM1JaVJlfY8Yc3DyhdBhEREWWJ04axp59+Gi+99BIikQiuu+46vP766/3WCaNTqyiwojMQQ1cwBptZp3Q5RERENMKdNoz9/e9/x2233YbLL78cKpUqkzVlNVEUMLE8H/vr/Ti/JvtW4yciIqLMOm0Y4wT9sze5wo79xxnGiIiIaGCDnsBPgzd5TO8kfunUS7gRERER9WEYSwOnzQCTXsNHIxEREdGAGMbSZGq1A3uO+pQug4iIiEY4hrE0mVbtxMdHOpQug4iIiEY4hrE0qSrOQ0dXBJ2BmNKlEBER0QjGMJYmapWIKZUO7DnK0TEiIiI6PYaxNJpa5cDuI5w3RkRERKfHMJZGNVUOHGjoRDyRUroUIiIiGqEYxtLIpNeg3GNB3Qk+WJ2IiIhOjWEszaZWO7Gbd1USERHRaTCMpdnUagd2H/VB5mr8REREdAoMY2lWYDdCqxbR0BZUuhQiIiIagRjG0kwQBMwY58KOg+1Kl0JEREQjEMNYBsya4MaOg15eqiQiIqLPYRjLgDEFFiSTKTR1hJQuhYiIiEYYhrEMEAQBM8e78dFBr9KlEBER0QjDMJYhs8a7OW+MiIiIPodhLEMqi60IRRJo8fFSJREREf0Dw1iGiIKAmeN4qZKIiIj6YxjLoJnjucQFERER9ccwlkHjSm3oCsTQ3hVRuhQiIiIaIRjGMkgUexeA/YijY0RERPQphrEMmznejR0HGMaIiIioF8NYhk0ot8HXHUVbZ1jpUoiIiGgEYBjLMJUoYvZED7Z90qZ0KURERDQCMIwpYN4kDz7Y38ZnVRIRERHDmBIqi6xISRIa2oJKl0JEREQKYxhTgCAImDupAFv3typdChERESmMYUwh8yZ5sL2uDZLES5VERESjGcOYQoqcJliNWhxs6FS6FCIiIlIQw5iC5k0uwAe8q5KIiGhUYxhT0JyJbuw85EUimVK6FCIiIlIIw5iC7FY9yjwW7DrcoXQpREREpBCGMYVdUFOI9/a2KF0GERERKYRhTGEzxrtQ39yDzkBM6VKIiIhIAQxjCtNpVJg53o0t+zg6RkRENBoxjI0AvZcqW/l4JCIiolGIYWwEqCq2QgBwtKlH6VKIiIgowxjGRgBBEHB+TQHe29usdClERESUYQxjI8T8KYXYccCLWJxrjhEREY0mDGMjRL5Fh7EledhexxX5iYiIRhOGsRHk4hkl2LSrSekyiIiIKIMYxkaQKZV2hCIJ1LdwIj8REdFowTA2goiCgIunF2PTzkalSyEiIqIMYRgbYS6oLcSuQx0IRhJKl0JEREQZwDA2wliMWkytduK9PVyRn4iIaDRgGBuBLplRjHd3NUHiivxEREQ5j2FsBKosskKvU+GTer/SpRAREVGapTWMrVu3DgsXLsQVV1yB55577nNf/9WvfoWLL74YS5YswZIlS065z2gkCAIumVGCjR9xIj8REVGuU6frwG1tbXjsscfw8ssvQ6vVYunSpZg7dy6qq6v79tm3bx9+9rOfYfr06ekqI2vNm+TBy5uPosUXQqHDpHQ5RERElCZpGxnbsmUL5s2bB5vNBqPRiCuvvBLr16/vt8++ffvw5JNPYvHixbj//vsRi8XSVU7W0WpU+ML0Yvx1B0fHiIiIclnaRsba29vhcrn6tt1uN/bs2dO3HQqFMHHiRNxxxx0oLy/HXXfdhV//+tdYtWrVoM/hcJiHteaR5trLx+M7D2/CN6+pQZ5Zl9ZzuVyWtB6f/oG9zgz2OXPY68xgnzMn071OWxiTJAmCIPRty7Lcb9tkMuHpp5/u277ppptw9913DymM+XxBSFJu33E4vdqJl94+hMXzx6TtHC6XBV5vIG3Hp39grzODfc4c9joz2OfMSUevRVE44wBS2i5TFhQUwOv19m17vV643e6+7ebmZqxdu7ZvW5ZlqNVpy4ZZ64rZpdi0sxGJpKR0KURERJQGaQtj8+fPx9atW+H3+xGJRLBhwwYsWLCg7+t6vR6PPPIITp48CVmW8dxzz+Hyyy9PVzlZq8RtRonThO11bUqXQkRERGmQtjDm8XiwatUqLF++HNdccw0WLVqE2tparFy5Env37oXdbsf999+P73znO/jiF78IWZbx9a9/PV3lZLUr5pThre0NkLkILBERUc4R5Cz+CT8a5owBvZdwf/jMh7j2C1WorXIM+/E5FyFz2OvMYJ8zh73ODPY5c3JqzhgNH0EQ8KV5Zfj9m3WIJVJKl0NERETDiGEsS8ye4IZBp+YDxImIiHIMw1iWUKtE3HTVRLy57QTvrCQiIsohDGNZpKooD8VOM97mMyuJiIhyBsNYlll2xTi88cEJNLYHlS6FiIiIhgHDWJZx2wy44bKx+MVLexAIx5Uuh4iIiM4Rw1gWmje5AFMq7Hhr+0mlSyEiIqJzxDCWpS6bVYr397ZwMj8REVGWYxjLUkVOEyoKrXhre4PSpRAREdE5YBjLYjdcNhYbPjyJ9q6I0qUQERHRWWIYy2JOmwFfnFuGP204yOdWEhERZSmGsSx3xexSdAZi2LKvVelSiIiI6CwwjGU5tUrEtxZPxgubjqDFF1K6HCIiIhoihrEcUOo248sXVeKJV/YhzgeJExERZRWGsRxx0dQiFDlN+PPbh5UuhYiIiIaAYSxHCIKAFV+cgLoTndj2SZvS5RAREdEgMYzlEINOje8smYLn/noIbf6w0uUQERHRIDCM5ZjyAguWXFCBJ17dh0SS88eIiIhGOoaxHHTJjGK48434/Ztcf4yIiGikYxjLQYIg4BtXTUSrP4TX3j+udDlERER0BgxjOUqnUeHWr9Ti/b0t2LKvRelyiIiI6DQYxnJYnlmHf7u2Fi9sOoKDDZ1Kl0NERESnwDCW44pdZnzr6sl44pV9XKGfiIhoBGIYGwUmj7Hjuour8Z//9TEDGRER0QjDMDZKnF9TiGsuqGAgIyIiGmEYxkaRC6cW4csLKvHT53fhWHOP0uUQERERGMZGnfNrCrHiixPw8xd34+MjHUqXQ0RENOoxjI1C08Y68W/X1eIPbx7AXz88yYVhiYiIFMQwNkpVFeXh7htnYsu+Vjz8xx2IxJJKl0RERDQqMYyNYi6bAXffOAMWoxb3/2EHTrYHlS6JiIho1GEYG+U0ahW+d+1UXD1/DB758y688cEJSBIvWxIREWWKWukCaGQ4b0oBxpbm4Zn/rsPHRzrwzasmwp1vVLosIiKinMeRMerjzDPg+1+bjlnjXPjRH3bg1ffqEU+klC6LiIgopzGMUT+iIOCKOWVY/fU5aPQG8R+/3YaPD3MJDCIionThZUo6JUeeHt/7pxrsr/fjub8ewju7mnDdxVUocZlP+5pkSkKLL4yeUBzhWBJ6rQpGnRpGvRomvQYmgxoqkfmfiIjof2IYozOaXGHH/d+Yg007m/Cff96F2ionrrmwAnarHgAgSTLqTnRiy74W7DrcgXyLDjazDkadGtFECuFoAuFoEqFoEvFkCkUOE8o8FpQXWFDmMaPEZYZOo1L4uyQiIlIOwxgNSK0SccXsUlxQU4g3t53AD5/ZjgVTi1DqNuPlvx2DSa/BeVMKcP0lY2E1aU97nGg8icb2EE60BXC8pQd/+7gZLb4QipwmTCzPx4TyfIwtyYNey7clERGNHvypR4Nm1KvxlYuqcMmMErz6Xj1+9+YB3HZtLSaOsQ/q9XqtGtUleaguyev7XCIpob6lB3UnOvHG1hM43hpAqduM6eOcmD3eDafNkK5vh4iIaEQQ5Cx+Fo7PF+SaWMPA5bLA6w0M+XWyLEMQhGGtJZZI4fDJLnx0yIuPDnrhyTfgwqlFmDPRnRMjZmfbaxoa9jlz2OvMYJ8zJx29FkUBDsfp51xn/083UsxwBzEA0GlUmFLpwJRKB5ZdPg77jvnxt93NePGdI5g9wY0F04owpsA67OclIiJSCsMYjVhqlYhpY52YNtaJzkAM7+1pxuMv74PZoMGCaUWYN8kDg45vYSIiym78SUZZId+iw+LzK3DV/DH4pN6Pzbub8dK7RzFjnAsLphWhqsialpE6IiKidGMYo6wiCkLfZczuUBxb9rbgN69/Ao1axBemFWPB1CJo1FzLjIiIsgfDGGWtPJMWX5pXji/OLcOBhi5s2N6AlzYfhTNPD6NOjUXnj8GUCofSZRIREZ0RwxhlPUEQMLE8HxPL8xGKJuDvieFkewBPvfYJ/vmKcZgz0aN0iURERKfFMEY5xaTXwKTXoNRtRpHThKde+wQ7D3nxz1eMh9mgUbo8IiKiz+HkGspZYwqsWP312bCZdfgBH3hOREQjFEfGKKdpNSosvXQspo914pk36vDRwXZ87bKxMOo5SkZERCMDR8ZoVBhflo/7bpoDrVaFe3+zDX/b3cynNxAR0YjAkTEaNfRaNW68YjzmTynAi5uO4K8fnsQ1F1Zi+lgnRJFrlBERkTIYxmjUqSrKw53LZmD3ER/WbanH2neP4Io5ZZg/pQA6jUrp8oiIaJRhGKNRSRAETBvrxNRqBw43dmP9tga88vdj+MK0Ylw0rQh2q17pEomIaJRgGKNRTRAEjCu1YVypDS2+EDZ+1IgfPrMdY0tsuGhaEWoqHbyESUREacUwRvSpQocJN14xHl/9QjW21bXhtffr8ccNB3H+lEJMH+dEmccCkc+/JCKiYcYwRvS/6LQqLJhahAVTi3CiNYCt+1vx1GufIBJLoqbKgfGlNowttcGVpx/0w8llWUZK6v2PiIjof0prGFu3bh2eeOIJJJNJrFixAsuWLTvlfu+++y7uv/9+bNq0KZ3lEA1ZeYEF5QUWLL10LNo7w9h7zI89R31Yu/koAGBsiQ1ji/NQXZKHUrcZalX/1WIisSS27GvFO7ua0OYPw2zUYMZYF+ZN9qC6OG/QYY6IiHJX2sJYW1sbHnvsMbz88svQarVYunQp5s6di+rq6n77dXR04OGHH05XGUTDxp1vxKUzjbh0ZglkWUZHdxSHTnbhSFM3/ranGR1dUYwpsKC6JA9VxXno7IniL3+vx4TyfPzz5eMwvswGSaXC+veP4fdvHoAsA3MneTChzIbKojxo1Fz2j4hoNEpbGNuyZQvmzZsHm80GALjyyiuxfv163Hzzzf32u/fee3HzzTfj0UcfTVcpRMNOEAS4bAa4bAacX1MIAAhHEzjW3IMjTd3YuOMkQpEk7lo2A0VOU9/rChwmXHXeGCycV44jTd3YdbgDL2w6ghZ/GJWFVkwos2F8WT4qi6yfG2UjIqLclLYw1t7eDpfL1bftdruxZ8+efvs8++yzmDRpEqZOnXpW53A4zOdUI/2Dy2VRuoScUF5qx8UD7PNZr91uK+ZPLwUAhCIJfFLvw54jHXhx81E0e4MYX25HbbUT08a5UFVs412dQ8T3dOaw15nBPmdOpnudtjAmSVK/+TCyLPfbPnToEDZs2IDf//73aG1tPatz+HxBPtJmGLhcFni9AaXLGBXO1OsxLhPGuEy4+rxyhKIJHGroQt2JTvx12wkEwglMrrBjSoUdkyvssJl1Ga48u/A9nTnsdWawz5mTjl6LonDGAaS0hbGCggLs2LGjb9vr9cLtdvdtr1+/Hl6vF1/5yleQSCTQ3t6OG264Ac8//3y6SiLKGia9BtPHuTB9XO/osq87iv3H/dh9pAP/9fZh5Ft0mFLhwORKO8aV5EGj5pMDiIiylSDLclqGltra2vC1r30Na9euhcFgwNKlS/GjH/0ItbW1n9u3sbERy5cvH/LdlBwZGx78jStzhqPXKUlCfUsA+475sP+4H03eEMaW2DClwo4plXYU2I1nvEtTlmXsPOSFtyuKsaV5KPdYcm5+Gt/TmcNeZwb7nDk5NTLm8XiwatUqLF++HIlEAtdeey1qa2uxcuVK3HrrraipqUnXqYlymkoUUV2ch+riPFxzYSVC0QTqjndiX70Pb33YAAFAVXEexhRYMebTpTkMut7/1ds7w/jTXw+hsyeGcaU2bNnXAm93FJPK8zF3kgdTq518PicRUYalbWQsE041MpZKJdHZ6UUyGVeoquwjiiIkSer3ObVai/x8F1Qqrgs8nNL9260sy2j1h3GsuQfHWwI43taDxvYQbBYdTHo12vxhfGleOa6YXdo3GhaMJLDrsBfb69pxrLkHU6scmDPJgykV9qwdMeMoQuaw15nBPmeOEiNjORfGOjpaoNcbYTJZuaDmIKnVIpLJf4QxWZYRCvUgGg3D6SxUsLLco8Q/qClJQktHGN2hOCoKrTDqTx+we0JxfHigHdvq2tDkDaKy0IrqEhsqCq0YU2iB1ajNYOVnjz+4Moe9zgz2OXNy6jKlUpLJOEymAgaxcyAIAkwmK4LBLqVLoWGgEkWUuM0oGcS+VpMWl84swaUzSxAIx3G0qQeHm7rw1vYGHG8NwKhTo7LIiqriPFQVW3NyvhkRUablXBgDwCA2DNhDshi1mDbWiWljnQAASZbR3hnB0aZuHG3uwZa9LWjtDKPUbUZVUe8ctqriPORbuOwGEdFQ5GQYI6LhJwoCCuxGFNiNfU8diMaTqG8J4GhTN7bsa8Wzbx2ERi1iQpkNk8YMfk00f08UVpOWo2xENCoxjBHRWdNr1ZhYno+J5fkAeucbtndGUHeiEx9/uiaazazDxDH5GF+aj/FlNpgNmr7XdwdjePW9ery3txUTymz4ykVVKPOYOTJLRKMKw1gaJZNJPProQzh27Cj8fj+qq6uxevUDeOWVl/DKKy9BpVJh/vwL8d3v3orW1hY8+OB96Oz0Q6/X4847/wMmkwm33PKvWLt2HQDgt799EgDwjW/8KxYtugzjx0+Cz9eB3/zm2VOeR6fT44UXnut3rhUrbsJXv7oEa9a8CpPJjJaWZtxxx7/hT396UclWUY4QBAEeuxEeuxFfmF4MSZJxvDWAuhN+bN7dhGfe+AR2ix5lHjNMeg227m/F+TWF+Ol3zsPfdjfj8b/shSyj72kDE8fkw6TXDHxiIqIsxjCWRvv27YFarcGTT/4OkiTh1lu/jRdf/C+8/vqr+M1v/gi9Xo/bb78VBw7U4be//X+46KJL8JWvfBVbt76HP/zht/jud2897bG7urqwbNlyzJgxCx9/vPNz59m69X14PAX4y1/W9jvXyZMncd55F+Cdd97GokVLsH79f2PhwsUZ7AqNJqIooLLIisoiK646r/fOzsb2EBraA/B1R/GDf5kNl80AALj6/Aosnj8Gzb4w9h/z4W97mvHbN+pQ4jJhWrUT06qdKHKaOGpGRDkn58PY8R/cg3hz07AfV1tUjDH3P3DGfaZNmwGrNQ8vvbQGDQ3H0dh4EvF4HOeffyHM5t5bXP/v//01AODjj3di9ere45133gU477wL0NLSfMbjT5485bTniUQi2LVr5ynPddVVV+OZZ57CokVL8Ne/rsfjjz919o0gGgKVKKL804VoT0UQBBQ7TSh2mnDFnDIkkikcPNmFjw934LEXd0OtEjFjrAvTxzlRVZTHh6cTUU7I+TA2UGBKp/fe24zf/OZJXHfdUixceDW6urpgNlsQCoX69uno8EKn0/dbXFWWZRw/Xg+DwYD/uQxcMpmEWv2P/XQ6/WnPI8vyp/sKnzvXtGkz4PV6sXnzJhQWFsPlcvVbZ4xopNCoVZhS4cCUCgeWXT4ODW1B7DzkxR/fOoSeUAxTq52YPs6FieX5fHIAEZ2VZEpS/OYh3rqURjt2bMcll1yGq666GmazGbt2fYRUKoUPPngf4XAYyWQSq1ffgwMHPsG0adOxceOGT1+3DT/96QMwmy3o6elBZ2cn4vE4tm3bOujzSFIKU6dOP+W5BEHAl750FX7+8//EwoWLMtkSorMmCALKCyz4pwWVuP8bc3D38lkodpqwflsDbvvle3hszW68/VEj2rsiSpdKRCPcrkNePP6Xvfj3pz7Abb94Dz0hZZ/ak/MjY0pavPifcN9992DjxregVmtQU1OLQKAHX/7yV/Htb38dkiTjoosuxuzZc1FWVo6HH/4x/vKXtZ9O4L8XZrMZy5Ytx8qVy+F2ezBp0uRBn6e5uRmLFl1zynMBwGWXXYk///lPuPDCL2SwI0TDx20z4Io5ZbhiThnC0QT2H+/EnqMdWLflOCxGLSaPyceUCjuqivP6ns05WB3dEYSjSXjsRo64EY0QsiwjFE2iOxhDNJ5CNJFCLJ5C7NM/o599/L+348m+/aOxFKLxJAw6NZZcUIFrLqiAx25UfGQs5x6H1Np6AgUF5QpVlB0kScIrr7yEhobjuO22Oz73OKTPsJfDj480ST9JlhGISdj8UQM+qffjRFsQHrsB40psGFtqw9iSvNOufRaMJPDq3+uxra4NFqMG3q4odBoR+RY97FYd7BYd8i06OG0GuPMN8OQbYdKrR/VNBXxPZ0Yu9bknHIfZoIH46f83yZSEQDiBnlAcPeF4359dgTi6gjF0BmPoCsTQFYxDpxFhNWlh0Kmh06ig06ig16qg06r6bWs/+7ym92t6rQp6rfrTP1Uw6tVQiacOYHwcEmXEPffcgba2Vjz66K+ULoVo2ImCgOpSG/L0Klx9fgUSSQknWgM43NiFLXtb8Oz6AzDpNRhbktcXzgrsRtSd6MRT6z7BzPEuPLByLixGLWRZRiCSQGdPDP5AFJ2BGPw9Mew+0oG2zgjaOyMQALjyDfDk9wY0t82IYpcJJS4TNGqOqhEBvb8k7TrkxcYdjTh4sgtumwFqtYieUByRWBImgwZWoxZ5Jg0sJi2sRi3yLTpUFFmQb9bBZtHBZtbl7Eg1R8aII2MZlEu/3Y5kZ+qzJMto6QjhcGM3DjV24fDJbsQSKciyjO/+U03fAraDIcsygpEE2rt6g1l7ZwRtnWE0tofQ3hmGO9+AMo8FZR4Lyj1mlLotZ3xQ+0DnisZT0GrE0/5GrwS+pzMjXX3+7D3cHYxDkmUIggBRFKBVi9CoRWj///buPjiq+t7j+Hsfs8nm+QlIBFSeBVGnXK8IJDbVgCYUgU7lqgg3WrmWsWWslwuS1NtaRFOGKuK9BRVHStrBEazKAIJ2oFbSiLQKeAEFITEJhDyRbDab7NO5fyRsCYJISbJJ+LxmmD17dvdwzje/yX5yfuf3O1YzVouZQNBo+xcIEgga+M8sB9raZVOLj+YWH26PH3eLD3dL+2P783pXK/HRdibfPIgbhyZTVe/BZGq77VpMpK1HjYzWmTERkS5mNplIT4kmPSWa225KB9pux+TxBkhPdl7StkwmU9uXSZSdIWlxHV7z+QOUV7spq3JRVtXEnoNVlFe7iXPaGdQvui2g9W8LanFO+wX/D38gyEcHq3jzz8dwebz4/EEcdiup7V2lyfEOEmMcxEe3daHGRtmwWMyhL1SH3XJFd6Ne6Xz+IHsOVXGq3oPFYiYQCNLo9lLT2EJtQwu1jS3YLGbioyMwmcAAAgEDnz+Izx/AFwji8wexmM1YzCasFhMWS9uyxWLGajbhsFtwRtqIclhxOmw4HW3t0xkZ0/7cRkyUjdSEyFBbHJh64WByJVIYE5ErXmKso9O3abNauGZALNcMiA2tCwYNTtY1U1rloqzKxbaSMsqqXFitZgb3i2FQv+j2x7YzaO99XM4H+ypJjY/kwZxRjBycQNAwaG7xU33aQ1VdMzUNLVTWiv3wSwAAEE5JREFUuvnsWB31Ta24mr3tZzDOfKEGiYywEBNlJyk2gqQ4B0mxDlITouiX2Hbd26UOcJCezzAMSg5WseFPR7gq2cmQ9Di8vkDoj5GxQ5NJjnWQFOfQz78H0E9ARKSbmM0m0pKdpCU7GT+6P9D2pVnb2ELpySbKqlz8Zd8Jyk59QaPby/gx/Xl0xtgOk+SaTSaiI21ER9o6BL0L8QeCtHgDNLi91Da0UNfYQk1DCx8fPkVVnYdTp5tx2Cxtt7E6K6CdGaAQYe+b1+j0NWeubzztaqXR7eWjQ6f4srKRR2eM5dq0i7cTCS+FMRGRMDKZTCTHRZIcF8l3RqSE1vv8gU4ZAGC1mImONBMdaTtvN6xhGJxu8lJV10xVfTNV9R6KPztJVb2H6tMenA4r/RKiGNw/hjHXJvJlRSOHyuoZP7o/E8cOuOz9k4szDIOGplZKT7o6DCSpd7W0P7ZS52olwmYmPiaCOKed1IQolsz+js569RL6KYmI9EDdNRLTZDKR0D5lx8hzBi8EDYO6xhaq6j18dqyOLcWlDOoXwy2j+/P+3nJ2fFzOnROuZmBiFGkpztBUBd2t3tXK51+dxmoxE2E3h6YwiLRbiXXasVl7zoCHb+LzBzhR20xFjZuKajcV1U2crGumztWKw24hzhnxjylWYh1cd3ViaDkhpu+ONLwSKIx1sb/97WPWrl3DqlW6/6OI9C7ms87ajb46scNrk8YOYP+Xdfxf2Wne2nWU5hY/w9unCrkqNZq0JCfx0fYuHTzQ6g3wxq6jFB84yYhB8QBtk3t62yb2bPEGaHR7iXJYSWgf4HAmuCRER5Bw1txxDnv3fR36A0FO1XvaQ1dTW/CqcVPb2EJqfCTpKW1d2RPHppGWHEVijIOr0uM1arUPUxgTEZFLZjKZGDskie/dcjXV1S7qGlv4/KvTfFHRwKdHaqisceMLGKQlRzEgyUlakrP9erkoEmMdl30WrabBwwsb95Oe4uSZ/xhPdKTtvO8LGgYut5c6V9vEoXWutm69g6V11Lcv17tasVjMoTOECTFtIS0+5kxYawtwF5vgNxg0KP7sJNv3fEWs086IgfFE2Cx4/QFqG1upOe2huv26vcSYCNKSnaSnRPMvo1K5O9nZI2aCl/BQGOsmZWWlFBYuxeVqxOGIZMGCxxk1ajTbt2/j979fh9lsJi0tjYKCp2hoOM0vf1mAx+PBbDbx05/+J2PGXB/uQxARuaDEWAe3jO7PLe0DEwBczV5O1DZTWeNuH/FZS2VtM41uL5ER1vapEKxERlixWy1t81rZzGctW7BZTGAyYQJMprYQaAL+/GklGTemMeXmQd8YkMwmE3HREcRFR8AFLnE7c5udf4S1tuuyjlU28jfXP67JCgSCoYAWHxPRNpDCYcMZacNiNvH+3nIiHVbuyRpKk8fH8RMuGtxe7FYz6clObhiSRHJ8JClxDuzqUpSzKIx1k6eeKuD+++eSmZnFgQP7yc//L/7wh0289NL/smbNqyQkJPLii89TVnacDz7Yxa23TuTeex/gr3/dzb59nyiMiUivc2YOtuED4zus9weCNLf6aW6fGLSlNYDXH8DrC4Yeff725fYJqQ0DDAww2pYn3zwoNE/c5TKdNUL1qm+Y/6rF6+9wNs3t8dHU4qeyxo3H62fmbUO4YUhSKBzePKpfp+yf9H19PowVvFxCRY2707ebnuzkqYf+9Vu91+PxUFlZQWZmFgBjxlxPbGwsZWWlTJgwiUceeZCMjNvIzMxi2LAReDwelixZyOefH+bWWycyc+YPO33/RUTCxWoxExvVdsub3sRhtzIgycqApEubHFjkYvp8GPu2gakrGcbXbzVkGBAIBFiw4HGOHJlGcfFfeOqpAvLyHmby5LtYv/51du/+C++/v50tW97huef+Jwx7LiIiIl2tz4exniAqyklaWjq7dv0p1E1ZV1fLtdcOYdas6axatYbZs/8dv9/P558f5ujRL0hOTuWHP/w3brppHHl594X7EERERKSLKIx1k5///Cl+/euneeWV1dhsdpYuLcRms/Hgg/NYsGA+ERERJCQksGTJf+P1evnFL/LZsuUdzGYz+fm/CPfui4iISBcxGYZhhHsn/lm1tU0Egx13/+TJUvr3HxymPeqdrFYzfv/Xu1JVy86XkhKjuYK6gercfVTr7qE6d5+uqLXZbCIp6cKDQzShiYiIiEgYKYyJiIiIhJHCmIiIiEgY9ckw1osvg+sxVEMREZHu0efCmNVqx+1uVJi4DIZh4HY3YrX2rgkZRUREeqM+N7VFQkIK9fXVNDWdDveu9Bpms5lgsONoSqvVTkJCSpj2SERE5MrR58KYxWIlOfkCd4OV89KQaRERkfDpc92UIiIiIr2JwpiIiIhIGPXqbkqz2RTuXegzVMvuo1p3D9W5+6jW3UN17j6dXeuLba9X3w5JREREpLdTN6WIiIhIGCmMiYiIiISRwpiIiIhIGCmMiYiIiISRwpiIiIhIGCmMiYiIiISRwpiIiIhIGCmMiYiIiISRwpiIiIhIGCmMXSFmz55NTk4O06ZNY9q0aXz66afs3r2bqVOnkp2dzW9+85vQew8ePMiMGTOYPHkyS5Yswe/3h3HPe4empiZyc3MpLy8HuOTaVlZWct999zFlyhQeeeQR3G53WI6jpzu3zosXLyY7OzvUrnfs2AGozpdr1apV5OTkkJOTQ2FhIaA23RXOV2e16a7x/PPPc9ddd5GTk8Orr74K9LA2bUifFwwGjYkTJxo+ny+0zuPxGJmZmUZZWZnh8/mMvLw8Y+fOnYZhGEZOTo7x97//3TAMw1i8eLFRVFQUlv3uLT755BMjNzfXGD16tPHVV1/9U7V9+OGHjc2bNxuGYRirVq0yCgsLw3MwPdi5dTYMw8jNzTWqqqq+9l7V+Z/34YcfGvfcc4/R2tpqeL1e44EHHjDeeecdtelOdr46b9++XW26C5SUlBizZs0yfD6f4fF4jO9+97vGwYMHe1Sb1pmxK8CXX34JQF5eHt///vdZv349+/btY/DgwQwcOBCr1crUqVPZtm0bFRUVtLS0cOONNwIwY8YMtm3bFs7d7/Fef/11nnzySVJTUwEuubY+n489e/YwefLkDuulo3Pr7PF4qKys5IknnmDq1KmsXLmSYDCoOl+mlJQUFi1ahN1ux2azMWTIEI4fP6423cnOV+fKykq16S5w8803s27dOqxWK7W1tQQCARobG3tUm7Z22pakx2psbGT8+PEUFBTg8/l44IEHeOihh0hJSQm9JzU1laqqKk6dOtVhfUpKClVVVeHY7V5j6dKlHZ6fW8OL1ba+vp7o6GisVmuH9dLRuXWuqanhlltu4cknnyQmJoZ58+bxxhtvMGzYMNX5MgwbNiy0fPz4cbZu3cr999+vNt3JzlfnoqIiPvroI7XpLmCz2Vi5ciVr165lypQpPe73tM6MXQFuuukmCgsLiYmJITExkR/84AesXLkSk8kUeo9hGJhMJoLB4HnXy7d3oRpeaP35aqyaX9zAgQN58cUXSU1NJTIyktmzZ7Nr1y7VuZN88cUX5OXlsXDhQgYOHKg23UXOrvO1116rNt2FfvKTn1BcXMyJEyc4fvx4j2rTCmNXgI8//pji4uLQc8MwSE9Pp7q6OrSuurqa1NRU+vfv32F9TU1NqFtIvp1za3ix2iYmJuJyuQgEAh3eL9/s8OHDvPvuu6HnhmFgtVpV506wd+9e5s6dy89+9jOmT5+uNt1Fzq2z2nTXOHr0KAcPHgQgMjKS7OxsSkpKelSbVhi7ArhcLgoLC2ltbaWpqYk333yTxx57jGPHjlFaWkogEGDz5s1kZGSQnp5OREQEe/fuBeCtt94iIyMjzEfQu9xwww2XVFubzca4cePYsmULAH/84x9V82/BMAyefvppGhoa8Pl8bNiwgTvuuEN1vkwnTpxg/vz5LF++nJycHEBtuiucr85q012jvLyc/Px8vF4vXq+X999/n1mzZvWoNm0yDMPotK1Jj/Xcc8/x7rvvEgwGuffee5kzZw7FxcUsW7aM1tZWMjMzWbx4MSaTiUOHDpGfn09TUxOjR49m2bJl2O32cB9Cj5eVlcW6deu46qqrLrm2FRUVLFq0iNraWgYMGMCKFSuIi4sL9yH1SGfXuaioiKKiIvx+P9nZ2Tz++OMAqvNl+NWvfsXGjRsZNGhQaN2sWbO4+uqr1aY70YXqHAwG1aa7wAsvvMDWrVuxWCxkZ2fz6KOP9qjf0wpjIiIiImGkbkoRERGRMFIYExEREQkjhTERERGRMFIYExEREQkjhTERERGRMFIYE5FeJysri/3797Nq1Sree++9Tt12Xl4edXV1APzoRz/iyJEjnbp9EZFz6d6UItJrlZSUMHTo0E7d5ocffhhafumllzp12yIi56MwJiK90q5duzhw4ACFhYVYLBYyMzNZvnw5e/bsIRAIcN1115Gfn090dDRZWVmMHTuWw4cP89hjj2G1Wlm9ejVer5e6ujruvvtuFixYwOLFiwGYM2cOa9as4b777uP555/n+uuvZ8OGDfzud7/DbDaTnJxMQUEB11xzDYsWLSI6OprDhw9z8uRJRowYwbPPPovT6WTlypXs2LEDm81GQkICy5Yt0+1qRORr1E0pIr1SZmYmY8aMYeHChdxxxx2sWbMGi8XCpk2bePvtt0lNTWX58uWh9w8bNoytW7dy++23s3btWp555hk2bdrEhg0bWLNmDXV1dSxbtgyA1157jQEDBoQ+W1xczMsvv8y6det4++23yc3NZf78+ZyZM/vAgQO88sorbNmyhYqKCrZt28aJEyd47bXX2LhxI5s2bWLChAns27eve4skIr2CzoyJSJ+wc+dOXC4Xu3fvBsDn85GUlBR6fdy4cQCYTCZ++9vfsnPnTjZv3szRo0cxDAOPx3PBbX/wwQfcddddJCYmAjBjxgyWLl1KeXk5AJMmTQrdMmz48OE0NDTQr18/Ro4cyfTp08nIyCAjI4Px48d3ybGLSO+mMCYifUIwGOSJJ54gMzMTALfbTWtra+j1qKgoAJqbm5k+fTq3334748aNY+bMmbz33nt8053hgsHg19YZhoHf7wfA4XCE1ptMJgzDwGw2s379evbv309xcTFPP/00kyZNYuHChZ1yvCLSd6ibUkR6LYvFEgpEEydOpKioCK/XSzAYpKCggBUrVnztM6WlpTQ1NbFgwQKysrIoKSkJfebcbZ4xadIktmzZEhpluXHjRuLj4xk8ePAF9+3QoUPk5uYyZMgQ5s2bx9y5c9m/f39nHbqI9CE6MyYivVZWVhYrVqzA5/Px4x//mGeffZbp06cTCAQYNWoUixYt+tpnRowYwW233cadd96J3W5n+PDhDB06lNLSUgYNGsSUKVOYPXs2L7zwQugzEyZMYO7cucyZM4dgMEhiYiKrV6/GbL7w37MjR47kzjvvZObMmURFReFwOMjPz++SOohI72YyvuncvIiIiIh0KXVTioiIiISRwpiIiIhIGCmMiYiIiISRwpiIiIhIGCmMiYiIiISRwpiIiIhIGCmMiYiIiISRwpiIiIhIGP0/UXMqrKpdDzgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x468 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "y1 = np.array(cnn_train_result['roll_180_acc'])\n",
    "y2 = np.array(cnn_train_result['roll_180_loss'])\n",
    "\n",
    "x = np.arange(len(y1))\n",
    "\n",
    "plt.figure(figsize=(10, 6.5))\n",
    "\n",
    "plt.plot(x, y1, color=\"r\", linewidth=1, label='accuracy')\n",
    "plt.plot(x, y2, color=\"b\", linewidth=1, label='loss')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Value')\n",
    "plt.title('CNN')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
