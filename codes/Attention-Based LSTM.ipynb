{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step0: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import tensorwatch as tw\n",
    "from sklearn import metrics\n",
    "from torchviz import make_dot\n",
    "from string import punctuation\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torchvision.models import AlexNet\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"../data/english_yep_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Count the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['someon', 'ha', 'work', 'mani', 'museum', 'wa', 'eager', 'visit', 'thi', 'galleri']\n",
      "Top ten occuring words :  [('wa', 199857), ('thi', 86639), ('place', 55772), ('food', 53489), ('good', 50852), ('great', 44401), ('veri', 44062), ('time', 42695), ('get', 38251), ('would', 38160)]\n"
     ]
    }
   ],
   "source": [
    "all_reviews = list(reviews['cleaned'])\n",
    "all_text = \" \".join(all_reviews)\n",
    "all_words = all_text.split()\n",
    "print(all_words[0:10])\n",
    "\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(all_words)\n",
    "total_words = len(all_words)\n",
    "sorted_words=count_words.most_common(total_words)\n",
    "print(\"Top ten occuring words : \", sorted_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "# We will start createing dictionary with index 1 because 0 \n",
    "    # is reserved for padding\n",
    "\n",
    "vocab_to_int = {w: i+1 for i, (w, c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4: Encode the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode review\n",
    "encoded_reviews = list()\n",
    "for review in all_reviews:\n",
    "    encoded_review = list()\n",
    "    for word in review.split():\n",
    "        if word not in vocab_to_int.keys():\n",
    "            # if word is not available in vocab_to_int put 0 in that place\n",
    "            encoded_review.append(0)\n",
    "        else:\n",
    "            encoded_review.append(vocab_to_int[word])\n",
    "    encoded_reviews.append(encoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Make the encode_review of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all the encoded_review of the same length\n",
    "# this step will return features of review_ints,\n",
    "# where each review is padded with 0's or truncated to the input seq_length.\n",
    "# the longest review has 564 words\n",
    "# sequence_length is 100, but also could be 150, 200, 250 (here just for save energy)\n",
    "\n",
    "sequence_length = 100\n",
    "features = np.zeros((len(encoded_reviews), sequence_length), dtype=int)\n",
    "for i, review in enumerate(encoded_reviews):\n",
    "    review_len = len(review)\n",
    "    if review_len <= sequence_length:\n",
    "        zeros = list(np.zeros(sequence_length-review_len))\n",
    "        new = zeros+review\n",
    "    else:\n",
    "        new = review[:sequence_length]\n",
    "    features[i, :] = np.array(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6: Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels, 0 negative, 1 neutral, 2 positive\n",
    "labels = list(reviews['Review_Labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step7: Split this feature data into Traning, Testing and Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79999 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# split this feature data into training and validation set\n",
    "# 80% training, 10% test and 10% validation dataset\n",
    "\n",
    "# However, for cpu running, set 10% of them\n",
    "# features = features[:int(0.5*len(features))]\n",
    "# labels = labels[:int(0.5*len(labels))]\n",
    "train_x = features[:int(0.8*len(features))]\n",
    "train_y = labels[:int(0.8*len(features))]\n",
    "valid_x = features[int(0.8*len(features)):int(0.9*len(features))]\n",
    "valid_y = labels[int(0.8*len(features)):int(0.9*len(features))]\n",
    "test_x = features[int(0.9*len(features)):]\n",
    "test_y = labels[int(0.9*len(features)):]\n",
    "print(len(train_y), len(valid_y), len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step8: Create DataLoader objects for Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#create Tensor Dataset\n",
    "train_data=TensorDataset(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n",
    "valid_data=TensorDataset(torch.FloatTensor(valid_x), torch.FloatTensor(valid_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(test_x), torch.FloatTensor(test_y))\n",
    "\n",
    "#dataloader\n",
    "# remember to add drop_last=True, which will delete the last batch of the data if it's size is not equal to batch_size\n",
    "batch_size=100\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step9: Analyze the dataloader data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([100, 100])\n",
      "Sample input: \n",
      " tensor([[   0.,    0.,    0.,  ..., 5899.,  371.,   35.],\n",
      "        [   0.,    0.,    0.,  ...,   47.,   20.,   16.],\n",
      "        [   0.,    0.,    0.,  ...,  125., 2599., 7442.],\n",
      "        ...,\n",
      "        [   0.,    0.,    0.,  ...,  242.,   58., 1307.],\n",
      "        [   0.,    0.,    0.,  ...,  203.,  128.,  198.],\n",
      "        [   0.,    0.,    0.,  ...,  130., 1242.,   13.]])\n",
      "Sample label size:  torch.Size([100])\n",
      "Sample label: \n",
      " tensor([2., 2., 2., 2., 2., 2., 1., 0., 0., 2., 2., 2., 2., 2., 2., 0., 0., 2.,\n",
      "        2., 2., 0., 0., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0.,\n",
      "        2., 0., 2., 2., 1., 0., 2., 2., 1., 1., 2., 2., 0., 0., 2., 1., 2., 0.,\n",
      "        2., 0., 2., 0., 2., 0., 2., 2., 0., 2., 1., 2., 1., 2., 2., 2., 0., 2.,\n",
      "        1., 0., 0., 0., 2., 2., 2., 1., 2., 2., 2., 0., 0., 1., 2., 2., 2., 2.,\n",
      "        0., 0., 2., 1., 2., 0., 0., 0., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step10: Create an Attention-Based Bi-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Att_LSTM_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embed, hidden_size, hidden_size2, num_layers, num_classes, dropout, bidirectional):\n",
    "        super(Att_LSTM_Model, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed)\n",
    "        self.lstm = nn.LSTM(embed, hidden_size, num_layers,\n",
    "                            bidirectional=bidirectional, batch_first=True, dropout=dropout)  # initialize Bi-LSTM model\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.w = nn.Parameter(torch.zeros(hidden_size * 2))\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size2)\n",
    "        self.fc = nn.Linear(hidden_size2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)  # [batch_size, seq_len, embeding]=[100, 100, 300]\n",
    "        H, _ = self.lstm(emb)  # batch_size, seq_len, hidden_size * num_direction]=[100, 100, 256]\n",
    "        \n",
    "        M = self.tanh1(H)  # [100, 100, 256]\n",
    "        alpha = F.softmax(torch.matmul(M, self.w), dim=1).unsqueeze(-1)  # [100, 100, 1]\n",
    "        out = H * alpha  # [100, 100, 256]\n",
    "        out = torch.sum(out, 1)  # [100, 256]\n",
    "        out = F.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc(out)  # [128, 64]\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step11: Initialize the Attention-Based Bi-LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Att-Bi-LSTM\n",
      "Att_LSTM_Model(\n",
      "  (embedding): Embedding(77398, 300)\n",
      "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (tanh1): Tanh()\n",
      "  (tanh2): Tanh()\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hperparams\n",
    "vocab_size = len(vocab_to_int) + 1  # +1 for the 0 padding, 0.1 for the 10% of the total data (see step7)\n",
    "output_size = 1\n",
    "embed = 300\n",
    "hidden_size = 128\n",
    "hidden_size2 = 64\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "num_classes = 3\n",
    "epochs = 10\n",
    "bidirectional = False\n",
    "\n",
    "model_att_bilstm = Att_LSTM_Model(vocab_size, output_size, embed, hidden_size, hidden_size2, num_layers, num_classes, dropout, True)\n",
    "print('Att-Bi-LSTM')\n",
    "print(model_att_bilstm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step12: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model):\n",
    "    lr = 0.0001  # learning rate\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "    total_batch = 0\n",
    "    train_loss = 0\n",
    "    result_train_dict = {}\n",
    "    result_valid_dict = {}\n",
    "    epoch_train_list = []\n",
    "    epoch_valid_list = []\n",
    "    batch_train_list = []\n",
    "    batch_valid_list = []\n",
    "    loss_train_list = []\n",
    "    loss_valid_list = []\n",
    "    acc_train_list = []\n",
    "    acc_valid_list = []\n",
    "    f1_train_list = []\n",
    "    f1_valid_list = []\n",
    "    recall_train_list = []\n",
    "    recall_valid_list = []\n",
    "    for batch_idx, (trains, labels) in enumerate(train_loader):\n",
    "        \n",
    "        outputs = model(trains.long())\n",
    "        model.zero_grad()\n",
    "        loss = F.cross_entropy(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        true = labels.data.cpu()\n",
    "        predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "        train_acc = metrics.accuracy_score(true, predic)\n",
    "        train_f1 = metrics.f1_score(true, predic, average='micro')\n",
    "        train_recall = metrics.recall_score(true, predic, average='micro')\n",
    "        loss_value = train_loss/(batch_idx+1)\n",
    "        \n",
    "        epoch_train_list.append(epoch)\n",
    "        batch_train_list.append(batch_idx)\n",
    "        loss_train_list.append(loss_value)\n",
    "        acc_train_list.append(train_acc)\n",
    "        f1_train_list.append(train_f1)\n",
    "        recall_train_list.append(train_recall)\n",
    "\n",
    "        if total_batch % 100 == 0 :\n",
    "            \n",
    "            for trains, labels in valid_loader:\n",
    "                outputs = model(trains.long())\n",
    "                valid_loss = F.cross_entropy(outputs, labels.long())\n",
    "                \n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                valid_acc = metrics.accuracy_score(true, predic)\n",
    "                valid_f1 = metrics.f1_score(true, predic, average='micro')\n",
    "                valid_recall = metrics.recall_score(true, predic, average='micro')\n",
    "                \n",
    "                epoch_valid_list.append(epoch)\n",
    "                batch_valid_list.append(batch_idx)\n",
    "                loss_valid_list.append(valid_loss.item())\n",
    "                acc_valid_list.append(valid_acc)\n",
    "                f1_valid_list.append(valid_f1)\n",
    "                recall_valid_list.append(valid_recall)\n",
    "                \n",
    "                avg_acc_valid = np.array(acc_valid_list).mean()\n",
    "                avg_loss_valid = np.array(loss_valid_list).mean()\n",
    "                \n",
    "            print('epoch: {}'.format(epoch), 'batch: {}'.format(batch_idx), \n",
    "                  'total train loader: {}'.format(len(train_loader)),\n",
    "                  'T_Loss: %.3f | T_Acc: %.3f |' % (loss_value, train_acc),\n",
    "                  'T_f1: %.3f | T_recall: %.3f ||' % (train_f1, train_recall),\n",
    "                  'V_Loss: %.3f | V_Acc: %.3f' % (avg_loss_valid, avg_acc_valid),\n",
    "                  'V_f1: %.3f | V_recall: %.3f' % (valid_f1, valid_recall))\n",
    "        total_batch += 1\n",
    "        \n",
    "    result_train_dict['epoch'] = epoch_train_list\n",
    "    result_train_dict['batch'] = batch_train_list\n",
    "    result_train_dict['loss'] = loss_train_list\n",
    "    result_train_dict['acc'] = acc_train_list\n",
    "    result_train_dict['f1'] = f1_train_list\n",
    "    result_train_dict['recall'] = recall_train_list\n",
    "    \n",
    "    result_valid_dict['epoch'] = epoch_valid_list\n",
    "    result_valid_dict['batch'] = batch_valid_list\n",
    "    result_valid_dict['loss'] = loss_valid_list\n",
    "    result_valid_dict['acc'] = acc_valid_list\n",
    "    result_valid_dict['f1'] = f1_valid_list\n",
    "    result_valid_dict['recall'] = recall_valid_list\n",
    "    \n",
    "    pd_train = pd.DataFrame(result_train_dict)\n",
    "    pd_valid = pd.DataFrame(result_valid_dict)\n",
    "\n",
    "    return pd_train, pd_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 total train loader: 799 T_Loss: 1.077 | T_Acc: 0.610 | T_f1: 0.610 | T_recall: 0.610 || V_Loss: 1.062 | V_Acc: 0.669 V_f1: 0.720 | V_recall: 0.720\n",
      "epoch: 0 batch: 100 total train loader: 799 T_Loss: 0.899 | T_Acc: 0.720 | T_f1: 0.720 | T_recall: 0.720 || V_Loss: 0.951 | V_Acc: 0.669 V_f1: 0.720 | V_recall: 0.720\n",
      "epoch: 0 batch: 200 total train loader: 799 T_Loss: 0.863 | T_Acc: 0.650 | T_f1: 0.650 | T_recall: 0.650 || V_Loss: 0.911 | V_Acc: 0.669 V_f1: 0.660 | V_recall: 0.660\n",
      "epoch: 0 batch: 300 total train loader: 799 T_Loss: 0.848 | T_Acc: 0.690 | T_f1: 0.690 | T_recall: 0.690 || V_Loss: 0.889 | V_Acc: 0.669 V_f1: 0.700 | V_recall: 0.700\n",
      "epoch: 0 batch: 400 total train loader: 799 T_Loss: 0.842 | T_Acc: 0.670 | T_f1: 0.670 | T_recall: 0.670 || V_Loss: 0.870 | V_Acc: 0.669 V_f1: 0.670 | V_recall: 0.670\n",
      "epoch: 0 batch: 500 total train loader: 799 T_Loss: 0.819 | T_Acc: 0.770 | T_f1: 0.770 | T_recall: 0.770 || V_Loss: 0.832 | V_Acc: 0.684 V_f1: 0.750 | V_recall: 0.750\n",
      "epoch: 0 batch: 600 total train loader: 799 T_Loss: 0.784 | T_Acc: 0.730 | T_f1: 0.730 | T_recall: 0.730 || V_Loss: 0.795 | V_Acc: 0.697 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 0 batch: 700 total train loader: 799 T_Loss: 0.753 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.766 | V_Acc: 0.709 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 1 batch: 0 total train loader: 799 T_Loss: 0.640 | T_Acc: 0.730 | T_f1: 0.730 | T_recall: 0.730 || V_Loss: 0.540 | V_Acc: 0.794 V_f1: 0.730 | V_recall: 0.730\n",
      "epoch: 1 batch: 100 total train loader: 799 T_Loss: 0.535 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.548 | V_Acc: 0.794 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 1 batch: 200 total train loader: 799 T_Loss: 0.534 | T_Acc: 0.680 | T_f1: 0.680 | T_recall: 0.680 || V_Loss: 0.537 | V_Acc: 0.798 V_f1: 0.880 | V_recall: 0.880\n",
      "epoch: 1 batch: 300 total train loader: 799 T_Loss: 0.529 | T_Acc: 0.760 | T_f1: 0.760 | T_recall: 0.760 || V_Loss: 0.529 | V_Acc: 0.801 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 1 batch: 400 total train loader: 799 T_Loss: 0.523 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.525 | V_Acc: 0.802 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 1 batch: 500 total train loader: 799 T_Loss: 0.518 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.519 | V_Acc: 0.804 V_f1: 0.750 | V_recall: 0.750\n",
      "epoch: 1 batch: 600 total train loader: 799 T_Loss: 0.513 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.515 | V_Acc: 0.806 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 1 batch: 700 total train loader: 799 T_Loss: 0.510 | T_Acc: 0.760 | T_f1: 0.760 | T_recall: 0.760 || V_Loss: 0.510 | V_Acc: 0.807 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 2 batch: 0 total train loader: 799 T_Loss: 0.601 | T_Acc: 0.760 | T_f1: 0.760 | T_recall: 0.760 || V_Loss: 0.477 | V_Acc: 0.819 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 2 batch: 100 total train loader: 799 T_Loss: 0.466 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.474 | V_Acc: 0.819 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 2 batch: 200 total train loader: 799 T_Loss: 0.464 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.470 | V_Acc: 0.821 V_f1: 0.690 | V_recall: 0.690\n",
      "epoch: 2 batch: 300 total train loader: 799 T_Loss: 0.460 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.469 | V_Acc: 0.821 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 2 batch: 400 total train loader: 799 T_Loss: 0.457 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.467 | V_Acc: 0.822 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 2 batch: 500 total train loader: 799 T_Loss: 0.456 | T_Acc: 0.900 | T_f1: 0.900 | T_recall: 0.900 || V_Loss: 0.464 | V_Acc: 0.823 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 2 batch: 600 total train loader: 799 T_Loss: 0.454 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.463 | V_Acc: 0.824 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 2 batch: 700 total train loader: 799 T_Loss: 0.453 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.461 | V_Acc: 0.824 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 3 batch: 0 total train loader: 799 T_Loss: 0.588 | T_Acc: 0.760 | T_f1: 0.760 | T_recall: 0.760 || V_Loss: 0.445 | V_Acc: 0.830 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 3 batch: 100 total train loader: 799 T_Loss: 0.442 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.447 | V_Acc: 0.828 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 3 batch: 200 total train loader: 799 T_Loss: 0.433 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.445 | V_Acc: 0.829 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 3 batch: 300 total train loader: 799 T_Loss: 0.426 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.443 | V_Acc: 0.829 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 3 batch: 400 total train loader: 799 T_Loss: 0.425 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.442 | V_Acc: 0.829 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 3 batch: 500 total train loader: 799 T_Loss: 0.426 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.440 | V_Acc: 0.830 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 3 batch: 600 total train loader: 799 T_Loss: 0.427 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.439 | V_Acc: 0.830 V_f1: 0.940 | V_recall: 0.940\n",
      "epoch: 3 batch: 700 total train loader: 799 T_Loss: 0.424 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.439 | V_Acc: 0.830 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 4 batch: 0 total train loader: 799 T_Loss: 0.481 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.429 | V_Acc: 0.835 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 4 batch: 100 total train loader: 799 T_Loss: 0.407 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.428 | V_Acc: 0.836 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 4 batch: 200 total train loader: 799 T_Loss: 0.403 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.431 | V_Acc: 0.835 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 4 batch: 300 total train loader: 799 T_Loss: 0.403 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.430 | V_Acc: 0.834 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 4 batch: 400 total train loader: 799 T_Loss: 0.403 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.430 | V_Acc: 0.834 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 4 batch: 500 total train loader: 799 T_Loss: 0.405 | T_Acc: 0.900 | T_f1: 0.900 | T_recall: 0.900 || V_Loss: 0.430 | V_Acc: 0.834 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 4 batch: 600 total train loader: 799 T_Loss: 0.404 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.429 | V_Acc: 0.834 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 4 batch: 700 total train loader: 799 T_Loss: 0.405 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.430 | V_Acc: 0.833 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 5 batch: 0 total train loader: 799 T_Loss: 0.372 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.438 | V_Acc: 0.831 V_f1: 0.750 | V_recall: 0.750\n",
      "epoch: 5 batch: 100 total train loader: 799 T_Loss: 0.385 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.431 | V_Acc: 0.832 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 5 batch: 200 total train loader: 799 T_Loss: 0.393 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.430 | V_Acc: 0.833 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 5 batch: 300 total train loader: 799 T_Loss: 0.390 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.428 | V_Acc: 0.833 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 5 batch: 400 total train loader: 799 T_Loss: 0.391 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.429 | V_Acc: 0.834 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 5 batch: 500 total train loader: 799 T_Loss: 0.392 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.428 | V_Acc: 0.834 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 5 batch: 600 total train loader: 799 T_Loss: 0.391 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.427 | V_Acc: 0.835 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 5 batch: 700 total train loader: 799 T_Loss: 0.392 | T_Acc: 0.770 | T_f1: 0.770 | T_recall: 0.770 || V_Loss: 0.430 | V_Acc: 0.834 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 6 batch: 0 total train loader: 799 T_Loss: 0.384 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.436 | V_Acc: 0.831 V_f1: 0.920 | V_recall: 0.920\n",
      "epoch: 6 batch: 100 total train loader: 799 T_Loss: 0.378 | T_Acc: 0.910 | T_f1: 0.910 | T_recall: 0.910 || V_Loss: 0.427 | V_Acc: 0.835 V_f1: 0.750 | V_recall: 0.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 batch: 200 total train loader: 799 T_Loss: 0.377 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.424 | V_Acc: 0.837 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 6 batch: 300 total train loader: 799 T_Loss: 0.378 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.426 | V_Acc: 0.837 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 6 batch: 400 total train loader: 799 T_Loss: 0.382 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.425 | V_Acc: 0.837 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 6 batch: 500 total train loader: 799 T_Loss: 0.382 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.425 | V_Acc: 0.837 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 6 batch: 600 total train loader: 799 T_Loss: 0.383 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.424 | V_Acc: 0.837 V_f1: 0.900 | V_recall: 0.900\n",
      "epoch: 6 batch: 700 total train loader: 799 T_Loss: 0.383 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.424 | V_Acc: 0.837 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 7 batch: 0 total train loader: 799 T_Loss: 0.363 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.425 | V_Acc: 0.835 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 7 batch: 100 total train loader: 799 T_Loss: 0.386 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.423 | V_Acc: 0.834 V_f1: 0.750 | V_recall: 0.750\n",
      "epoch: 7 batch: 200 total train loader: 799 T_Loss: 0.374 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.423 | V_Acc: 0.834 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 7 batch: 300 total train loader: 799 T_Loss: 0.375 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.422 | V_Acc: 0.834 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 7 batch: 400 total train loader: 799 T_Loss: 0.378 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.421 | V_Acc: 0.835 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 7 batch: 500 total train loader: 799 T_Loss: 0.376 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.423 | V_Acc: 0.835 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 7 batch: 600 total train loader: 799 T_Loss: 0.376 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.423 | V_Acc: 0.835 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 7 batch: 700 total train loader: 799 T_Loss: 0.378 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.422 | V_Acc: 0.835 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 8 batch: 0 total train loader: 799 T_Loss: 0.395 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.468 | V_Acc: 0.827 V_f1: 0.880 | V_recall: 0.880\n",
      "epoch: 8 batch: 100 total train loader: 799 T_Loss: 0.386 | T_Acc: 0.900 | T_f1: 0.900 | T_recall: 0.900 || V_Loss: 0.450 | V_Acc: 0.828 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 8 batch: 200 total train loader: 799 T_Loss: 0.377 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.443 | V_Acc: 0.832 V_f1: 0.900 | V_recall: 0.900\n",
      "epoch: 8 batch: 300 total train loader: 799 T_Loss: 0.372 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.444 | V_Acc: 0.833 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 8 batch: 400 total train loader: 799 T_Loss: 0.373 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.442 | V_Acc: 0.833 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 8 batch: 500 total train loader: 799 T_Loss: 0.375 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.441 | V_Acc: 0.831 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 8 batch: 600 total train loader: 799 T_Loss: 0.374 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.439 | V_Acc: 0.832 V_f1: 0.740 | V_recall: 0.740\n",
      "epoch: 8 batch: 700 total train loader: 799 T_Loss: 0.375 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.438 | V_Acc: 0.832 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 9 batch: 0 total train loader: 799 T_Loss: 0.431 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.468 | V_Acc: 0.823 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 9 batch: 100 total train loader: 799 T_Loss: 0.373 | T_Acc: 0.910 | T_f1: 0.910 | T_recall: 0.910 || V_Loss: 0.453 | V_Acc: 0.826 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 9 batch: 200 total train loader: 799 T_Loss: 0.366 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.447 | V_Acc: 0.828 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 9 batch: 300 total train loader: 799 T_Loss: 0.369 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.442 | V_Acc: 0.830 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 9 batch: 400 total train loader: 799 T_Loss: 0.370 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.438 | V_Acc: 0.832 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 9 batch: 500 total train loader: 799 T_Loss: 0.373 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.436 | V_Acc: 0.833 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 9 batch: 600 total train loader: 799 T_Loss: 0.372 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.435 | V_Acc: 0.833 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 9 batch: 700 total train loader: 799 T_Loss: 0.373 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.433 | V_Acc: 0.833 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 10 batch: 0 total train loader: 799 T_Loss: 0.363 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.425 | V_Acc: 0.837 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 10 batch: 100 total train loader: 799 T_Loss: 0.371 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.423 | V_Acc: 0.837 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 10 batch: 200 total train loader: 799 T_Loss: 0.370 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.423 | V_Acc: 0.837 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 10 batch: 300 total train loader: 799 T_Loss: 0.365 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.424 | V_Acc: 0.838 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 10 batch: 400 total train loader: 799 T_Loss: 0.364 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.425 | V_Acc: 0.837 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 10 batch: 500 total train loader: 799 T_Loss: 0.363 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.426 | V_Acc: 0.836 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 10 batch: 600 total train loader: 799 T_Loss: 0.365 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.426 | V_Acc: 0.837 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 10 batch: 700 total train loader: 799 T_Loss: 0.365 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.427 | V_Acc: 0.836 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 11 batch: 0 total train loader: 799 T_Loss: 0.471 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.428 | V_Acc: 0.836 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 11 batch: 100 total train loader: 799 T_Loss: 0.372 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.430 | V_Acc: 0.834 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 11 batch: 200 total train loader: 799 T_Loss: 0.369 | T_Acc: 0.910 | T_f1: 0.910 | T_recall: 0.910 || V_Loss: 0.434 | V_Acc: 0.832 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 11 batch: 300 total train loader: 799 T_Loss: 0.370 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.434 | V_Acc: 0.831 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 11 batch: 400 total train loader: 799 T_Loss: 0.368 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.433 | V_Acc: 0.832 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 11 batch: 500 total train loader: 799 T_Loss: 0.369 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.432 | V_Acc: 0.834 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 11 batch: 600 total train loader: 799 T_Loss: 0.369 | T_Acc: 0.930 | T_f1: 0.930 | T_recall: 0.930 || V_Loss: 0.432 | V_Acc: 0.833 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 11 batch: 700 total train loader: 799 T_Loss: 0.368 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.430 | V_Acc: 0.834 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 12 batch: 0 total train loader: 799 T_Loss: 0.396 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.527 | V_Acc: 0.793 V_f1: 0.740 | V_recall: 0.740\n",
      "epoch: 12 batch: 100 total train loader: 799 T_Loss: 0.357 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.480 | V_Acc: 0.810 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 12 batch: 200 total train loader: 799 T_Loss: 0.360 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.467 | V_Acc: 0.817 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 12 batch: 300 total train loader: 799 T_Loss: 0.361 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.457 | V_Acc: 0.821 V_f1: 0.810 | V_recall: 0.810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 batch: 400 total train loader: 799 T_Loss: 0.362 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.453 | V_Acc: 0.823 V_f1: 0.740 | V_recall: 0.740\n",
      "epoch: 12 batch: 500 total train loader: 799 T_Loss: 0.364 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.449 | V_Acc: 0.826 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 12 batch: 600 total train loader: 799 T_Loss: 0.365 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.450 | V_Acc: 0.826 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 12 batch: 700 total train loader: 799 T_Loss: 0.367 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.448 | V_Acc: 0.827 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 13 batch: 0 total train loader: 799 T_Loss: 0.345 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.472 | V_Acc: 0.811 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 13 batch: 100 total train loader: 799 T_Loss: 0.373 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.451 | V_Acc: 0.821 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 13 batch: 200 total train loader: 799 T_Loss: 0.369 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.445 | V_Acc: 0.827 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 13 batch: 300 total train loader: 799 T_Loss: 0.370 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.444 | V_Acc: 0.828 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 13 batch: 400 total train loader: 799 T_Loss: 0.365 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.450 | V_Acc: 0.829 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 13 batch: 500 total train loader: 799 T_Loss: 0.369 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.446 | V_Acc: 0.830 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 13 batch: 600 total train loader: 799 T_Loss: 0.373 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.444 | V_Acc: 0.830 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 13 batch: 700 total train loader: 799 T_Loss: 0.373 | T_Acc: 0.750 | T_f1: 0.750 | T_recall: 0.750 || V_Loss: 0.441 | V_Acc: 0.831 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 14 batch: 0 total train loader: 799 T_Loss: 0.366 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.548 | V_Acc: 0.805 V_f1: 0.700 | V_recall: 0.700\n",
      "epoch: 14 batch: 100 total train loader: 799 T_Loss: 0.366 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.495 | V_Acc: 0.820 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 14 batch: 200 total train loader: 799 T_Loss: 0.371 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.475 | V_Acc: 0.824 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 14 batch: 300 total train loader: 799 T_Loss: 0.373 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.466 | V_Acc: 0.826 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 14 batch: 400 total train loader: 799 T_Loss: 0.373 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.463 | V_Acc: 0.826 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 14 batch: 500 total train loader: 799 T_Loss: 0.374 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.458 | V_Acc: 0.828 V_f1: 0.910 | V_recall: 0.910\n",
      "epoch: 14 batch: 600 total train loader: 799 T_Loss: 0.375 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.455 | V_Acc: 0.828 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 14 batch: 700 total train loader: 799 T_Loss: 0.376 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.451 | V_Acc: 0.829 V_f1: 0.830 | V_recall: 0.830\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "att_bilstm_train_result = pd.DataFrame()\n",
    "att_bilstm_valid_result = pd.DataFrame()\n",
    "for epoch in range(epochs):\n",
    "    train_result, valid_result = train(epoch, model_att_bilstm)\n",
    "    att_bilstm_train_result = att_bilstm_train_result.append(train_result, ignore_index=True)\n",
    "    att_bilstm_valid_result = att_bilstm_valid_result.append(train_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.077081</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.070047</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.068034</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.063703</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.061141</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11980</th>\n",
       "      <td>14</td>\n",
       "      <td>794</td>\n",
       "      <td>0.377253</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11981</th>\n",
       "      <td>14</td>\n",
       "      <td>795</td>\n",
       "      <td>0.377299</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11982</th>\n",
       "      <td>14</td>\n",
       "      <td>796</td>\n",
       "      <td>0.377405</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11983</th>\n",
       "      <td>14</td>\n",
       "      <td>797</td>\n",
       "      <td>0.377343</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11984</th>\n",
       "      <td>14</td>\n",
       "      <td>798</td>\n",
       "      <td>0.377267</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11985 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       epoch  batch      loss   acc    f1  recall\n",
       "0          0      0  1.077081  0.61  0.61    0.61\n",
       "1          0      1  1.070047  0.66  0.66    0.66\n",
       "2          0      2  1.068034  0.61  0.61    0.61\n",
       "3          0      3  1.063703  0.63  0.63    0.63\n",
       "4          0      4  1.061141  0.65  0.65    0.65\n",
       "...      ...    ...       ...   ...   ...     ...\n",
       "11980     14    794  0.377253  0.83  0.83    0.83\n",
       "11981     14    795  0.377299  0.80  0.80    0.80\n",
       "11982     14    796  0.377405  0.80  0.80    0.80\n",
       "11983     14    797  0.377343  0.88  0.88    0.88\n",
       "11984     14    798  0.377267  0.91  0.91    0.91\n",
       "\n",
       "[11985 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_bilstm_train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_bilstm_train_result['roll_730_acc'] = att_bilstm_train_result['acc'].rolling(730).mean()\n",
    "att_bilstm_valid_result['roll_730_acc'] = att_bilstm_valid_result['acc'].rolling(730).mean()\n",
    "att_bilstm_train_result['roll_730_loss'] = att_bilstm_train_result['loss'].rolling(730).mean()\n",
    "att_bilstm_valid_result['roll_730_loss'] = att_bilstm_valid_result['loss'].rolling(730).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_bilstm_train_result.to_csv('../result/Attention-Based-Bi-LSTM/att_bilstm_train_result.csv')\n",
    "att_bilstm_valid_result.to_csv('../result/Attention-Based-Bi-LSTM/att_bilstm_valid_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>roll_40_acc</th>\n",
       "      <th>roll_730_acc</th>\n",
       "      <th>roll_730_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.077081</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.070047</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.068034</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.063703</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.061141</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  batch      loss   acc    f1  recall  roll_40_acc  roll_730_acc  roll_730_loss\n",
       "0      0      0  1.077081  0.61  0.61    0.61          NaN           NaN            NaN\n",
       "1      0      1  1.070047  0.66  0.66    0.66          NaN           NaN            NaN\n",
       "2      0      2  1.068034  0.61  0.61    0.61          NaN           NaN            NaN\n",
       "3      0      3  1.063703  0.63  0.63    0.63          NaN           NaN            NaN\n",
       "4      0      4  1.061141  0.65  0.65    0.65          NaN           NaN            NaN"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_bilstm_valid_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch             14.000000\n",
       "batch            798.000000\n",
       "loss               0.377267\n",
       "acc                0.910000\n",
       "f1                 0.910000\n",
       "recall             0.910000\n",
       "roll_40_acc        0.849250\n",
       "roll_730_acc       0.851288\n",
       "roll_730_loss      0.373263\n",
       "Name: 11984, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_bilstm_valid_result.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step13: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model):\n",
    "    lr = 0.001  # learning rate\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    total_batch = 0\n",
    "    train_loss = 0\n",
    "    result_dict = {}\n",
    "    epoch_list = []\n",
    "    batch_list = []\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    \n",
    "    for batch_idx, (trains, labels) in enumerate(test_loader):\n",
    "\n",
    "        outputs = model(trains.long())\n",
    "        test_loss = F.cross_entropy(outputs, labels.long())\n",
    "\n",
    "        \n",
    "        true = labels.data.cpu()\n",
    "        predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "        test_acc = metrics.accuracy_score(true, predic)\n",
    "        test_f1 = metrics.f1_score(true, predic, average='micro')\n",
    "        test_recall = metrics.recall_score(true, predic, average='micro')\n",
    "        \n",
    "        \n",
    "        epoch_list.append(epoch)\n",
    "        batch_list.append(batch_idx)\n",
    "        loss_list.append(test_loss.item())\n",
    "        acc_list.append(test_acc)\n",
    "        recall_list.append(test_recall)\n",
    "        f1_list.append(test_f1)\n",
    "            \n",
    "        if total_batch % 50 == 0:\n",
    "            print('epoch: {}'.format(epoch), 'batch: {}'.format(batch_idx), \n",
    "                  'total train loader: {}'.format(len(test_loader)),\n",
    "                  'Loss: %.3f | Acc: %.3f |' % (test_loss, test_acc),\n",
    "                  'F1: %.3f | Recall: %.3f ' % (test_f1, test_recall))\n",
    "        \n",
    "#         total_batch += 1\n",
    "        \n",
    "    result_dict['epoch'] = epoch_list\n",
    "    result_dict['batch'] = batch_list\n",
    "    result_dict['loss'] = loss_list\n",
    "    result_dict['acc'] = acc_list\n",
    "    result_dict['f1'] = f1_list\n",
    "    result_dict['recall'] = recall_list\n",
    "    \n",
    "    return pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 total train loader: 100 Loss: 0.475 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 1 total train loader: 100 Loss: 0.472 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 2 total train loader: 100 Loss: 0.551 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 3 total train loader: 100 Loss: 0.513 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 4 total train loader: 100 Loss: 0.497 | Acc: 0.750 | F1: 0.750 | Recall: 0.750 \n",
      "epoch: 0 batch: 5 total train loader: 100 Loss: 0.334 | Acc: 0.910 | F1: 0.910 | Recall: 0.910 \n",
      "epoch: 0 batch: 6 total train loader: 100 Loss: 0.588 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 7 total train loader: 100 Loss: 0.584 | Acc: 0.760 | F1: 0.760 | Recall: 0.760 \n",
      "epoch: 0 batch: 8 total train loader: 100 Loss: 0.363 | Acc: 0.890 | F1: 0.890 | Recall: 0.890 \n",
      "epoch: 0 batch: 9 total train loader: 100 Loss: 0.332 | Acc: 0.890 | F1: 0.890 | Recall: 0.890 \n",
      "epoch: 0 batch: 10 total train loader: 100 Loss: 0.383 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 11 total train loader: 100 Loss: 0.470 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 12 total train loader: 100 Loss: 0.427 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 13 total train loader: 100 Loss: 0.406 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 14 total train loader: 100 Loss: 0.474 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 15 total train loader: 100 Loss: 0.432 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 16 total train loader: 100 Loss: 0.370 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 17 total train loader: 100 Loss: 0.392 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 18 total train loader: 100 Loss: 0.398 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 19 total train loader: 100 Loss: 0.473 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 20 total train loader: 100 Loss: 0.443 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 21 total train loader: 100 Loss: 0.469 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 22 total train loader: 100 Loss: 0.428 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 23 total train loader: 100 Loss: 0.403 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 24 total train loader: 100 Loss: 0.486 | Acc: 0.780 | F1: 0.780 | Recall: 0.780 \n",
      "epoch: 0 batch: 25 total train loader: 100 Loss: 0.520 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 26 total train loader: 100 Loss: 0.420 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 27 total train loader: 100 Loss: 0.346 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 28 total train loader: 100 Loss: 0.470 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 29 total train loader: 100 Loss: 0.467 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 30 total train loader: 100 Loss: 0.455 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 31 total train loader: 100 Loss: 0.414 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 32 total train loader: 100 Loss: 0.432 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 33 total train loader: 100 Loss: 0.492 | Acc: 0.780 | F1: 0.780 | Recall: 0.780 \n",
      "epoch: 0 batch: 34 total train loader: 100 Loss: 0.472 | Acc: 0.780 | F1: 0.780 | Recall: 0.780 \n",
      "epoch: 0 batch: 35 total train loader: 100 Loss: 0.407 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 36 total train loader: 100 Loss: 0.346 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 37 total train loader: 100 Loss: 0.468 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 38 total train loader: 100 Loss: 0.453 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 39 total train loader: 100 Loss: 0.568 | Acc: 0.740 | F1: 0.740 | Recall: 0.740 \n",
      "epoch: 0 batch: 40 total train loader: 100 Loss: 0.408 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 41 total train loader: 100 Loss: 0.472 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 42 total train loader: 100 Loss: 0.426 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 43 total train loader: 100 Loss: 0.460 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 44 total train loader: 100 Loss: 0.436 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 45 total train loader: 100 Loss: 0.452 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 46 total train loader: 100 Loss: 0.584 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 47 total train loader: 100 Loss: 0.597 | Acc: 0.750 | F1: 0.750 | Recall: 0.750 \n",
      "epoch: 0 batch: 48 total train loader: 100 Loss: 0.518 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 49 total train loader: 100 Loss: 0.464 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 50 total train loader: 100 Loss: 0.472 | Acc: 0.780 | F1: 0.780 | Recall: 0.780 \n",
      "epoch: 0 batch: 51 total train loader: 100 Loss: 0.666 | Acc: 0.700 | F1: 0.700 | Recall: 0.700 \n",
      "epoch: 0 batch: 52 total train loader: 100 Loss: 0.506 | Acc: 0.790 | F1: 0.790 | Recall: 0.790 \n",
      "epoch: 0 batch: 53 total train loader: 100 Loss: 0.366 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 54 total train loader: 100 Loss: 0.371 | Acc: 0.880 | F1: 0.880 | Recall: 0.880 \n",
      "epoch: 0 batch: 55 total train loader: 100 Loss: 0.523 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 56 total train loader: 100 Loss: 0.312 | Acc: 0.880 | F1: 0.880 | Recall: 0.880 \n",
      "epoch: 0 batch: 57 total train loader: 100 Loss: 0.430 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 58 total train loader: 100 Loss: 0.379 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 59 total train loader: 100 Loss: 0.469 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 60 total train loader: 100 Loss: 0.463 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 61 total train loader: 100 Loss: 0.504 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 62 total train loader: 100 Loss: 0.537 | Acc: 0.760 | F1: 0.760 | Recall: 0.760 \n",
      "epoch: 0 batch: 63 total train loader: 100 Loss: 0.436 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 64 total train loader: 100 Loss: 0.494 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 65 total train loader: 100 Loss: 0.573 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 66 total train loader: 100 Loss: 0.363 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 67 total train loader: 100 Loss: 0.510 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 68 total train loader: 100 Loss: 0.377 | Acc: 0.900 | F1: 0.900 | Recall: 0.900 \n",
      "epoch: 0 batch: 69 total train loader: 100 Loss: 0.352 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 70 total train loader: 100 Loss: 0.544 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 71 total train loader: 100 Loss: 0.400 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 72 total train loader: 100 Loss: 0.230 | Acc: 0.920 | F1: 0.920 | Recall: 0.920 \n",
      "epoch: 0 batch: 73 total train loader: 100 Loss: 0.473 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 74 total train loader: 100 Loss: 0.457 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 75 total train loader: 100 Loss: 0.392 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 76 total train loader: 100 Loss: 0.358 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 77 total train loader: 100 Loss: 0.362 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 78 total train loader: 100 Loss: 0.426 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 79 total train loader: 100 Loss: 0.380 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 80 total train loader: 100 Loss: 0.346 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 81 total train loader: 100 Loss: 0.342 | Acc: 0.880 | F1: 0.880 | Recall: 0.880 \n",
      "epoch: 0 batch: 82 total train loader: 100 Loss: 0.528 | Acc: 0.780 | F1: 0.780 | Recall: 0.780 \n",
      "epoch: 0 batch: 83 total train loader: 100 Loss: 0.389 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 84 total train loader: 100 Loss: 0.409 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 85 total train loader: 100 Loss: 0.584 | Acc: 0.790 | F1: 0.790 | Recall: 0.790 \n",
      "epoch: 0 batch: 86 total train loader: 100 Loss: 0.298 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 87 total train loader: 100 Loss: 0.478 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 88 total train loader: 100 Loss: 0.382 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 89 total train loader: 100 Loss: 0.394 | Acc: 0.880 | F1: 0.880 | Recall: 0.880 \n",
      "epoch: 0 batch: 90 total train loader: 100 Loss: 0.512 | Acc: 0.780 | F1: 0.780 | Recall: 0.780 \n",
      "epoch: 0 batch: 91 total train loader: 100 Loss: 0.452 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 92 total train loader: 100 Loss: 0.422 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 93 total train loader: 100 Loss: 0.516 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 94 total train loader: 100 Loss: 0.421 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 95 total train loader: 100 Loss: 0.447 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 96 total train loader: 100 Loss: 0.357 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 97 total train loader: 100 Loss: 0.510 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 98 total train loader: 100 Loss: 0.333 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 99 total train loader: 100 Loss: 0.405 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n"
     ]
    }
   ],
   "source": [
    "att_bilstm_test_result = pd.DataFrame()\n",
    "att_bilstm_test_result = att_bilstm_test_result.append(test(0, model_att_bilstm), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_bilstm_test_result.to_csv('../result/Attention-Based-Bi-LSTM/att_bilstm_test_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8303999999999998"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_bilstm_test_result['acc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8303999999999998"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_bilstm_test_result['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8303999999999998"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_bilstm_test_result['recall'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step14: Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x468 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "y1 = np.array(att_bilstm_train_result['roll_730_acc'])\n",
    "y2 = np.array(att_bilstm_train_result['roll_730_loss'])\n",
    "\n",
    "x = np.arange(len(y1))\n",
    "\n",
    "plt.figure(figsize=(10, 6.5))\n",
    "\n",
    "plt.plot(x, y1, color=\"r\", linewidth=1, label='accuracy')\n",
    "plt.plot(x, y2, color=\"b\", linewidth=1, label='loss')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Attetion-Based BiLSTM')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(att_bilstm_train_result['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61, 0.66, 0.61, ..., 0.8 , 0.88, 0.91])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
