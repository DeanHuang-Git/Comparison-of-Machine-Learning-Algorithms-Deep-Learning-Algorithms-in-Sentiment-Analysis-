{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step0: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import tensorwatch as tw\n",
    "from sklearn import metrics\n",
    "from torchviz import make_dot\n",
    "from string import punctuation\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torchvision.models import AlexNet\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"../data/english_yep_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Count the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['someon', 'ha', 'work', 'mani', 'museum', 'wa', 'eager', 'visit', 'thi', 'galleri']\n",
      "Top ten occuring words :  [('wa', 199857), ('thi', 86639), ('place', 55772), ('food', 53489), ('good', 50852), ('great', 44401), ('veri', 44062), ('time', 42695), ('get', 38251), ('would', 38160)]\n"
     ]
    }
   ],
   "source": [
    "all_reviews = list(reviews['cleaned'])\n",
    "all_text = \" \".join(all_reviews)\n",
    "all_words = all_text.split()\n",
    "print(all_words[0:10])\n",
    "\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(all_words)\n",
    "total_words = len(all_words)\n",
    "sorted_words=count_words.most_common(total_words)\n",
    "print(\"Top ten occuring words : \", sorted_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "# We will start createing dictionary with index 1 because 0 \n",
    "    # is reserved for padding\n",
    "\n",
    "vocab_to_int = {w: i+1 for i, (w, c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4: Encode the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode review\n",
    "encoded_reviews = list()\n",
    "for review in all_reviews:\n",
    "    encoded_review = list()\n",
    "    for word in review.split():\n",
    "        if word not in vocab_to_int.keys():\n",
    "            # if word is not available in vocab_to_int put 0 in that place\n",
    "            encoded_review.append(0)\n",
    "        else:\n",
    "            encoded_review.append(vocab_to_int[word])\n",
    "    encoded_reviews.append(encoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Make the encode_review of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all the encoded_review of the same length\n",
    "# this step will return features of review_ints,\n",
    "# where each review is padded with 0's or truncated to the input seq_length.\n",
    "# the longest review has 564 words\n",
    "# sequence_length is 100, but also could be 150, 200, 250 (here just for save energy)\n",
    "\n",
    "sequence_length = 100\n",
    "features = np.zeros((len(encoded_reviews), sequence_length), dtype=int)\n",
    "for i, review in enumerate(encoded_reviews):\n",
    "    review_len = len(review)\n",
    "    if review_len <= sequence_length:\n",
    "        zeros = list(np.zeros(sequence_length-review_len))\n",
    "        new = zeros+review\n",
    "    else:\n",
    "        new = review[:sequence_length]\n",
    "    features[i, :] = np.array(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6: Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels, 0 negative, 1 neutral, 2 positive\n",
    "labels = list(reviews['Review_Labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step7: Split this feature data into Traning, Testing and Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79999 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# split this feature data into training and validation set\n",
    "# 80% training, 10% test and 10% validation dataset\n",
    "\n",
    "# However, for cpu running, set 10% of them\n",
    "# features = features[:int(0.5*len(features))]\n",
    "# labels = labels[:int(0.5*len(labels))]\n",
    "train_x = features[:int(0.8*len(features))]\n",
    "train_y = labels[:int(0.8*len(features))]\n",
    "valid_x = features[int(0.8*len(features)):int(0.9*len(features))]\n",
    "valid_y = labels[int(0.8*len(features)):int(0.9*len(features))]\n",
    "test_x = features[int(0.9*len(features)):]\n",
    "test_y = labels[int(0.9*len(features)):]\n",
    "print(len(train_y), len(valid_y), len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step8: Create DataLoader objects for Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#create Tensor Dataset\n",
    "train_data=TensorDataset(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n",
    "valid_data=TensorDataset(torch.FloatTensor(valid_x), torch.FloatTensor(valid_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(test_x), torch.FloatTensor(test_y))\n",
    "\n",
    "#dataloader\n",
    "# remember to add drop_last=True, which will delete the last batch of the data if it's size is not equal to batch_size\n",
    "batch_size=100\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step9: Analyze the dataloader data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([100, 100])\n",
      "Sample input: \n",
      " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6900e+02, 1.2660e+03,\n",
      "         1.8480e+03],\n",
      "        [4.6000e+01, 7.9800e+02, 4.0900e+02,  ..., 1.9000e+01, 5.4900e+02,\n",
      "         4.5000e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.6100e+02, 1.1600e+02,\n",
      "         1.3680e+03],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.7800e+02, 8.7600e+02,\n",
      "         5.1000e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9060e+03, 1.0580e+03,\n",
      "         1.7890e+03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2000e+01, 1.0000e+00,\n",
      "         3.6240e+03]])\n",
      "Sample label size:  torch.Size([100])\n",
      "Sample label: \n",
      " tensor([2., 2., 0., 2., 2., 0., 2., 2., 0., 2., 1., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 0., 0., 1., 2., 1., 2., 0.,\n",
      "        2., 2., 2., 2., 2., 2., 0., 0., 0., 2., 0., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        0., 0., 2., 1., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 0., 2., 0.,\n",
      "        2., 0., 0., 0., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 0., 2.,\n",
      "        1., 2., 2., 2., 2., 1., 2., 2., 2., 0.])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step10-1: Create a Bi-LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embed, hidden_size, num_layers, num_classes, dropout, bidirectional):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed)\n",
    "        self.lstm = nn.LSTM(embed, hidden_size, num_layers,\n",
    "                            bidirectional=bidirectional, batch_first=True, dropout=dropout)  # initialize Bi-LSTM model\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # This task only has three classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.embedding(x)  # [batch_size, seq_len, embeding]=[100, 100, 300]\n",
    "        out, _ = self.lstm(out)  # the output of lstm are out layer, c layer and hidden layer\n",
    "        out = self.fc(out[:, -1, :])  # last hidden state of the sequence\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step11-1: Initialize the Bi-LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-LSTM\n",
      "LSTM_Model(\n",
      "  (embedding): Embedding(77398, 300)\n",
      "  (lstm): LSTM(300, 25, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hperparams\n",
    "vocab_size = len(vocab_to_int) + 1  # +1 for the 0 padding, 0.1 for the 10% of the total data (see step7)\n",
    "output_size = 1\n",
    "embed = 300\n",
    "hidden_size = 25\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "num_classes = 3\n",
    "epochs = 10\n",
    "bidirectional = False\n",
    "\n",
    "model_bilstm = LSTM_Model(vocab_size, output_size, embed, hidden_size, num_layers, num_classes, dropout, True)\n",
    "print('Bi-LSTM')\n",
    "print(model_bilstm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step12: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model):\n",
    "    lr = 0.0001  # learning rate\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "    total_batch = 0\n",
    "    train_loss = 0\n",
    "    result_train_dict = {}\n",
    "    result_valid_dict = {}\n",
    "    epoch_train_list = []\n",
    "    epoch_valid_list = []\n",
    "    batch_train_list = []\n",
    "    batch_valid_list = []\n",
    "    loss_train_list = []\n",
    "    loss_valid_list = []\n",
    "    acc_train_list = []\n",
    "    acc_valid_list = []\n",
    "    f1_train_list = []\n",
    "    f1_valid_list = []\n",
    "    recall_train_list = []\n",
    "    recall_valid_list = []\n",
    "    for batch_idx, (trains, labels) in enumerate(train_loader):\n",
    "        \n",
    "        outputs = model(trains.long())\n",
    "        model.zero_grad()\n",
    "        loss = F.cross_entropy(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        true = labels.data.cpu()\n",
    "        predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "        train_acc = metrics.accuracy_score(true, predic)\n",
    "        train_f1 = metrics.f1_score(true, predic, average='micro')\n",
    "        train_recall = metrics.recall_score(true, predic, average='micro')\n",
    "        loss_value = train_loss/(batch_idx+1)\n",
    "        \n",
    "        epoch_train_list.append(epoch)\n",
    "        batch_train_list.append(batch_idx)\n",
    "        loss_train_list.append(loss_value)\n",
    "        acc_train_list.append(train_acc)\n",
    "        f1_train_list.append(train_f1)\n",
    "        recall_train_list.append(train_recall)\n",
    "\n",
    "        if total_batch % 100 == 0 :\n",
    "            \n",
    "            for trains, labels in valid_loader:\n",
    "                outputs = model(trains.long())\n",
    "                valid_loss = F.cross_entropy(outputs, labels.long())\n",
    "                \n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                valid_acc = metrics.accuracy_score(true, predic)\n",
    "                valid_f1 = metrics.f1_score(true, predic, average='micro')\n",
    "                valid_recall = metrics.recall_score(true, predic, average='micro')\n",
    "                \n",
    "                epoch_valid_list.append(epoch)\n",
    "                batch_valid_list.append(batch_idx)\n",
    "                loss_valid_list.append(valid_loss.item())\n",
    "                acc_valid_list.append(valid_acc)\n",
    "                f1_valid_list.append(valid_f1)\n",
    "                recall_valid_list.append(valid_recall)\n",
    "                \n",
    "                avg_acc_valid = np.array(acc_valid_list).mean()\n",
    "                avg_loss_valid = np.array(loss_valid_list).mean()\n",
    "                \n",
    "            print('epoch: {}'.format(epoch), 'batch: {}'.format(batch_idx), \n",
    "                  'total train loader: {}'.format(len(train_loader)),\n",
    "                  'T_Loss: %.3f | T_Acc: %.3f |' % (loss_value, train_acc),\n",
    "                  'T_f1: %.3f | T_recall: %.3f ||' % (train_f1, train_recall),\n",
    "                  'V_Loss: %.3f | V_Acc: %.3f' % (avg_loss_valid, avg_acc_valid),\n",
    "                  'V_f1: %.3f | V_recall: %.3f' % (valid_f1, valid_recall))\n",
    "        total_batch += 1\n",
    "        \n",
    "    result_train_dict['epoch'] = epoch_train_list\n",
    "    result_train_dict['batch'] = batch_train_list\n",
    "    result_train_dict['loss'] = loss_train_list\n",
    "    result_train_dict['acc'] = acc_train_list\n",
    "    result_train_dict['f1'] = f1_train_list\n",
    "    result_train_dict['recall'] = recall_train_list\n",
    "    \n",
    "    result_valid_dict['epoch'] = epoch_valid_list\n",
    "    result_valid_dict['batch'] = batch_valid_list\n",
    "    result_valid_dict['loss'] = loss_valid_list\n",
    "    result_valid_dict['acc'] = acc_valid_list\n",
    "    result_valid_dict['f1'] = f1_valid_list\n",
    "    result_valid_dict['recall'] = recall_valid_list\n",
    "    \n",
    "    pd_train = pd.DataFrame(result_train_dict)\n",
    "    pd_valid = pd.DataFrame(result_valid_dict)\n",
    "\n",
    "    return pd_train, pd_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step12-1. Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 total train loader: 799 T_Loss: 1.136 | T_Acc: 0.190 | T_f1: 0.190 | T_recall: 0.190 || V_Loss: 1.133 | V_Acc: 0.164 V_f1: 0.150 | V_recall: 0.150\n",
      "epoch: 0 batch: 100 total train loader: 799 T_Loss: 1.106 | T_Acc: 0.570 | T_f1: 0.570 | T_recall: 0.570 || V_Loss: 1.106 | V_Acc: 0.337 V_f1: 0.580 | V_recall: 0.580\n",
      "epoch: 0 batch: 200 total train loader: 799 T_Loss: 1.076 | T_Acc: 0.750 | T_f1: 0.750 | T_recall: 0.750 || V_Loss: 1.072 | V_Acc: 0.447 V_f1: 0.700 | V_recall: 0.700\n",
      "epoch: 0 batch: 300 total train loader: 799 T_Loss: 1.032 | T_Acc: 0.710 | T_f1: 0.710 | T_recall: 0.710 || V_Loss: 1.023 | V_Acc: 0.503 V_f1: 0.630 | V_recall: 0.630\n",
      "epoch: 0 batch: 400 total train loader: 799 T_Loss: 0.983 | T_Acc: 0.660 | T_f1: 0.660 | T_recall: 0.660 || V_Loss: 0.984 | V_Acc: 0.536 V_f1: 0.690 | V_recall: 0.690\n",
      "epoch: 0 batch: 500 total train loader: 799 T_Loss: 0.952 | T_Acc: 0.630 | T_f1: 0.630 | T_recall: 0.630 || V_Loss: 0.956 | V_Acc: 0.558 V_f1: 0.680 | V_recall: 0.680\n",
      "epoch: 0 batch: 600 total train loader: 799 T_Loss: 0.928 | T_Acc: 0.640 | T_f1: 0.640 | T_recall: 0.640 || V_Loss: 0.933 | V_Acc: 0.574 V_f1: 0.620 | V_recall: 0.620\n",
      "epoch: 0 batch: 700 total train loader: 799 T_Loss: 0.906 | T_Acc: 0.720 | T_f1: 0.720 | T_recall: 0.720 || V_Loss: 0.910 | V_Acc: 0.586 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 1 batch: 0 total train loader: 799 T_Loss: 0.815 | T_Acc: 0.630 | T_f1: 0.630 | T_recall: 0.630 || V_Loss: 0.702 | V_Acc: 0.688 V_f1: 0.680 | V_recall: 0.680\n",
      "epoch: 1 batch: 100 total train loader: 799 T_Loss: 0.703 | T_Acc: 0.660 | T_f1: 0.660 | T_recall: 0.660 || V_Loss: 0.695 | V_Acc: 0.695 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 1 batch: 200 total train loader: 799 T_Loss: 0.693 | T_Acc: 0.700 | T_f1: 0.700 | T_recall: 0.700 || V_Loss: 0.688 | V_Acc: 0.701 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 1 batch: 300 total train loader: 799 T_Loss: 0.686 | T_Acc: 0.740 | T_f1: 0.740 | T_recall: 0.740 || V_Loss: 0.684 | V_Acc: 0.707 V_f1: 0.650 | V_recall: 0.650\n",
      "epoch: 1 batch: 400 total train loader: 799 T_Loss: 0.681 | T_Acc: 0.730 | T_f1: 0.730 | T_recall: 0.730 || V_Loss: 0.679 | V_Acc: 0.711 V_f1: 0.710 | V_recall: 0.710\n",
      "epoch: 1 batch: 500 total train loader: 799 T_Loss: 0.675 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.673 | V_Acc: 0.716 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 1 batch: 600 total train loader: 799 T_Loss: 0.670 | T_Acc: 0.700 | T_f1: 0.700 | T_recall: 0.700 || V_Loss: 0.669 | V_Acc: 0.720 V_f1: 0.720 | V_recall: 0.720\n",
      "epoch: 1 batch: 700 total train loader: 799 T_Loss: 0.666 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.665 | V_Acc: 0.723 V_f1: 0.740 | V_recall: 0.740\n",
      "epoch: 2 batch: 0 total train loader: 799 T_Loss: 0.695 | T_Acc: 0.690 | T_f1: 0.690 | T_recall: 0.690 || V_Loss: 0.628 | V_Acc: 0.756 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 2 batch: 100 total train loader: 799 T_Loss: 0.628 | T_Acc: 0.660 | T_f1: 0.660 | T_recall: 0.660 || V_Loss: 0.626 | V_Acc: 0.756 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 2 batch: 200 total train loader: 799 T_Loss: 0.623 | T_Acc: 0.730 | T_f1: 0.730 | T_recall: 0.730 || V_Loss: 0.623 | V_Acc: 0.759 V_f1: 0.730 | V_recall: 0.730\n",
      "epoch: 2 batch: 300 total train loader: 799 T_Loss: 0.621 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.621 | V_Acc: 0.761 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 2 batch: 400 total train loader: 799 T_Loss: 0.620 | T_Acc: 0.710 | T_f1: 0.710 | T_recall: 0.710 || V_Loss: 0.618 | V_Acc: 0.763 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 2 batch: 500 total train loader: 799 T_Loss: 0.615 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.615 | V_Acc: 0.765 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 2 batch: 600 total train loader: 799 T_Loss: 0.611 | T_Acc: 0.750 | T_f1: 0.750 | T_recall: 0.750 || V_Loss: 0.612 | V_Acc: 0.767 V_f1: 0.720 | V_recall: 0.720\n",
      "epoch: 2 batch: 700 total train loader: 799 T_Loss: 0.609 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.609 | V_Acc: 0.768 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 3 batch: 0 total train loader: 799 T_Loss: 0.494 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.591 | V_Acc: 0.774 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 3 batch: 100 total train loader: 799 T_Loss: 0.581 | T_Acc: 0.730 | T_f1: 0.730 | T_recall: 0.730 || V_Loss: 0.590 | V_Acc: 0.774 V_f1: 0.730 | V_recall: 0.730\n",
      "epoch: 3 batch: 200 total train loader: 799 T_Loss: 0.581 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.586 | V_Acc: 0.777 V_f1: 0.700 | V_recall: 0.700\n",
      "epoch: 3 batch: 300 total train loader: 799 T_Loss: 0.581 | T_Acc: 0.730 | T_f1: 0.730 | T_recall: 0.730 || V_Loss: 0.584 | V_Acc: 0.778 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 3 batch: 400 total train loader: 799 T_Loss: 0.580 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.582 | V_Acc: 0.779 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 3 batch: 500 total train loader: 799 T_Loss: 0.577 | T_Acc: 0.770 | T_f1: 0.770 | T_recall: 0.770 || V_Loss: 0.580 | V_Acc: 0.781 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 3 batch: 600 total train loader: 799 T_Loss: 0.576 | T_Acc: 0.740 | T_f1: 0.740 | T_recall: 0.740 || V_Loss: 0.579 | V_Acc: 0.781 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 3 batch: 700 total train loader: 799 T_Loss: 0.576 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.577 | V_Acc: 0.782 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 4 batch: 0 total train loader: 799 T_Loss: 0.505 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.562 | V_Acc: 0.790 V_f1: 0.740 | V_recall: 0.740\n",
      "epoch: 4 batch: 100 total train loader: 799 T_Loss: 0.554 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.558 | V_Acc: 0.792 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 4 batch: 200 total train loader: 799 T_Loss: 0.555 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.557 | V_Acc: 0.792 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 4 batch: 300 total train loader: 799 T_Loss: 0.552 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.556 | V_Acc: 0.792 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 4 batch: 400 total train loader: 799 T_Loss: 0.550 | T_Acc: 0.770 | T_f1: 0.770 | T_recall: 0.770 || V_Loss: 0.555 | V_Acc: 0.792 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 4 batch: 500 total train loader: 799 T_Loss: 0.552 | T_Acc: 0.700 | T_f1: 0.700 | T_recall: 0.700 || V_Loss: 0.554 | V_Acc: 0.793 V_f1: 0.740 | V_recall: 0.740\n",
      "epoch: 4 batch: 600 total train loader: 799 T_Loss: 0.549 | T_Acc: 0.760 | T_f1: 0.760 | T_recall: 0.760 || V_Loss: 0.553 | V_Acc: 0.793 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 4 batch: 700 total train loader: 799 T_Loss: 0.548 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.552 | V_Acc: 0.794 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 5 batch: 0 total train loader: 799 T_Loss: 0.616 | T_Acc: 0.760 | T_f1: 0.760 | T_recall: 0.760 || V_Loss: 0.541 | V_Acc: 0.798 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 5 batch: 100 total train loader: 799 T_Loss: 0.550 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.541 | V_Acc: 0.798 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 5 batch: 200 total train loader: 799 T_Loss: 0.537 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.540 | V_Acc: 0.798 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 5 batch: 300 total train loader: 799 T_Loss: 0.536 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.538 | V_Acc: 0.799 V_f1: 0.880 | V_recall: 0.880\n",
      "epoch: 5 batch: 400 total train loader: 799 T_Loss: 0.529 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.536 | V_Acc: 0.800 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 5 batch: 500 total train loader: 799 T_Loss: 0.529 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.535 | V_Acc: 0.801 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 5 batch: 600 total train loader: 799 T_Loss: 0.527 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.534 | V_Acc: 0.801 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 5 batch: 700 total train loader: 799 T_Loss: 0.528 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.533 | V_Acc: 0.801 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 6 batch: 0 total train loader: 799 T_Loss: 0.485 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.531 | V_Acc: 0.800 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 6 batch: 100 total train loader: 799 T_Loss: 0.502 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.526 | V_Acc: 0.805 V_f1: 0.860 | V_recall: 0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 batch: 200 total train loader: 799 T_Loss: 0.507 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.524 | V_Acc: 0.805 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 6 batch: 300 total train loader: 799 T_Loss: 0.509 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.524 | V_Acc: 0.805 V_f1: 0.730 | V_recall: 0.730\n",
      "epoch: 6 batch: 400 total train loader: 799 T_Loss: 0.508 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.524 | V_Acc: 0.805 V_f1: 0.900 | V_recall: 0.900\n",
      "epoch: 6 batch: 500 total train loader: 799 T_Loss: 0.509 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.523 | V_Acc: 0.805 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 6 batch: 600 total train loader: 799 T_Loss: 0.510 | T_Acc: 0.770 | T_f1: 0.770 | T_recall: 0.770 || V_Loss: 0.521 | V_Acc: 0.806 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 6 batch: 700 total train loader: 799 T_Loss: 0.512 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.520 | V_Acc: 0.807 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 7 batch: 0 total train loader: 799 T_Loss: 0.482 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.516 | V_Acc: 0.809 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 7 batch: 100 total train loader: 799 T_Loss: 0.506 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.513 | V_Acc: 0.810 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 7 batch: 200 total train loader: 799 T_Loss: 0.501 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.512 | V_Acc: 0.809 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 7 batch: 300 total train loader: 799 T_Loss: 0.502 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.510 | V_Acc: 0.810 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 7 batch: 400 total train loader: 799 T_Loss: 0.501 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.509 | V_Acc: 0.811 V_f1: 0.750 | V_recall: 0.750\n",
      "epoch: 7 batch: 500 total train loader: 799 T_Loss: 0.499 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.508 | V_Acc: 0.812 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 7 batch: 600 total train loader: 799 T_Loss: 0.500 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.507 | V_Acc: 0.812 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 7 batch: 700 total train loader: 799 T_Loss: 0.499 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.506 | V_Acc: 0.812 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 8 batch: 0 total train loader: 799 T_Loss: 0.481 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.498 | V_Acc: 0.815 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 8 batch: 100 total train loader: 799 T_Loss: 0.488 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.501 | V_Acc: 0.813 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 8 batch: 200 total train loader: 799 T_Loss: 0.480 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.499 | V_Acc: 0.814 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 8 batch: 300 total train loader: 799 T_Loss: 0.483 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.498 | V_Acc: 0.814 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 8 batch: 400 total train loader: 799 T_Loss: 0.485 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.497 | V_Acc: 0.814 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 8 batch: 500 total train loader: 799 T_Loss: 0.486 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.497 | V_Acc: 0.815 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 8 batch: 600 total train loader: 799 T_Loss: 0.487 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.496 | V_Acc: 0.815 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 8 batch: 700 total train loader: 799 T_Loss: 0.485 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.496 | V_Acc: 0.815 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 9 batch: 0 total train loader: 799 T_Loss: 0.464 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.492 | V_Acc: 0.816 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 9 batch: 100 total train loader: 799 T_Loss: 0.472 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.492 | V_Acc: 0.816 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 9 batch: 200 total train loader: 799 T_Loss: 0.473 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.491 | V_Acc: 0.816 V_f1: 0.740 | V_recall: 0.740\n",
      "epoch: 9 batch: 300 total train loader: 799 T_Loss: 0.474 | T_Acc: 0.770 | T_f1: 0.770 | T_recall: 0.770 || V_Loss: 0.491 | V_Acc: 0.816 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 9 batch: 400 total train loader: 799 T_Loss: 0.476 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.490 | V_Acc: 0.817 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 9 batch: 500 total train loader: 799 T_Loss: 0.475 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.490 | V_Acc: 0.817 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 9 batch: 600 total train loader: 799 T_Loss: 0.475 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.489 | V_Acc: 0.818 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 9 batch: 700 total train loader: 799 T_Loss: 0.477 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.489 | V_Acc: 0.818 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 10 batch: 0 total train loader: 799 T_Loss: 0.544 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.486 | V_Acc: 0.817 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 10 batch: 100 total train loader: 799 T_Loss: 0.472 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.486 | V_Acc: 0.816 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 10 batch: 200 total train loader: 799 T_Loss: 0.474 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.484 | V_Acc: 0.818 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 10 batch: 300 total train loader: 799 T_Loss: 0.475 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.484 | V_Acc: 0.818 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 10 batch: 400 total train loader: 799 T_Loss: 0.472 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.484 | V_Acc: 0.818 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 10 batch: 500 total train loader: 799 T_Loss: 0.472 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.483 | V_Acc: 0.818 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 10 batch: 600 total train loader: 799 T_Loss: 0.471 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.483 | V_Acc: 0.818 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 10 batch: 700 total train loader: 799 T_Loss: 0.470 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.482 | V_Acc: 0.819 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 11 batch: 0 total train loader: 799 T_Loss: 0.394 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.479 | V_Acc: 0.821 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 11 batch: 100 total train loader: 799 T_Loss: 0.454 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.480 | V_Acc: 0.821 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 11 batch: 200 total train loader: 799 T_Loss: 0.452 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.479 | V_Acc: 0.820 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 11 batch: 300 total train loader: 799 T_Loss: 0.455 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.478 | V_Acc: 0.821 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 11 batch: 400 total train loader: 799 T_Loss: 0.455 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.478 | V_Acc: 0.821 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 11 batch: 500 total train loader: 799 T_Loss: 0.457 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.478 | V_Acc: 0.821 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 11 batch: 600 total train loader: 799 T_Loss: 0.457 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.478 | V_Acc: 0.821 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 11 batch: 700 total train loader: 799 T_Loss: 0.459 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.478 | V_Acc: 0.821 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 12 batch: 0 total train loader: 799 T_Loss: 0.447 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.472 | V_Acc: 0.823 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 12 batch: 100 total train loader: 799 T_Loss: 0.459 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.475 | V_Acc: 0.821 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 12 batch: 200 total train loader: 799 T_Loss: 0.457 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.475 | V_Acc: 0.821 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 12 batch: 300 total train loader: 799 T_Loss: 0.449 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.473 | V_Acc: 0.822 V_f1: 0.840 | V_recall: 0.840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 batch: 400 total train loader: 799 T_Loss: 0.450 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.473 | V_Acc: 0.822 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 12 batch: 500 total train loader: 799 T_Loss: 0.452 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.473 | V_Acc: 0.822 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 12 batch: 600 total train loader: 799 T_Loss: 0.453 | T_Acc: 0.770 | T_f1: 0.770 | T_recall: 0.770 || V_Loss: 0.473 | V_Acc: 0.822 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 12 batch: 700 total train loader: 799 T_Loss: 0.453 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.473 | V_Acc: 0.822 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 13 batch: 0 total train loader: 799 T_Loss: 0.337 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.472 | V_Acc: 0.821 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 13 batch: 100 total train loader: 799 T_Loss: 0.457 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.476 | V_Acc: 0.820 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 13 batch: 200 total train loader: 799 T_Loss: 0.456 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.475 | V_Acc: 0.821 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 13 batch: 300 total train loader: 799 T_Loss: 0.457 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.477 | V_Acc: 0.820 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 13 batch: 400 total train loader: 799 T_Loss: 0.455 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.477 | V_Acc: 0.820 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 13 batch: 500 total train loader: 799 T_Loss: 0.454 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.475 | V_Acc: 0.821 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 13 batch: 600 total train loader: 799 T_Loss: 0.454 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.474 | V_Acc: 0.821 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 13 batch: 700 total train loader: 799 T_Loss: 0.452 | T_Acc: 0.900 | T_f1: 0.900 | T_recall: 0.900 || V_Loss: 0.474 | V_Acc: 0.821 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 14 batch: 0 total train loader: 799 T_Loss: 0.335 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.475 | V_Acc: 0.820 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 14 batch: 100 total train loader: 799 T_Loss: 0.452 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.474 | V_Acc: 0.822 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 14 batch: 200 total train loader: 799 T_Loss: 0.447 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.473 | V_Acc: 0.822 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 14 batch: 300 total train loader: 799 T_Loss: 0.447 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.473 | V_Acc: 0.822 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 14 batch: 400 total train loader: 799 T_Loss: 0.447 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.472 | V_Acc: 0.822 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 14 batch: 500 total train loader: 799 T_Loss: 0.447 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.471 | V_Acc: 0.822 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 14 batch: 600 total train loader: 799 T_Loss: 0.448 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.471 | V_Acc: 0.822 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 14 batch: 700 total train loader: 799 T_Loss: 0.448 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.470 | V_Acc: 0.822 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 15 batch: 0 total train loader: 799 T_Loss: 0.456 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.465 | V_Acc: 0.825 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 15 batch: 100 total train loader: 799 T_Loss: 0.426 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.465 | V_Acc: 0.824 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 15 batch: 200 total train loader: 799 T_Loss: 0.432 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.466 | V_Acc: 0.824 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 15 batch: 300 total train loader: 799 T_Loss: 0.437 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.470 | V_Acc: 0.822 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 15 batch: 400 total train loader: 799 T_Loss: 0.441 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.471 | V_Acc: 0.822 V_f1: 0.770 | V_recall: 0.770\n",
      "epoch: 15 batch: 500 total train loader: 799 T_Loss: 0.441 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.470 | V_Acc: 0.823 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 15 batch: 600 total train loader: 799 T_Loss: 0.441 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.469 | V_Acc: 0.823 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 15 batch: 700 total train loader: 799 T_Loss: 0.443 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.468 | V_Acc: 0.823 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 16 batch: 0 total train loader: 799 T_Loss: 0.487 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.465 | V_Acc: 0.823 V_f1: 0.880 | V_recall: 0.880\n",
      "epoch: 16 batch: 100 total train loader: 799 T_Loss: 0.441 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.464 | V_Acc: 0.822 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 16 batch: 200 total train loader: 799 T_Loss: 0.441 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.465 | V_Acc: 0.823 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 16 batch: 300 total train loader: 799 T_Loss: 0.442 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.465 | V_Acc: 0.823 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 16 batch: 400 total train loader: 799 T_Loss: 0.437 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.465 | V_Acc: 0.824 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 16 batch: 500 total train loader: 799 T_Loss: 0.438 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.465 | V_Acc: 0.824 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 16 batch: 600 total train loader: 799 T_Loss: 0.438 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.465 | V_Acc: 0.824 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 16 batch: 700 total train loader: 799 T_Loss: 0.438 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.464 | V_Acc: 0.824 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 17 batch: 0 total train loader: 799 T_Loss: 0.336 | T_Acc: 0.930 | T_f1: 0.930 | T_recall: 0.930 || V_Loss: 0.461 | V_Acc: 0.824 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 17 batch: 100 total train loader: 799 T_Loss: 0.425 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.462 | V_Acc: 0.823 V_f1: 0.900 | V_recall: 0.900\n",
      "epoch: 17 batch: 200 total train loader: 799 T_Loss: 0.436 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.464 | V_Acc: 0.823 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 17 batch: 300 total train loader: 799 T_Loss: 0.433 | T_Acc: 0.760 | T_f1: 0.760 | T_recall: 0.760 || V_Loss: 0.465 | V_Acc: 0.823 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 17 batch: 400 total train loader: 799 T_Loss: 0.431 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.465 | V_Acc: 0.823 V_f1: 0.880 | V_recall: 0.880\n",
      "epoch: 17 batch: 500 total train loader: 799 T_Loss: 0.432 | T_Acc: 0.780 | T_f1: 0.780 | T_recall: 0.780 || V_Loss: 0.465 | V_Acc: 0.823 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 17 batch: 600 total train loader: 799 T_Loss: 0.433 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.464 | V_Acc: 0.823 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 17 batch: 700 total train loader: 799 T_Loss: 0.434 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.463 | V_Acc: 0.824 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 18 batch: 0 total train loader: 799 T_Loss: 0.358 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.472 | V_Acc: 0.823 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 18 batch: 100 total train loader: 799 T_Loss: 0.430 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.464 | V_Acc: 0.824 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 18 batch: 200 total train loader: 799 T_Loss: 0.430 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.465 | V_Acc: 0.824 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 18 batch: 300 total train loader: 799 T_Loss: 0.429 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.462 | V_Acc: 0.824 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 18 batch: 400 total train loader: 799 T_Loss: 0.431 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.461 | V_Acc: 0.825 V_f1: 0.910 | V_recall: 0.910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 batch: 500 total train loader: 799 T_Loss: 0.432 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.460 | V_Acc: 0.825 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 18 batch: 600 total train loader: 799 T_Loss: 0.432 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.461 | V_Acc: 0.825 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 18 batch: 700 total train loader: 799 T_Loss: 0.433 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.460 | V_Acc: 0.825 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 19 batch: 0 total train loader: 799 T_Loss: 0.392 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.457 | V_Acc: 0.826 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 19 batch: 100 total train loader: 799 T_Loss: 0.425 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.459 | V_Acc: 0.825 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 19 batch: 200 total train loader: 799 T_Loss: 0.431 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.460 | V_Acc: 0.825 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 19 batch: 300 total train loader: 799 T_Loss: 0.427 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.460 | V_Acc: 0.825 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 19 batch: 400 total train loader: 799 T_Loss: 0.430 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.459 | V_Acc: 0.826 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 19 batch: 500 total train loader: 799 T_Loss: 0.431 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.459 | V_Acc: 0.825 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 19 batch: 600 total train loader: 799 T_Loss: 0.431 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.458 | V_Acc: 0.825 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 19 batch: 700 total train loader: 799 T_Loss: 0.428 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.457 | V_Acc: 0.826 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 20 batch: 0 total train loader: 799 T_Loss: 0.387 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.483 | V_Acc: 0.819 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 20 batch: 100 total train loader: 799 T_Loss: 0.424 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.469 | V_Acc: 0.822 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 20 batch: 200 total train loader: 799 T_Loss: 0.425 | T_Acc: 0.770 | T_f1: 0.770 | T_recall: 0.770 || V_Loss: 0.464 | V_Acc: 0.823 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 20 batch: 300 total train loader: 799 T_Loss: 0.425 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.463 | V_Acc: 0.824 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 20 batch: 400 total train loader: 799 T_Loss: 0.425 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.462 | V_Acc: 0.825 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 20 batch: 500 total train loader: 799 T_Loss: 0.429 | T_Acc: 0.760 | T_f1: 0.760 | T_recall: 0.760 || V_Loss: 0.460 | V_Acc: 0.825 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 20 batch: 600 total train loader: 799 T_Loss: 0.426 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.460 | V_Acc: 0.825 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 20 batch: 700 total train loader: 799 T_Loss: 0.426 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.459 | V_Acc: 0.825 V_f1: 0.800 | V_recall: 0.800\n",
      "epoch: 21 batch: 0 total train loader: 799 T_Loss: 0.452 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.453 | V_Acc: 0.828 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 21 batch: 100 total train loader: 799 T_Loss: 0.414 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.451 | V_Acc: 0.828 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 21 batch: 200 total train loader: 799 T_Loss: 0.419 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.453 | V_Acc: 0.828 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 21 batch: 300 total train loader: 799 T_Loss: 0.419 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.451 | V_Acc: 0.828 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 21 batch: 400 total train loader: 799 T_Loss: 0.418 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.452 | V_Acc: 0.828 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 21 batch: 500 total train loader: 799 T_Loss: 0.420 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.452 | V_Acc: 0.827 V_f1: 0.910 | V_recall: 0.910\n",
      "epoch: 21 batch: 600 total train loader: 799 T_Loss: 0.420 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.451 | V_Acc: 0.828 V_f1: 0.880 | V_recall: 0.880\n",
      "epoch: 21 batch: 700 total train loader: 799 T_Loss: 0.419 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.450 | V_Acc: 0.828 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 22 batch: 0 total train loader: 799 T_Loss: 0.433 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.442 | V_Acc: 0.830 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 22 batch: 100 total train loader: 799 T_Loss: 0.405 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.457 | V_Acc: 0.826 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 22 batch: 200 total train loader: 799 T_Loss: 0.410 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.454 | V_Acc: 0.827 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 22 batch: 300 total train loader: 799 T_Loss: 0.408 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.452 | V_Acc: 0.827 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 22 batch: 400 total train loader: 799 T_Loss: 0.409 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.451 | V_Acc: 0.827 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 22 batch: 500 total train loader: 799 T_Loss: 0.412 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.451 | V_Acc: 0.827 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 22 batch: 600 total train loader: 799 T_Loss: 0.415 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.450 | V_Acc: 0.827 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 22 batch: 700 total train loader: 799 T_Loss: 0.416 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.449 | V_Acc: 0.828 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 23 batch: 0 total train loader: 799 T_Loss: 0.381 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.437 | V_Acc: 0.831 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 23 batch: 100 total train loader: 799 T_Loss: 0.400 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.441 | V_Acc: 0.830 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 23 batch: 200 total train loader: 799 T_Loss: 0.405 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.442 | V_Acc: 0.829 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 23 batch: 300 total train loader: 799 T_Loss: 0.404 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.441 | V_Acc: 0.829 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 23 batch: 400 total train loader: 799 T_Loss: 0.405 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.439 | V_Acc: 0.830 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 23 batch: 500 total train loader: 799 T_Loss: 0.407 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.439 | V_Acc: 0.830 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 23 batch: 600 total train loader: 799 T_Loss: 0.409 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.438 | V_Acc: 0.831 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 23 batch: 700 total train loader: 799 T_Loss: 0.410 | T_Acc: 0.910 | T_f1: 0.910 | T_recall: 0.910 || V_Loss: 0.438 | V_Acc: 0.831 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 24 batch: 0 total train loader: 799 T_Loss: 0.547 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.451 | V_Acc: 0.826 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 24 batch: 100 total train loader: 799 T_Loss: 0.397 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.448 | V_Acc: 0.829 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 24 batch: 200 total train loader: 799 T_Loss: 0.400 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.443 | V_Acc: 0.829 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 24 batch: 300 total train loader: 799 T_Loss: 0.402 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.441 | V_Acc: 0.830 V_f1: 0.880 | V_recall: 0.880\n",
      "epoch: 24 batch: 400 total train loader: 799 T_Loss: 0.403 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.441 | V_Acc: 0.831 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 24 batch: 500 total train loader: 799 T_Loss: 0.403 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.440 | V_Acc: 0.831 V_f1: 0.830 | V_recall: 0.830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 batch: 600 total train loader: 799 T_Loss: 0.403 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.439 | V_Acc: 0.831 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 24 batch: 700 total train loader: 799 T_Loss: 0.404 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.438 | V_Acc: 0.831 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 25 batch: 0 total train loader: 799 T_Loss: 0.373 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.439 | V_Acc: 0.832 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 25 batch: 100 total train loader: 799 T_Loss: 0.377 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.436 | V_Acc: 0.832 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 25 batch: 200 total train loader: 799 T_Loss: 0.380 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.435 | V_Acc: 0.833 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 25 batch: 300 total train loader: 799 T_Loss: 0.386 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.436 | V_Acc: 0.832 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 25 batch: 400 total train loader: 799 T_Loss: 0.389 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.435 | V_Acc: 0.833 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 25 batch: 500 total train loader: 799 T_Loss: 0.393 | T_Acc: 0.900 | T_f1: 0.900 | T_recall: 0.900 || V_Loss: 0.434 | V_Acc: 0.833 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 25 batch: 600 total train loader: 799 T_Loss: 0.396 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.433 | V_Acc: 0.834 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 25 batch: 700 total train loader: 799 T_Loss: 0.398 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.432 | V_Acc: 0.834 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 26 batch: 0 total train loader: 799 T_Loss: 0.313 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.433 | V_Acc: 0.834 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 26 batch: 100 total train loader: 799 T_Loss: 0.392 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.428 | V_Acc: 0.835 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 26 batch: 200 total train loader: 799 T_Loss: 0.395 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.428 | V_Acc: 0.834 V_f1: 0.780 | V_recall: 0.780\n",
      "epoch: 26 batch: 300 total train loader: 799 T_Loss: 0.396 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.426 | V_Acc: 0.835 V_f1: 0.850 | V_recall: 0.850\n",
      "epoch: 26 batch: 400 total train loader: 799 T_Loss: 0.396 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.424 | V_Acc: 0.836 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 26 batch: 500 total train loader: 799 T_Loss: 0.394 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.424 | V_Acc: 0.837 V_f1: 0.790 | V_recall: 0.790\n",
      "epoch: 26 batch: 600 total train loader: 799 T_Loss: 0.394 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.424 | V_Acc: 0.837 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 26 batch: 700 total train loader: 799 T_Loss: 0.394 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.423 | V_Acc: 0.837 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 27 batch: 0 total train loader: 799 T_Loss: 0.308 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.426 | V_Acc: 0.834 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 27 batch: 100 total train loader: 799 T_Loss: 0.382 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.423 | V_Acc: 0.838 V_f1: 0.900 | V_recall: 0.900\n",
      "epoch: 27 batch: 200 total train loader: 799 T_Loss: 0.382 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.424 | V_Acc: 0.839 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 27 batch: 300 total train loader: 799 T_Loss: 0.383 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.423 | V_Acc: 0.839 V_f1: 0.820 | V_recall: 0.820\n",
      "epoch: 27 batch: 400 total train loader: 799 T_Loss: 0.387 | T_Acc: 0.830 | T_f1: 0.830 | T_recall: 0.830 || V_Loss: 0.425 | V_Acc: 0.838 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 27 batch: 500 total train loader: 799 T_Loss: 0.388 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.424 | V_Acc: 0.838 V_f1: 0.910 | V_recall: 0.910\n",
      "epoch: 27 batch: 600 total train loader: 799 T_Loss: 0.389 | T_Acc: 0.870 | T_f1: 0.870 | T_recall: 0.870 || V_Loss: 0.423 | V_Acc: 0.838 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 27 batch: 700 total train loader: 799 T_Loss: 0.389 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.422 | V_Acc: 0.839 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 28 batch: 0 total train loader: 799 T_Loss: 0.381 | T_Acc: 0.860 | T_f1: 0.860 | T_recall: 0.860 || V_Loss: 0.412 | V_Acc: 0.842 V_f1: 0.880 | V_recall: 0.880\n",
      "epoch: 28 batch: 100 total train loader: 799 T_Loss: 0.385 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.418 | V_Acc: 0.840 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 28 batch: 200 total train loader: 799 T_Loss: 0.381 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.419 | V_Acc: 0.840 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 28 batch: 300 total train loader: 799 T_Loss: 0.381 | T_Acc: 0.820 | T_f1: 0.820 | T_recall: 0.820 || V_Loss: 0.419 | V_Acc: 0.840 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 28 batch: 400 total train loader: 799 T_Loss: 0.387 | T_Acc: 0.810 | T_f1: 0.810 | T_recall: 0.810 || V_Loss: 0.419 | V_Acc: 0.840 V_f1: 0.890 | V_recall: 0.890\n",
      "epoch: 28 batch: 500 total train loader: 799 T_Loss: 0.386 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.419 | V_Acc: 0.840 V_f1: 0.760 | V_recall: 0.760\n",
      "epoch: 28 batch: 600 total train loader: 799 T_Loss: 0.387 | T_Acc: 0.850 | T_f1: 0.850 | T_recall: 0.850 || V_Loss: 0.418 | V_Acc: 0.840 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 28 batch: 700 total train loader: 799 T_Loss: 0.387 | T_Acc: 0.880 | T_f1: 0.880 | T_recall: 0.880 || V_Loss: 0.418 | V_Acc: 0.840 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 29 batch: 0 total train loader: 799 T_Loss: 0.273 | T_Acc: 0.920 | T_f1: 0.920 | T_recall: 0.920 || V_Loss: 0.420 | V_Acc: 0.838 V_f1: 0.840 | V_recall: 0.840\n",
      "epoch: 29 batch: 100 total train loader: 799 T_Loss: 0.379 | T_Acc: 0.840 | T_f1: 0.840 | T_recall: 0.840 || V_Loss: 0.418 | V_Acc: 0.841 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 29 batch: 200 total train loader: 799 T_Loss: 0.389 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.416 | V_Acc: 0.842 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 29 batch: 300 total train loader: 799 T_Loss: 0.386 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.415 | V_Acc: 0.841 V_f1: 0.870 | V_recall: 0.870\n",
      "epoch: 29 batch: 400 total train loader: 799 T_Loss: 0.383 | T_Acc: 0.790 | T_f1: 0.790 | T_recall: 0.790 || V_Loss: 0.416 | V_Acc: 0.841 V_f1: 0.860 | V_recall: 0.860\n",
      "epoch: 29 batch: 500 total train loader: 799 T_Loss: 0.385 | T_Acc: 0.800 | T_f1: 0.800 | T_recall: 0.800 || V_Loss: 0.415 | V_Acc: 0.841 V_f1: 0.830 | V_recall: 0.830\n",
      "epoch: 29 batch: 600 total train loader: 799 T_Loss: 0.384 | T_Acc: 0.890 | T_f1: 0.890 | T_recall: 0.890 || V_Loss: 0.415 | V_Acc: 0.841 V_f1: 0.810 | V_recall: 0.810\n",
      "epoch: 29 batch: 700 total train loader: 799 T_Loss: 0.383 | T_Acc: 0.750 | T_f1: 0.750 | T_recall: 0.750 || V_Loss: 0.415 | V_Acc: 0.840 V_f1: 0.840 | V_recall: 0.840\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "bilstm_train_result = pd.DataFrame()\n",
    "bilstm_valid_result = pd.DataFrame()\n",
    "for epoch in range(epochs):\n",
    "    train_result, valid_result = train(epoch, model_bilstm)\n",
    "    bilstm_train_result = bilstm_train_result.append(train_result, ignore_index=True)\n",
    "    bilstm_valid_result = bilstm_valid_result.append(train_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_train_result['roll_730_acc'] = bilstm_train_result['acc'].rolling(730).mean()\n",
    "bilstm_valid_result['roll_730_acc'] = bilstm_valid_result['acc'].rolling(730).mean()\n",
    "bilstm_train_result['roll_730_loss'] = bilstm_train_result['loss'].rolling(730).mean()\n",
    "bilstm_valid_result['roll_730_loss'] = bilstm_valid_result['loss'].rolling(730).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_train_result.to_csv('../result/Bi-LSTM/bilstm_train_result.csv')\n",
    "bilstm_valid_result.to_csv('../result/Bi-LSTM/bilstm_valid_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch             29.000000\n",
       "batch            798.000000\n",
       "loss               0.384338\n",
       "acc                0.890000\n",
       "f1                 0.890000\n",
       "recall             0.890000\n",
       "roll_730_acc       0.847507\n",
       "roll_730_loss      0.384324\n",
       "Name: 23969, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_valid_result.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step13: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model):\n",
    "    lr = 0.001  # learning rate\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    total_batch = 0\n",
    "    train_loss = 0\n",
    "    result_dict = {}\n",
    "    epoch_list = []\n",
    "    batch_list = []\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    \n",
    "    for batch_idx, (trains, labels) in enumerate(test_loader):\n",
    "\n",
    "        outputs = model(trains.long())\n",
    "        test_loss = F.cross_entropy(outputs, labels.long())\n",
    "\n",
    "        \n",
    "        true = labels.data.cpu()\n",
    "        predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "        test_acc = metrics.accuracy_score(true, predic)\n",
    "        test_f1 = metrics.f1_score(true, predic, average='micro')\n",
    "        test_recall = metrics.recall_score(true, predic, average='micro')\n",
    "        \n",
    "        epoch_list.append(epoch)\n",
    "        batch_list.append(batch_idx)\n",
    "        loss_list.append(test_loss.item())\n",
    "        acc_list.append(test_acc)\n",
    "        recall_list.append(test_recall)\n",
    "        f1_list.append(test_f1)\n",
    "            \n",
    "        if total_batch % 50 == 0 :\n",
    "            print('epoch: {}'.format(epoch), 'batch: {}'.format(batch_idx), \n",
    "                  'total train loader: {}'.format(len(test_loader)),\n",
    "                  'Loss: %.3f | Acc: %.3f |' % (test_loss, test_acc),\n",
    "                  'F1: %.3f | Recall: %.3f ' % (test_f1, test_recall))\n",
    "        \n",
    "#         total_batch += 1\n",
    "        \n",
    "    result_dict['epoch'] = epoch_list\n",
    "    result_dict['batch'] = batch_list\n",
    "    result_dict['loss'] = loss_list\n",
    "    result_dict['acc'] = acc_list\n",
    "    result_dict['f1'] = f1_list\n",
    "    result_dict['recall'] = recall_list\n",
    "    \n",
    "    return pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step13-1 Bi-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 total train loader: 100 Loss: 0.270 | Acc: 0.900 | F1: 0.900 | Recall: 0.900 \n",
      "epoch: 0 batch: 1 total train loader: 100 Loss: 0.432 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 2 total train loader: 100 Loss: 0.325 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 3 total train loader: 100 Loss: 0.515 | Acc: 0.790 | F1: 0.790 | Recall: 0.790 \n",
      "epoch: 0 batch: 4 total train loader: 100 Loss: 0.473 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 5 total train loader: 100 Loss: 0.351 | Acc: 0.880 | F1: 0.880 | Recall: 0.880 \n",
      "epoch: 0 batch: 6 total train loader: 100 Loss: 0.406 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 7 total train loader: 100 Loss: 0.310 | Acc: 0.880 | F1: 0.880 | Recall: 0.880 \n",
      "epoch: 0 batch: 8 total train loader: 100 Loss: 0.434 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 9 total train loader: 100 Loss: 0.381 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 10 total train loader: 100 Loss: 0.394 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 11 total train loader: 100 Loss: 0.447 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 12 total train loader: 100 Loss: 0.383 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 13 total train loader: 100 Loss: 0.504 | Acc: 0.770 | F1: 0.770 | Recall: 0.770 \n",
      "epoch: 0 batch: 14 total train loader: 100 Loss: 0.319 | Acc: 0.890 | F1: 0.890 | Recall: 0.890 \n",
      "epoch: 0 batch: 15 total train loader: 100 Loss: 0.381 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 16 total train loader: 100 Loss: 0.411 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 17 total train loader: 100 Loss: 0.410 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 18 total train loader: 100 Loss: 0.313 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 19 total train loader: 100 Loss: 0.413 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 20 total train loader: 100 Loss: 0.481 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 21 total train loader: 100 Loss: 0.486 | Acc: 0.780 | F1: 0.780 | Recall: 0.780 \n",
      "epoch: 0 batch: 22 total train loader: 100 Loss: 0.410 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 23 total train loader: 100 Loss: 0.470 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 24 total train loader: 100 Loss: 0.405 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 25 total train loader: 100 Loss: 0.517 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 26 total train loader: 100 Loss: 0.449 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 27 total train loader: 100 Loss: 0.411 | Acc: 0.880 | F1: 0.880 | Recall: 0.880 \n",
      "epoch: 0 batch: 28 total train loader: 100 Loss: 0.364 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 29 total train loader: 100 Loss: 0.340 | Acc: 0.900 | F1: 0.900 | Recall: 0.900 \n",
      "epoch: 0 batch: 30 total train loader: 100 Loss: 0.492 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 31 total train loader: 100 Loss: 0.392 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 32 total train loader: 100 Loss: 0.412 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 33 total train loader: 100 Loss: 0.363 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 34 total train loader: 100 Loss: 0.445 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 35 total train loader: 100 Loss: 0.476 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 36 total train loader: 100 Loss: 0.376 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 37 total train loader: 100 Loss: 0.547 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 38 total train loader: 100 Loss: 0.502 | Acc: 0.790 | F1: 0.790 | Recall: 0.790 \n",
      "epoch: 0 batch: 39 total train loader: 100 Loss: 0.534 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 40 total train loader: 100 Loss: 0.381 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 41 total train loader: 100 Loss: 0.431 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 42 total train loader: 100 Loss: 0.411 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 43 total train loader: 100 Loss: 0.421 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 44 total train loader: 100 Loss: 0.421 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 45 total train loader: 100 Loss: 0.340 | Acc: 0.890 | F1: 0.890 | Recall: 0.890 \n",
      "epoch: 0 batch: 46 total train loader: 100 Loss: 0.460 | Acc: 0.770 | F1: 0.770 | Recall: 0.770 \n",
      "epoch: 0 batch: 47 total train loader: 100 Loss: 0.459 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 48 total train loader: 100 Loss: 0.475 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 49 total train loader: 100 Loss: 0.418 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 50 total train loader: 100 Loss: 0.460 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 51 total train loader: 100 Loss: 0.414 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 52 total train loader: 100 Loss: 0.363 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 53 total train loader: 100 Loss: 0.474 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 54 total train loader: 100 Loss: 0.331 | Acc: 0.880 | F1: 0.880 | Recall: 0.880 \n",
      "epoch: 0 batch: 55 total train loader: 100 Loss: 0.390 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 56 total train loader: 100 Loss: 0.465 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 57 total train loader: 100 Loss: 0.514 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 58 total train loader: 100 Loss: 0.314 | Acc: 0.930 | F1: 0.930 | Recall: 0.930 \n",
      "epoch: 0 batch: 59 total train loader: 100 Loss: 0.481 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 60 total train loader: 100 Loss: 0.518 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 61 total train loader: 100 Loss: 0.433 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 62 total train loader: 100 Loss: 0.396 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 63 total train loader: 100 Loss: 0.404 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 64 total train loader: 100 Loss: 0.384 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 65 total train loader: 100 Loss: 0.501 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 66 total train loader: 100 Loss: 0.522 | Acc: 0.800 | F1: 0.800 | Recall: 0.800 \n",
      "epoch: 0 batch: 67 total train loader: 100 Loss: 0.356 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 68 total train loader: 100 Loss: 0.366 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 69 total train loader: 100 Loss: 0.471 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 70 total train loader: 100 Loss: 0.365 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 71 total train loader: 100 Loss: 0.343 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 72 total train loader: 100 Loss: 0.321 | Acc: 0.880 | F1: 0.880 | Recall: 0.880 \n",
      "epoch: 0 batch: 73 total train loader: 100 Loss: 0.608 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 74 total train loader: 100 Loss: 0.422 | Acc: 0.870 | F1: 0.870 | Recall: 0.870 \n",
      "epoch: 0 batch: 75 total train loader: 100 Loss: 0.510 | Acc: 0.790 | F1: 0.790 | Recall: 0.790 \n",
      "epoch: 0 batch: 76 total train loader: 100 Loss: 0.593 | Acc: 0.750 | F1: 0.750 | Recall: 0.750 \n",
      "epoch: 0 batch: 77 total train loader: 100 Loss: 0.381 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n",
      "epoch: 0 batch: 78 total train loader: 100 Loss: 0.421 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 79 total train loader: 100 Loss: 0.460 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 80 total train loader: 100 Loss: 0.457 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 81 total train loader: 100 Loss: 0.421 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 82 total train loader: 100 Loss: 0.654 | Acc: 0.730 | F1: 0.730 | Recall: 0.730 \n",
      "epoch: 0 batch: 83 total train loader: 100 Loss: 0.354 | Acc: 0.890 | F1: 0.890 | Recall: 0.890 \n",
      "epoch: 0 batch: 84 total train loader: 100 Loss: 0.406 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 85 total train loader: 100 Loss: 0.333 | Acc: 0.880 | F1: 0.880 | Recall: 0.880 \n",
      "epoch: 0 batch: 86 total train loader: 100 Loss: 0.327 | Acc: 0.900 | F1: 0.900 | Recall: 0.900 \n",
      "epoch: 0 batch: 87 total train loader: 100 Loss: 0.422 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 88 total train loader: 100 Loss: 0.540 | Acc: 0.780 | F1: 0.780 | Recall: 0.780 \n",
      "epoch: 0 batch: 89 total train loader: 100 Loss: 0.599 | Acc: 0.820 | F1: 0.820 | Recall: 0.820 \n",
      "epoch: 0 batch: 90 total train loader: 100 Loss: 0.394 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 91 total train loader: 100 Loss: 0.384 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 92 total train loader: 100 Loss: 0.358 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 93 total train loader: 100 Loss: 0.436 | Acc: 0.830 | F1: 0.830 | Recall: 0.830 \n",
      "epoch: 0 batch: 94 total train loader: 100 Loss: 0.440 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 95 total train loader: 100 Loss: 0.425 | Acc: 0.810 | F1: 0.810 | Recall: 0.810 \n",
      "epoch: 0 batch: 96 total train loader: 100 Loss: 0.337 | Acc: 0.850 | F1: 0.850 | Recall: 0.850 \n",
      "epoch: 0 batch: 97 total train loader: 100 Loss: 0.466 | Acc: 0.840 | F1: 0.840 | Recall: 0.840 \n",
      "epoch: 0 batch: 98 total train loader: 100 Loss: 0.607 | Acc: 0.750 | F1: 0.750 | Recall: 0.750 \n",
      "epoch: 0 batch: 99 total train loader: 100 Loss: 0.385 | Acc: 0.860 | F1: 0.860 | Recall: 0.860 \n"
     ]
    }
   ],
   "source": [
    "bilstm_test_result = pd.DataFrame()\n",
    "bilstm_test_result = bilstm_test_result.append(test(0, model_bilstm), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_test_result.to_csv('../result/Bi-LSTM/bilstm_test_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8360999999999998"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_test_result['acc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8360999999999997"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_test_result['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8360999999999998"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_test_result['recall'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step14: plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x468 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "y1 = np.array(bilstm_train_result['roll_730_acc'])\n",
    "y2 = np.array(bilstm_train_result['roll_730_loss'])\n",
    "\n",
    "x = np.arange(len(y1))\n",
    "\n",
    "plt.figure(figsize=(10, 6.5))\n",
    "\n",
    "plt.plot(x, y1, color=\"r\", linewidth=1, label='accuracy')\n",
    "plt.plot(x, y2, color=\"b\", linewidth=1, label='loss')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Value')\n",
    "plt.title('BiLSTM')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
