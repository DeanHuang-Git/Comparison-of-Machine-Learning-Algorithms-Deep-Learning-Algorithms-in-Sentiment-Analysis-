{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step0: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from string import punctuation\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"../data/english_yep_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Count the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['someon', 'ha', 'work', 'mani', 'museum', 'wa', 'eager', 'visit', 'thi', 'galleri']\n",
      "Top ten occuring words :  [('wa', 199857), ('thi', 86639), ('place', 55772), ('food', 53489), ('good', 50852), ('great', 44401), ('veri', 44062), ('time', 42695), ('get', 38251), ('would', 38160)]\n"
     ]
    }
   ],
   "source": [
    "all_reviews = list(reviews['cleaned'])\n",
    "all_text = \" \".join(all_reviews)\n",
    "all_words = all_text.split()\n",
    "print(all_words[0:10])\n",
    "\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(all_words)\n",
    "total_words = len(all_words)\n",
    "sorted_words=count_words.most_common(total_words)\n",
    "print(\"Top ten occuring words : \", sorted_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "# We will start createing dictionary with index 1 because 0 \n",
    "    # is reserved for padding\n",
    "\n",
    "vocab_to_int = {w: i+1 for i, (w, c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4: Encode the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode review\n",
    "encoded_reviews = list()\n",
    "for review in all_reviews:\n",
    "    encoded_review = list()\n",
    "    for word in review.split():\n",
    "        if word not in vocab_to_int.keys():\n",
    "            # if word is not available in vocab_to_int put 0 in that place\n",
    "            encoded_review.append(0)\n",
    "        else:\n",
    "            encoded_review.append(vocab_to_int[word])\n",
    "    encoded_reviews.append(encoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Make the encode_review of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all the encoded_review of the same length\n",
    "# this step will return features of review_ints,\n",
    "# where each review is padded with 0's or truncated to the input seq_length.\n",
    "# the longest review has 564 words\n",
    "# sequence_length is 100, but also could be 150, 200, 250 (here just for save energy)\n",
    "\n",
    "sequence_length = 100\n",
    "features = np.zeros((len(encoded_reviews), sequence_length), dtype=int)\n",
    "for i, review in enumerate(encoded_reviews):\n",
    "    review_len = len(review)\n",
    "    if review_len <= sequence_length:\n",
    "        zeros = list(np.zeros(sequence_length-review_len))\n",
    "        new = zeros+review\n",
    "    else:\n",
    "        new = review[:sequence_length]\n",
    "    features[i, :] = np.array(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6: Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels, 0 negative, 1 neutral, 2 positive\n",
    "labels = list(reviews['Review_Labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step7: Split this feature data into Traning, Testing and Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39999 5000 5000\n"
     ]
    }
   ],
   "source": [
    "# split this feature data into training and validation set\n",
    "# 80% training, 10% test and 10% validation dataset\n",
    "\n",
    "# However, for cpu running, set 10% of them\n",
    "features = features[:int(0.5*len(features))]\n",
    "labels = labels[:int(0.5*len(labels))]\n",
    "train_x = features[:int(0.8*len(features))]\n",
    "train_y = labels[:int(0.8*len(features))]\n",
    "valid_x = features[int(0.8*len(features)):int(0.9*len(features))]\n",
    "valid_y = labels[int(0.8*len(features)):int(0.9*len(features))]\n",
    "test_x = features[int(0.9*len(features)):]\n",
    "test_y = labels[int(0.9*len(features)):]\n",
    "print(len(train_y), len(valid_y), len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step8: Create DataLoader objects for Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#create Tensor Dataset\n",
    "train_data=TensorDataset(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n",
    "valid_data=TensorDataset(torch.FloatTensor(valid_x), torch.FloatTensor(valid_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(test_x), torch.FloatTensor(test_y))\n",
    "\n",
    "#dataloader\n",
    "# remember to add drop_last=True, which will delete the last batch of the data if it's size is not equal to batch_size\n",
    "batch_size=100\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step9: Analyze the dataloader data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([100, 100])\n",
      "Sample input: \n",
      " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1740e+03, 2.0000e+00,\n",
      "         7.0000e+01],\n",
      "        [1.0000e+00, 6.8200e+02, 3.0900e+03,  ..., 1.0000e+00, 3.0200e+02,\n",
      "         1.5700e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.3710e+03, 1.1308e+04,\n",
      "         3.2500e+02],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.0000e+02, 8.3000e+01,\n",
      "         7.9900e+02],\n",
      "        [2.3700e+02, 2.8000e+01, 4.0000e+01,  ..., 1.0000e+00, 1.7400e+02,\n",
      "         4.0700e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.0000e+00, 1.2900e+02,\n",
      "         1.9600e+02]])\n",
      "Sample label size:  torch.Size([100])\n",
      "Sample label: \n",
      " tensor([0., 2., 2., 2., 2., 2., 2., 0., 0., 2., 2., 2., 2., 0., 2., 0., 2., 2.,\n",
      "        0., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 0., 0., 2., 2., 0., 2., 0.,\n",
      "        1., 1., 2., 2., 2., 2., 2., 2., 1., 0., 0., 2., 2., 0., 2., 2., 0., 2.,\n",
      "        2., 0., 2., 2., 2., 2., 2., 0., 2., 1., 1., 2., 0., 2., 2., 0., 0., 2.,\n",
      "        2., 2., 0., 2., 2., 2., 1., 0., 2., 2., 0., 1., 1., 2., 2., 1., 0., 1.,\n",
      "        2., 2., 0., 2., 2., 1., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step10: Create an R-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN_model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed, hidden_size, num_layers, dropout, num_classes, pad_size):\n",
    "        super(RCNN_model, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed)\n",
    "        self.bidirectinoal=True\n",
    "        self.lstm = nn.LSTM(embed, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.maxpool = nn.MaxPool1d(pad_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2 + embed, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embed = self.embedding(x)\n",
    "        print(embed.shape)\n",
    "        out, _ = self.lstm(embed)\n",
    "        out = torch.cat((embed, out), 2)\n",
    "        out = F.relu(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.maxpool(out).squeeze()\n",
    "        print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step11: Initialize the R-CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCNN_model(\n",
      "  (embedding): Embedding(77398, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (maxpool): MaxPool1d(kernel_size=32, stride=32, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=812, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int) + 1\n",
    "embed = 300\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "dropout = 0.5\n",
    "pad_size = 32\n",
    "\n",
    "model_rcnn = RCNN_model(vocab_size, embed, hidden_size, num_layers,\n",
    "                        dropout, num_classes, pad_size)\n",
    "print(model_rcnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step12: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model):\n",
    "    lr = 0.001  # learning rate\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    total_batch = 0\n",
    "    train_loss = 0\n",
    "    result_dict = {}\n",
    "    epoch_list = []\n",
    "    batch_list = []\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for batch_idx, (trains, labels) in enumerate(train_loader):\n",
    "        \n",
    "        outputs = model(trains.long())\n",
    "        model.zero_grad()\n",
    "        loss = F.cross_entropy(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        true = labels.data.cpu()\n",
    "        predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "        train_acc = metrics.accuracy_score(true, predic)\n",
    "        loss_value = train_loss/(batch_idx+1)\n",
    "        epoch_list.append(epoch)\n",
    "        batch_list.append(batch_idx)\n",
    "        loss_list.append(loss_value)\n",
    "        acc_list.append(train_acc)\n",
    "\n",
    "        if total_batch % 20 == 0 :\n",
    "            print('epoch: {}'.format(epoch), 'batch: {}'.format(batch_idx), \n",
    "                  'total train loader: {}'.format(len(train_loader)),\n",
    "                  'Loss: %.3f | Acc: %.3f'\n",
    "            % (loss_value, train_acc))\n",
    "        total_batch += 1\n",
    "        \n",
    "    result_dict['epoch'] = epoch_list\n",
    "    result_dict['batch'] = batch_list\n",
    "    result_dict['loss'] = loss_list\n",
    "    result_dict['acc'] = acc_list\n",
    "\n",
    "    return pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step12-1. CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100, 300])\n",
      "torch.Size([100, 556, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (55600x3 and 812x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-bd383ec9cda4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrcnn_train_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_rcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrcnn_train_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcnn_train_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-6dbadfa03aba>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-9d2076e3a4be>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (55600x3 and 812x3)"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "rcnn_train_result = pd.DataFrame()\n",
    "for epoch in range(epochs):\n",
    "    train_result = train(epoch, model_rcnn)\n",
    "    rcnn_train_result = rcnn_train_result.append(train_result, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnn_train_result.to_csv('../result/RCNN/rcnn_train_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step13: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model):\n",
    "    lr = 0.001  # learning rate\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    total_batch = 0\n",
    "    train_loss = 0\n",
    "    result_dict = {}\n",
    "    epoch_list = []\n",
    "    batch_list = []\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for batch_idx, (trains, labels) in enumerate(test_loader):\n",
    "\n",
    "        outputs = model(trains.long()[None, ...])\n",
    "        model.zero_grad()\n",
    "        loss = F.cross_entropy(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        true = labels.data.cpu()\n",
    "        predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "        train_acc = metrics.accuracy_score(true, predic)\n",
    "        train_loss += loss.item()\n",
    "        loss_value = train_loss/(batch_idx+1)\n",
    "        epoch_list.append(epoch)\n",
    "        batch_list.append(batch_idx)\n",
    "        loss_list.append(loss_value)\n",
    "        acc_list.append(train_acc)\n",
    "            \n",
    "        if total_batch % 50 == 0 :\n",
    "            print('epoch: {}'.format(epoch), 'batch: {}'.format(batch_idx), \n",
    "                  'total train loader: {}'.format(len(test_loader)),\n",
    "                  'Loss: %.3f | Acc: %.3f'\n",
    "            % (loss_value, train_acc))\n",
    "        \n",
    "        total_batch += 1\n",
    "        \n",
    "    result_dict['epoch'] = epoch_list\n",
    "    result_dict['batch'] = batch_list\n",
    "    result_dict['loss'] = loss_list\n",
    "    result_dict['acc'] = acc_list\n",
    "    \n",
    "    return pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 total train loader: 50 Loss: 1.044 | Acc: 0.760\n"
     ]
    }
   ],
   "source": [
    "bilstm_test_result = pd.DataFrame()\n",
    "bilstm_test_result.append(train(0, model_bilstm), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
